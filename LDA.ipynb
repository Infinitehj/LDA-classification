{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\u6211', u'\\u7231', u'\\u4f60', u'', u'', u'', u'', u'', u'', u'', u'']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "string = u\"我 爱 你        \"\n",
    "print(string.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题0:  30(0.01889042299485377) 企业(0.01889042299485377) 费用(0.01889042299485377) \n",
      "\n",
      "主题1:  汽车(0.029240107388817552) 强制(0.029240107388817552) 资金(0.02340375860861445) \n",
      "\n",
      "主题2:  车主(0.029411764705882353) 发展(0.029411764705882353) 陆风(0.023541152988141363) \n",
      "\n",
      "主题3:  利润(0.03657052452233175) 奥克斯(0.03048557867834976) 三者(0.02440063283436777) \n",
      "\n",
      "主题4:  新飞(0.05046889180195623) 责任(0.03534334980336795) 投保(0.03030150247050519) \n",
      "\n",
      "主题5:  强制保险(0.034872925612162) 限额(0.029070442149239872) 保费(0.029070442149239872) \n",
      "\n",
      "文档0: (4, 0.24905087319665906) ['新飞', '责任', '投保'] \n",
      "\n",
      "文档1: (2, 0.19962335216572505) ['车主', '发展', '陆风'] \n",
      "\n",
      "文档2: (5, 0.19423558897243107) ['强制保险', '限额', '保费'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jieba\n",
    "import random\n",
    "import numpy\n",
    "def read_document(doc_num,stop_words,dic):\n",
    "    doc = []\n",
    "    for m in range(doc_num):\n",
    "        doc.append([])\n",
    "        path = os.getcwd() + \"\\\\data\\\\C000007\\\\1\"+str(m)+\".txt\"\n",
    "        #path = os.getcwd() + \"\\\\data\\\\1\"+str(m)+\".txt\"\n",
    "        f = open(path)\n",
    "        try:\n",
    "            ftext = f.read()\n",
    "\n",
    "        finally:\n",
    "            f.close()                  \n",
    "        f_seg_list = jieba.cut(ftext)  \n",
    "        #liststr=\"/\".join(f_seg_list)\n",
    "        \n",
    "        term_id = 0\n",
    "        for myword in f_seg_list:\n",
    "        \n",
    "            if not(myword.strip() in stop_words) and len(myword.strip())>1:\n",
    "                dic.setdefault(myword,0)\n",
    "               \n",
    "                doc[m].append(myword)\n",
    "        for key in dic.keys():\n",
    "            dic[key] = term_id\n",
    "            term_id += 1\n",
    "    return doc,dic\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "  \n",
    "    \n",
    "def load_stopwords():\n",
    "    f_stop = open('stopwords.txt')\n",
    "    try:\n",
    "        f_stop_text = f_stop.read()\n",
    "\n",
    "    finally:\n",
    "        f_stop.close()\n",
    "\n",
    "    f_stop_seg_list = f_stop_text.split('\\n')\n",
    "    return f_stop_seg_list\n",
    "\n",
    "\n",
    "\n",
    "def gibbs_sampling(z, m, i, nt, nd, nt_sum, nd_sum, term):\n",
    "    topic = z[m][i]      #当前主题\n",
    "    nt[term][topic] -= 1 #去除当前词\n",
    "    nd[m][topic] -= 1    \n",
    "    nt_sum[topic] -= 1\n",
    "    nd_sum[m] -= 1\n",
    "\n",
    "    topic_alpha = topic_number * alpha\n",
    "    term_beta = len(dic) * beta\n",
    "    p = [0 for x in range(topic_number)]   #p[k]: 属于主题k的概率\n",
    "    for k in range(topic_number):\n",
    "        p[k] = (nd[m][k] + alpha) / (nd_sum[m] + topic_alpha) \\\n",
    "                * (nt[term][k] + beta) / (nt_sum[k] +term_beta)\n",
    "        if k >= 1:             #转换成累加概率\n",
    "            p[k] += p[k-1]\n",
    "    gs = random.random() * p[topic_number-1]  #随机采样\n",
    "    new_topic = 0\n",
    "    while new_topic < topic_number:\n",
    "        if p[new_topic] > gs:\n",
    "            break\n",
    "        new_topic += 1\n",
    "    nt[term][new_topic] += 1\n",
    "    nd[m][new_topic] += 1\n",
    "    nt_sum[new_topic] += 1\n",
    "    nd_sum[m] += 1\n",
    "    z[m][i] = new_topic #新主题   \n",
    "    \n",
    "def calc_theta(nd, nd_sum): #每个文档的主题分布\n",
    "    doc_num = len(nd)\n",
    "    topic_alpha = topic_number * alpha\n",
    "    theta = [[0 for t in range(topic_number)] for d in range(doc_num)]\n",
    "    for m in range(doc_num):\n",
    "        for k in range(topic_number):\n",
    "            theta[m][k] = (nd[m][k] + alpha) / (nd_sum[m] + topic_alpha)\n",
    "    return theta\n",
    "\n",
    "def calc_phi(nt, nt_sum):  #每个主题的词分布\n",
    "    term_num = len(nt)\n",
    "    term_beta = term_num * beta\n",
    "    phi = [[0 for w in range(term_num)] for t in range(topic_number)]\n",
    "    for k in range(topic_number):\n",
    "        for term in range(term_num):\n",
    "            phi[k][term] = (nt[term][k] + beta) / (nt_sum[k] + term_beta)\n",
    "    return phi\n",
    "\n",
    "def lda(z, nt, nd, nt_sum, dic, doc):\n",
    "    doc_num = len(z)    #文档数\n",
    "    for time in range(10000):\n",
    "        for m in range(doc_num):\n",
    "            doc_length = len(z[m])  #第m篇文档的长度\n",
    "            for i in range(doc_length):\n",
    "                term = dic[doc[m][i]]  #第m篇文档的第i个词 --> 词汇\n",
    "                gibbs_sampling(z, m, i, nt, nd, nt_sum, nd_sum, term)\n",
    "        theta = calc_theta(nd, nd_sum)   #计算每个文档的主题分布\n",
    "        phi = calc_phi(nt, nt_sum)       #计算每个主题的的词分布\n",
    "        return theta,phi\n",
    "\n",
    "\n",
    "                \n",
    "def init_topic(doc, nt, nd, nt_sum, nd_sum, dic):\n",
    "    #随机分配类型\n",
    "    \n",
    "    #topic_number = len(nd[m]) \n",
    "\n",
    "    doc_num = len(nd)\n",
    "    z = [[0 for x in range(len(doc[m]))] for m in range(doc_num)]\n",
    "    for x in range(doc_num):\n",
    "        nd_sum[x] = len(doc[x])\n",
    "        for y in range(len(doc[x])):\n",
    "            topic = random.randint(0,topic_number-1)\n",
    "            z[x][y] = topic\n",
    "            term = dic.get(doc[x][y])\n",
    "            nt[term][topic] += 1\n",
    "            nd[x][topic] += 1\n",
    "            nt_sum[topic] += 1\n",
    "\n",
    "    return z                \n",
    "\n",
    "def show_result(theta, phi, dic):\n",
    "    term_num = len(dic)\n",
    "    doc_num = len(theta)\n",
    "    topic_num = len(phi)\n",
    "    dics = []\n",
    "    dics = list(dic.items())\n",
    "    #print(dics)\n",
    "    dics.sort(key = lambda i:i[1],reverse = False)\n",
    "    #print(dics)\n",
    "    #排序后的词元组\n",
    "    \n",
    "    topic_word=[]\n",
    "    for t in range(topic_num):\n",
    "        topic_word.append([])\n",
    "        print(\"主题\" +str(t)+ \": \",end = \" \")\n",
    "        top_terms = []\n",
    "        top_terms = [(n,phi[t][n]) for n in range(term_num)]\n",
    "        top_terms.sort(key = lambda i:i[1],reverse = True)\n",
    "        #排序后最优主题词汇\n",
    "        for top in range(top_num):\n",
    "            top_word = dics[top_terms[top][0]][0]\n",
    "            topic_word[t].append(top_word)\n",
    "            #top_word = sorted_dic({value:key for key, value in dic.items()})[top_terms[top][0]]\n",
    "            print(top_word +\"(\" + str(top_terms[top][1]) +\") \",end = \"\")\n",
    "            \n",
    "            \n",
    "        print(\"\\n\")\n",
    "    doc_topic = []\n",
    "    for m in range(doc_num):\n",
    "        print(\"文档\"+str(m)+\": \",end = \"\")\n",
    "        \n",
    "        doc_topic.append([])\n",
    "        doc_topic[m] = [(k,theta[m][k]) for k in range(topic_num)]\n",
    "        doc_topic[m].sort(key = lambda i:i[1],reverse = True)\n",
    "       \n",
    "        print(doc_topic[m][0],end = \" \")\n",
    "        tid = doc_topic[m][0][0]\n",
    "        print(topic_word[tid],end = \" \")\n",
    "        \n",
    "        print(\"\\n\")      \n",
    "    #print(topic_word) \n",
    "    #print(doc_topic)\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    doc_num = 3#文档数目\n",
    "    #载入停用词表\n",
    "    stop_words = load_stopwords()\n",
    "    dic = {}\n",
    "    doc,dic = read_document(doc_num,stop_words,dic)\n",
    "    #print(len(doc))\n",
    "    #print(len(dic))\n",
    "    #LDA\n",
    "    \n",
    " \n",
    "    topic_number = 6 #主题数\n",
    "    term_num = len(dic)        #词汇数\n",
    "    top_num = 3               #主题关键词数\n",
    "    alpha = 50/topic_number\n",
    "    beta = 0.01\n",
    "    # nt[w][t]：第term个词属于第t个主题的次数\n",
    "    nt = [[0 for t in range(topic_number)] for term in range(term_num)]\n",
    "    # nd[d][t]: 第d个文档中出现第t个主题的次数\n",
    "    nd = [[0 for t in range(topic_number)] for d in range(doc_num)]\n",
    "    # nt_sum[t]: 第t个主题出现的次数(nt矩阵的第t列)\n",
    "    nt_sum = [0 for t in range(topic_number)]\n",
    "    # nd_sum[d]: 第d个文档的长度（nd矩阵的第d行）\n",
    "    nd_sum = [0 for d in range(doc_num)]\n",
    "    \n",
    "    z = init_topic(doc, nt, nd, nt_sum, nd_sum, dic)\n",
    "    theta, phi = lda(z, nt, nd, nt_sum, dic, doc)\n",
    "    # 输出每个文档的主题和每个主题的关键字\n",
    "    show_result(theta, phi, dic)\n",
    "    #print(doc[1])\n",
    "    #print(z[1])\n",
    "    #print(theta)\n",
    "    #print(phi)\n",
    "    #print(dic)\n",
    "    #print(stop_words)\n",
    "\n",
    "    #print(theta[5])\n",
    "    #print('虽然' in dic)\n",
    "    #print('已经' in stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "X = theta[0:5]\n",
    "Y = [0,0,0,1,1]\n",
    "clf = svm.SVC()\n",
    "#clf = svm.SVC(decision_function_shape ='ovo')\n",
    "clf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.2642369020501139,\n",
       "  0.2072892938496583,\n",
       "  0.13895216400911162,\n",
       "  0.20956719817767655,\n",
       "  0.17995444191343962],\n",
       " [0.13559322033898305,\n",
       "  0.2598870056497175,\n",
       "  0.14689265536723164,\n",
       "  0.2711864406779661,\n",
       "  0.1864406779661017],\n",
       " [0.16917293233082706,\n",
       "  0.21804511278195488,\n",
       "  0.19924812030075187,\n",
       "  0.22180451127819548,\n",
       "  0.19172932330827067],\n",
       " [0.17857142857142858,\n",
       "  0.13214285714285715,\n",
       "  0.2892857142857143,\n",
       "  0.22142857142857142,\n",
       "  0.17857142857142858],\n",
       " [0.3240418118466899,\n",
       "  0.13937282229965156,\n",
       "  0.1672473867595819,\n",
       "  0.22996515679442509,\n",
       "  0.13937282229965156]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[0.1732283464566929, 0.1968503937007874, 0.23622047244094488, 0.18110236220472442, 0.2125984251968504]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.15793470007593013,\n",
       "  0.1533788914198937,\n",
       "  0.13515565679574792,\n",
       "  0.17160212604403946,\n",
       "  0.24905087319665906,\n",
       "  0.1328777524677297],\n",
       " [0.1657250470809793,\n",
       "  0.18832391713747648,\n",
       "  0.19962335216572505,\n",
       "  0.13747645951035783,\n",
       "  0.14312617702448213,\n",
       "  0.1657250470809793],\n",
       " [0.14912280701754385,\n",
       "  0.16791979949874686,\n",
       "  0.17731829573934837,\n",
       "  0.15664160401002505,\n",
       "  0.15476190476190474,\n",
       "  0.19423558897243107]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package gensim:\n",
      "\n",
      "NAME\n",
      "    gensim\n",
      "\n",
      "DESCRIPTION\n",
      "    This package contains interfaces and functionality to compute pair-wise document\n",
      "    similarities within a corpus of documents.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    corpora (package)\n",
      "    interfaces\n",
      "    matutils\n",
      "    models (package)\n",
      "    nosy\n",
      "    parsing (package)\n",
      "    scripts (package)\n",
      "    similarities (package)\n",
      "    summarization (package)\n",
      "    test (package)\n",
      "    utils\n",
      "\n",
      "CLASSES\n",
      "    logging.Handler(logging.Filterer)\n",
      "        NullHandler\n",
      "    \n",
      "    class NullHandler(logging.Handler)\n",
      "     |  For python versions <= 2.6; same as `logging.NullHandler` in 2.7.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NullHandler\n",
      "     |      logging.Handler\n",
      "     |      logging.Filterer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  emit(self, record)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from logging.Handler:\n",
      "     |  \n",
      "     |  __init__(self, level=0)\n",
      "     |      Initializes the instance - basically setting the formatter to None\n",
      "     |      and the filter list to empty.\n",
      "     |  \n",
      "     |  acquire(self)\n",
      "     |      Acquire the I/O thread lock.\n",
      "     |  \n",
      "     |  close(self)\n",
      "     |      Tidy up any resources used by the handler.\n",
      "     |      \n",
      "     |      This version removes the handler from an internal map of handlers,\n",
      "     |      _handlers, which is used for handler lookup by name. Subclasses\n",
      "     |      should ensure that this gets called from overridden close()\n",
      "     |      methods.\n",
      "     |  \n",
      "     |  createLock(self)\n",
      "     |      Acquire a thread lock for serializing access to the underlying I/O.\n",
      "     |  \n",
      "     |  flush(self)\n",
      "     |      Ensure all logging output has been flushed.\n",
      "     |      \n",
      "     |      This version does nothing and is intended to be implemented by\n",
      "     |      subclasses.\n",
      "     |  \n",
      "     |  format(self, record)\n",
      "     |      Format the specified record.\n",
      "     |      \n",
      "     |      If a formatter is set, use it. Otherwise, use the default formatter\n",
      "     |      for the module.\n",
      "     |  \n",
      "     |  get_name(self)\n",
      "     |  \n",
      "     |  handle(self, record)\n",
      "     |      Conditionally emit the specified logging record.\n",
      "     |      \n",
      "     |      Emission depends on filters which may have been added to the handler.\n",
      "     |      Wrap the actual emission of the record with acquisition/release of\n",
      "     |      the I/O thread lock. Returns whether the filter passed the record for\n",
      "     |      emission.\n",
      "     |  \n",
      "     |  handleError(self, record)\n",
      "     |      Handle errors which occur during an emit() call.\n",
      "     |      \n",
      "     |      This method should be called from handlers when an exception is\n",
      "     |      encountered during an emit() call. If raiseExceptions is false,\n",
      "     |      exceptions get silently ignored. This is what is mostly wanted\n",
      "     |      for a logging system - most users will not care about errors in\n",
      "     |      the logging system, they are more interested in application errors.\n",
      "     |      You could, however, replace this with a custom handler if you wish.\n",
      "     |      The record which was being processed is passed in to this method.\n",
      "     |  \n",
      "     |  release(self)\n",
      "     |      Release the I/O thread lock.\n",
      "     |  \n",
      "     |  setFormatter(self, fmt)\n",
      "     |      Set the formatter for this handler.\n",
      "     |  \n",
      "     |  setLevel(self, level)\n",
      "     |      Set the logging level of this handler.  level must be an int or a str.\n",
      "     |  \n",
      "     |  set_name(self, name)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from logging.Handler:\n",
      "     |  \n",
      "     |  name\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from logging.Filterer:\n",
      "     |  \n",
      "     |  addFilter(self, filter)\n",
      "     |      Add the specified filter to this handler.\n",
      "     |  \n",
      "     |  filter(self, record)\n",
      "     |      Determine if a record is loggable by consulting all the filters.\n",
      "     |      \n",
      "     |      The default is to allow the record to be logged; any filter can veto\n",
      "     |      this and the record is then dropped. Returns a zero value if a record\n",
      "     |      is to be dropped, else non-zero.\n",
      "     |      \n",
      "     |      .. versionchanged: 3.2\n",
      "     |      \n",
      "     |         Allow filters to be just callables.\n",
      "     |  \n",
      "     |  removeFilter(self, filter)\n",
      "     |      Remove the specified filter from this handler.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from logging.Filterer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    logger = <logging.Logger object>\n",
      "\n",
      "VERSION\n",
      "    0.12.4\n",
      "\n",
      "FILE\n",
      "    d:\\360downloads\\anaconda3\\lib\\site-packages\\gensim-0.12.4-py3.4-win-amd64.egg\\gensim\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "help(gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package gensim.models in gensim:\n",
      "\n",
      "NAME\n",
      "    gensim.models\n",
      "\n",
      "DESCRIPTION\n",
      "    This package contains algorithms for extracting document representations from their raw\n",
      "    bag-of-word counts.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    doc2vec\n",
      "    hdpmodel\n",
      "    lda_dispatcher\n",
      "    lda_worker\n",
      "    ldamodel\n",
      "    ldamulticore\n",
      "    logentropy_model\n",
      "    lsi_dispatcher\n",
      "    lsi_worker\n",
      "    lsimodel\n",
      "    phrases\n",
      "    rpmodel\n",
      "    tfidfmodel\n",
      "    word2vec\n",
      "    wrappers (package)\n",
      "\n",
      "CLASSES\n",
      "    gensim.interfaces.TransformationABC(gensim.utils.SaveLoad)\n",
      "        VocabTransform\n",
      "    \n",
      "    class VocabTransform(gensim.interfaces.TransformationABC)\n",
      "     |  Remap feature ids to new values.\n",
      "     |  \n",
      "     |  Given a mapping between old ids and new ids (some old ids may be missing = these\n",
      "     |  features are to be discarded), this will wrap a corpus so that iterating over\n",
      "     |  `VocabTransform[corpus]` returns the same vectors but with the new ids.\n",
      "     |  \n",
      "     |  Old features that have no counterpart in the new ids are discarded. This\n",
      "     |  can be used to filter vocabulary of a corpus \"online\"::\n",
      "     |  \n",
      "     |  >>> old2new = dict((oldid, newid) for newid, oldid in enumerate(ids_you_want_to_keep))\n",
      "     |  >>> vt = VocabTransform(old2new)\n",
      "     |  >>> for vec_with_new_ids in vt[corpus_with_old_ids]:\n",
      "     |  >>>     ...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VocabTransform\n",
      "     |      gensim.interfaces.TransformationABC\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, bow)\n",
      "     |      Return representation with the ids transformed.\n",
      "     |  \n",
      "     |  __init__(self, old2new, id2token=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=2)\n",
      "     |      Save the object to file (also see `load`).\n",
      "     |      \n",
      "     |      `fname_or_handle` is either a string specifying the file name to\n",
      "     |      save to, or an open file-like object which can be written to. If\n",
      "     |      the object is a file handle, no special array handling will be\n",
      "     |      performed; all attributes will be saved to the same file.\n",
      "     |      \n",
      "     |      If `separately` is None, automatically detect large\n",
      "     |      numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |      them into separate files. This avoids pickle memory errors and\n",
      "     |      allows mmap'ing large arrays back on load efficiently.\n",
      "     |      \n",
      "     |      You can also set `separately` manually, in which case it must be\n",
      "     |      a list of attribute names to be stored in separate files. The\n",
      "     |      automatic check is not performed in this case.\n",
      "     |      \n",
      "     |      `ignore` is a set of attribute names to *not* serialize (file\n",
      "     |      handles, caches etc). On subsequent load() these attributes will\n",
      "     |      be set to None.\n",
      "     |      \n",
      "     |      `pickle_protocol` defaults to 2 so the pickled object can be imported\n",
      "     |      in both Python 2 and 3.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load a previously saved object from file (also see `save`).\n",
      "     |      \n",
      "     |      If the object was saved with large arrays stored separately, you can load\n",
      "     |      these arrays via mmap (shared memory) using `mmap='r'`. Default: don't use\n",
      "     |      mmap, load large arrays as normal objects.\n",
      "     |      \n",
      "     |      If the file being loaded is compressed (either '.gz' or '.bz2'), then\n",
      "     |      `mmap=None` must be set.  Load will raise an `IOError` if this condition\n",
      "     |      is encountered.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FILE\n",
      "    d:\\360downloads\\anaconda3\\lib\\site-packages\\gensim-0.12.4-py3.4-win-amd64.egg\\gensim\\models\\__init__.py\n",
      "\n",
      "\n",
      "Help on module gensim.models.ldamodel in gensim.models:\n",
      "\n",
      "NAME\n",
      "    gensim.models.ldamodel - **For a faster implementation of LDA (parallelized for multicore machines), see** :mod:`gensim.models.ldamulticore`.\n",
      "\n",
      "DESCRIPTION\n",
      "    Latent Dirichlet Allocation (LDA) in Python.\n",
      "    \n",
      "    This module allows both LDA model estimation from a training corpus and inference of topic\n",
      "    distribution on new, unseen documents. The model can also be updated with new documents\n",
      "    for online training.\n",
      "    \n",
      "    The core estimation code is based on the `onlineldavb.py` script by M. Hoffman [1]_, see\n",
      "    **Hoffman, Blei, Bach: Online Learning for Latent Dirichlet Allocation, NIPS 2010.**\n",
      "    \n",
      "    The algorithm:\n",
      "    \n",
      "    * is **streamed**: training documents may come in sequentially, no random access required,\n",
      "    * runs in **constant memory** w.r.t. the number of documents: size of the\n",
      "      training corpus does not affect memory footprint, can process corpora larger than RAM, and\n",
      "    * is **distributed**: makes use of a cluster of machines, if available, to\n",
      "      speed up model estimation.\n",
      "    \n",
      "    .. [1] http://www.cs.princeton.edu/~mdhoffma\n",
      "\n",
      "CLASSES\n",
      "    gensim.interfaces.TransformationABC(gensim.utils.SaveLoad)\n",
      "        LdaModel\n",
      "    gensim.utils.SaveLoad(builtins.object)\n",
      "        LdaState\n",
      "    \n",
      "    class LdaModel(gensim.interfaces.TransformationABC)\n",
      "     |  The constructor estimates Latent Dirichlet Allocation model parameters based\n",
      "     |  on a training corpus:\n",
      "     |  \n",
      "     |  >>> lda = LdaModel(corpus, num_topics=10)\n",
      "     |  \n",
      "     |  You can then infer topic distributions on new, unseen documents, with\n",
      "     |  \n",
      "     |  >>> doc_lda = lda[doc_bow]\n",
      "     |  \n",
      "     |  The model can be updated (trained) with new documents via\n",
      "     |  \n",
      "     |  >>> lda.update(other_corpus)\n",
      "     |  \n",
      "     |  Model persistency is achieved through its `load`/`save` methods.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LdaModel\n",
      "     |      gensim.interfaces.TransformationABC\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, bow, eps=None)\n",
      "     |      Return topic distribution for the given document `bow`, as a list of\n",
      "     |      (topic_id, topic_probability) 2-tuples.\n",
      "     |      \n",
      "     |      Ignore topics with very low probability (below `eps`).\n",
      "     |  \n",
      "     |  __init__(self, corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01)\n",
      "     |      If given, start training from the iterable `corpus` straight away. If not given,\n",
      "     |      the model is left untrained (presumably because you want to call `update()` manually).\n",
      "     |      \n",
      "     |      `num_topics` is the number of requested latent topics to be extracted from\n",
      "     |      the training corpus.\n",
      "     |      \n",
      "     |      `id2word` is a mapping from word ids (integers) to words (strings). It is\n",
      "     |      used to determine the vocabulary size, as well as for debugging and topic\n",
      "     |      printing.\n",
      "     |      \n",
      "     |      `alpha` and `eta` are hyperparameters that affect sparsity of the document-topic\n",
      "     |      (theta) and topic-word (lambda) distributions. Both default to a symmetric\n",
      "     |      1.0/num_topics prior.\n",
      "     |      \n",
      "     |      `alpha` can be set to an explicit array = prior of your choice. It also\n",
      "     |      support special values of 'asymmetric' and 'auto': the former uses a fixed\n",
      "     |      normalized asymmetric 1.0/topicno prior, the latter learns an asymmetric\n",
      "     |      prior directly from your data.\n",
      "     |      \n",
      "     |      `eta` can be a scalar for a symmetric prior over topic/word\n",
      "     |      distributions, or a matrix of shape num_topics x num_words, which can\n",
      "     |      be used to impose asymmetric priors over the word distribution on a\n",
      "     |      per-topic basis. This may be useful if you want to seed certain topics\n",
      "     |      with particular words by boosting the priors for those words.  It also\n",
      "     |      supports the special value 'auto', which learns an asymmetric prior\n",
      "     |      directly from your data.\n",
      "     |      \n",
      "     |      Turn on `distributed` to force distributed computing (see the `web tutorial <http://radimrehurek.com/gensim/distributed.html>`_\n",
      "     |      on how to set up a cluster of machines for gensim).\n",
      "     |      \n",
      "     |      Calculate and log perplexity estimate from the latest mini-batch every\n",
      "     |      `eval_every` model updates (setting this to 1 slows down training ~2x;\n",
      "     |      default is 10 for better performance). Set to None to disable perplexity estimation.\n",
      "     |      \n",
      "     |      `decay` and `offset` parameters are the same as Kappa and Tau_0 in\n",
      "     |      Hoffman et al, respectively.\n",
      "     |      \n",
      "     |      `minimum_probability` controls filtering the topics returned for a document (bow).\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      \n",
      "     |      >>> lda = LdaModel(corpus, num_topics=100)  # train model\n",
      "     |      >>> print(lda[doc_bow]) # get topic probability distribution for a document\n",
      "     |      >>> lda.update(corpus2) # update the LDA model with additional documents\n",
      "     |      >>> print(lda[doc_bow])\n",
      "     |      \n",
      "     |      >>> lda = LdaModel(corpus, num_topics=50, alpha='auto', eval_every=5)  # train asymmetric alpha from data\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |  \n",
      "     |  bound(self, corpus, gamma=None, subsample_ratio=1.0)\n",
      "     |      Estimate the variational bound of documents from `corpus`:\n",
      "     |      E_q[log p(corpus)] - E_q[log q(corpus)]\n",
      "     |      \n",
      "     |      `gamma` are the variational parameters on topic weights for each `corpus`\n",
      "     |      document (=2d matrix=what comes out of `inference()`).\n",
      "     |      If not supplied, will be inferred from the model.\n",
      "     |  \n",
      "     |  clear(self)\n",
      "     |      Clear model state (free up some memory). Used in the distributed algo.\n",
      "     |  \n",
      "     |  do_estep(self, chunk, state=None)\n",
      "     |      Perform inference on a chunk of documents, and accumulate the collected\n",
      "     |      sufficient statistics in `state` (or `self.state` if None).\n",
      "     |  \n",
      "     |  do_mstep(self, rho, other, extra_pass=False)\n",
      "     |      M step: use linear interpolation between the existing topics and\n",
      "     |      collected sufficient statistics in `other` to update the topics.\n",
      "     |  \n",
      "     |  get_document_topics(self, bow, minimum_probability=None)\n",
      "     |      Return topic distribution for the given document `bow`, as a list of\n",
      "     |      (topic_id, topic_probability) 2-tuples.\n",
      "     |      \n",
      "     |      Ignore topics with very low probability (below `minimum_probability`).\n",
      "     |  \n",
      "     |  get_topic_terms(self, topicid, topn=10)\n",
      "     |      Return a list of `(word_id, probability)` 2-tuples for the most\n",
      "     |      probable words in topic `topicid`.\n",
      "     |      \n",
      "     |      Only return 2-tuples for the topn most probable words (ignore the rest).\n",
      "     |  \n",
      "     |  inference(self, chunk, collect_sstats=False)\n",
      "     |      Given a chunk of sparse document vectors, estimate gamma (parameters\n",
      "     |      controlling the topic weights) for each document in the chunk.\n",
      "     |      \n",
      "     |      This function does not modify the model (=is read-only aka const). The\n",
      "     |      whole input chunk of document is assumed to fit in RAM; chunking of a\n",
      "     |      large corpus must be done earlier in the pipeline.\n",
      "     |      \n",
      "     |      If `collect_sstats` is True, also collect sufficient statistics needed\n",
      "     |      to update the model's topic-word distributions, and return a 2-tuple\n",
      "     |      `(gamma, sstats)`. Otherwise, return `(gamma, None)`. `gamma` is of shape\n",
      "     |      `len(chunk) x self.num_topics`.\n",
      "     |      \n",
      "     |      Avoids computing the `phi` variational parameter directly using the\n",
      "     |      optimization presented in **Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001**.\n",
      "     |  \n",
      "     |  init_dir_prior(self, prior, name)\n",
      "     |  \n",
      "     |  log_perplexity(self, chunk, total_docs=None)\n",
      "     |      Calculate and return per-word likelihood bound, using the `chunk` of\n",
      "     |      documents as evaluation corpus. Also output the calculated statistics. incl.\n",
      "     |      perplexity=2^(-bound), to log at INFO level.\n",
      "     |  \n",
      "     |  print_topic(self, topicid, topn=10)\n",
      "     |      Return the result of `show_topic`, but formatted as a single string.\n",
      "     |  \n",
      "     |  print_topics(self, num_topics=10, num_words=10)\n",
      "     |  \n",
      "     |  save(self, fname, ignore=['state', 'dispatcher'], *args, **kwargs)\n",
      "     |      Save the model to file.\n",
      "     |      \n",
      "     |      Large internal arrays may be stored into separate files, with `fname` as prefix.\n",
      "     |      \n",
      "     |      `separately` can be used to define which arrays should be stored in separate files.\n",
      "     |      \n",
      "     |      `ignore` parameter can be used to define which variables should be ignored, i.e. left\n",
      "     |      out from the pickled lda model. By default the internal `state` is ignored as it uses\n",
      "     |      its own serialisation not the one provided by `LdaModel`. The `state` and `dispatcher\n",
      "     |      will be added to any ignore parameter defined.\n",
      "     |      \n",
      "     |      \n",
      "     |      Note: do not save as a compressed file if you intend to load the file back with `mmap`.\n",
      "     |      \n",
      "     |      Note: If you intend to use models across Python 2/3 versions there are a few things to\n",
      "     |      keep in mind:\n",
      "     |      \n",
      "     |        1. The pickled Python dictionaries will not work across Python versions\n",
      "     |        2. The `save` method does not automatically save all NumPy arrays using NumPy, only\n",
      "     |           those ones that exceed `sep_limit` set in `gensim.utils.SaveLoad.save`. The main\n",
      "     |           concern here is the `alpha` array if for instance using `alpha='auto'`.\n",
      "     |      \n",
      "     |      Please refer to the wiki recipes section (https://github.com/piskvorky/gensim/wiki/Recipes-&-FAQ#q9-how-do-i-load-a-model-in-python-3-that-was-trained-and-saved-using-python-2)\n",
      "     |      for an example on how to work around these issues.\n",
      "     |  \n",
      "     |  show_topic(self, topicid, topn=10)\n",
      "     |      Return a list of `(word, probability)` 2-tuples for the most probable\n",
      "     |      words in topic `topicid`.\n",
      "     |      \n",
      "     |      Only return 2-tuples for the topn most probable words (ignore the rest).\n",
      "     |  \n",
      "     |  show_topics(self, num_topics=10, num_words=10, log=False, formatted=True)\n",
      "     |      For `num_topics` number of topics, return `num_words` most significant words\n",
      "     |      (10 words per topic, by default).\n",
      "     |      \n",
      "     |      The topics are returned as a list -- a list of strings if `formatted` is\n",
      "     |      True, or a list of `(word, probability)` 2-tuples if False.\n",
      "     |      \n",
      "     |      If `log` is True, also output this result to log.\n",
      "     |      \n",
      "     |      Unlike LSA, there is no natural ordering between the topics in LDA.\n",
      "     |      The returned `num_topics <= self.num_topics` subset of all topics is therefore\n",
      "     |      arbitrary and may change between two LDA training runs.\n",
      "     |  \n",
      "     |  sync_state(self)\n",
      "     |  \n",
      "     |  top_topics(self, corpus, num_words=20)\n",
      "     |      Calculate the Umass topic coherence for each topic. Algorithm from\n",
      "     |      **Mimno, Wallach, Talley, Leenders, McCallum: Optimizing Semantic Coherence in Topic Models, CEMNLP 2011.**\n",
      "     |  \n",
      "     |  update(self, corpus, chunksize=None, decay=None, offset=None, passes=None, update_every=None, eval_every=None, iterations=None, gamma_threshold=None, chunks_as_numpy=False)\n",
      "     |      Train the model with new documents, by EM-iterating over `corpus` until\n",
      "     |      the topics converge (or until the maximum number of allowed iterations\n",
      "     |      is reached). `corpus` must be an iterable (repeatable stream of documents),\n",
      "     |      \n",
      "     |      In distributed mode, the E step is distributed over a cluster of machines.\n",
      "     |      \n",
      "     |      This update also supports updating an already trained model (`self`)\n",
      "     |      with new documents from `corpus`; the two models are then merged in\n",
      "     |      proportion to the number of old vs. new documents. This feature is still\n",
      "     |      experimental for non-stationary input streams.\n",
      "     |      \n",
      "     |      For stationary input (no topic drift in new documents), on the other hand,\n",
      "     |      this equals the online update of Hoffman et al. and is guaranteed to\n",
      "     |      converge for any `decay` in (0.5, 1.0>. Additionally, for smaller\n",
      "     |      `corpus` sizes, an increasing `offset` may be beneficial (see\n",
      "     |      Table 1 in Hoffman et al.)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ------------\n",
      "     |      corpus: (gensim corpus object, list of tuples)\n",
      "     |          The corpus with which the LDA model should be updated with.\n",
      "     |      \n",
      "     |      chunks_as_numpy: bool\n",
      "     |          Whether each chunk passed to `.inference` should be a numpy\n",
      "     |          array of not. Numpy can in some settings turn the term IDs\n",
      "     |          into floats, these will be converted back into integers in\n",
      "     |          inference, which incurs a performance hit. For distributed\n",
      "     |          computing it may be desirable to keep the chunks as numpy\n",
      "     |          arrays.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      For other parameter settings see LdaModel().\n",
      "     |  \n",
      "     |  update_alpha(self, gammat, rho)\n",
      "     |      Update parameters for the Dirichlet prior on the per-document\n",
      "     |      topic weights `alpha` given the last `gammat`.\n",
      "     |  \n",
      "     |  update_eta(self, lambdat, rho)\n",
      "     |      Update parameters for the Dirichlet prior on the per-topic\n",
      "     |      word weights `eta` given the last `lambdat`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  load(fname, *args, **kwargs) from builtins.type\n",
      "     |      Load a previously saved object from file (also see `save`).\n",
      "     |      \n",
      "     |      Large arrays can be memmap'ed back as read-only (shared memory) by setting `mmap='r'`:\n",
      "     |      \n",
      "     |          >>> LdaModel.load(fname, mmap='r')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LdaState(gensim.utils.SaveLoad)\n",
      "     |  Encapsulate information for distributed computation of LdaModel objects.\n",
      "     |  \n",
      "     |  Objects of this class are sent over the network, so try to keep them lean to\n",
      "     |  reduce traffic.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LdaState\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, eta, shape)\n",
      "     |  \n",
      "     |  blend(self, rhot, other, targetsize=None)\n",
      "     |      Given LdaState `other`, merge it with the current state. Stretch both to\n",
      "     |      `targetsize` documents before merging, so that they are of comparable\n",
      "     |      magnitude.\n",
      "     |      \n",
      "     |      Merging is done by average weighting: in the extremes, `rhot=0.0` means\n",
      "     |      `other` is completely ignored; `rhot=1.0` means `self` is completely ignored.\n",
      "     |      \n",
      "     |      This procedure corresponds to the stochastic gradient update from Hoffman\n",
      "     |      et al., algorithm 2 (eq. 14).\n",
      "     |  \n",
      "     |  blend2(self, rhot, other, targetsize=None)\n",
      "     |      Alternative, more simple blend.\n",
      "     |  \n",
      "     |  get_Elogbeta(self)\n",
      "     |  \n",
      "     |  get_lambda(self)\n",
      "     |  \n",
      "     |  merge(self, other)\n",
      "     |      Merge the result of an E step from one node with that of another node\n",
      "     |      (summing up sufficient statistics).\n",
      "     |      \n",
      "     |      The merging is trivial and after merging all cluster nodes, we have the\n",
      "     |      exact same result as if the computation was run on a single node (no\n",
      "     |      approximation).\n",
      "     |  \n",
      "     |  reset(self)\n",
      "     |      Prepare the state for a new EM iteration (reset sufficient stats).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=2)\n",
      "     |      Save the object to file (also see `load`).\n",
      "     |      \n",
      "     |      `fname_or_handle` is either a string specifying the file name to\n",
      "     |      save to, or an open file-like object which can be written to. If\n",
      "     |      the object is a file handle, no special array handling will be\n",
      "     |      performed; all attributes will be saved to the same file.\n",
      "     |      \n",
      "     |      If `separately` is None, automatically detect large\n",
      "     |      numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |      them into separate files. This avoids pickle memory errors and\n",
      "     |      allows mmap'ing large arrays back on load efficiently.\n",
      "     |      \n",
      "     |      You can also set `separately` manually, in which case it must be\n",
      "     |      a list of attribute names to be stored in separate files. The\n",
      "     |      automatic check is not performed in this case.\n",
      "     |      \n",
      "     |      `ignore` is a set of attribute names to *not* serialize (file\n",
      "     |      handles, caches etc). On subsequent load() these attributes will\n",
      "     |      be set to None.\n",
      "     |      \n",
      "     |      `pickle_protocol` defaults to 2 so the pickled object can be imported\n",
      "     |      in both Python 2 and 3.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load a previously saved object from file (also see `save`).\n",
      "     |      \n",
      "     |      If the object was saved with large arrays stored separately, you can load\n",
      "     |      these arrays via mmap (shared memory) using `mmap='r'`. Default: don't use\n",
      "     |      mmap, load large arrays as normal objects.\n",
      "     |      \n",
      "     |      If the file being loaded is compressed (either '.gz' or '.bz2'), then\n",
      "     |      `mmap=None` must be set.  Load will raise an `IOError` if this condition\n",
      "     |      is encountered.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    dirichlet_expectation(alpha)\n",
      "        For a vector `theta~Dir(alpha)`, compute `E[log(theta)]`.\n",
      "    \n",
      "    update_dir_prior(prior, N, logphat, rho)\n",
      "        Updates a given prior using Newton's method, described in\n",
      "        **Huang: Maximum Likelihood Estimation of Dirichlet Distribution Parameters.**\n",
      "        http://jonathan-huang.org/research/dirichlet/dirichlet.pdf\n",
      "\n",
      "DATA\n",
      "    gammaln = <ufunc 'gammaln'>\n",
      "    logger = <logging.Logger object>\n",
      "    psi = <ufunc 'psi'>\n",
      "\n",
      "FILE\n",
      "    d:\\360downloads\\anaconda3\\lib\\site-packages\\gensim-0.12.4-py3.4-win-amd64.egg\\gensim\\models\\ldamodel.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "help(models)\n",
    "help(models.ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5c6490174dd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "help(models.ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704\n",
      "617\n",
      "617\n",
      "422\n",
      "\n",
      "\n",
      "137\n",
      "\n",
      "\n",
      "553\n",
      "\n",
      "\n",
      "704\n"
     ]
    }
   ],
   "source": [
    "#-*- coding:utf-8 –*-\n",
    "import os\n",
    "import jieba\n",
    "all_words={}\n",
    "f_stop = open('stopwords.txt')\n",
    "try:\n",
    "    f_stop_text = f_stop.read()\n",
    "    \n",
    "finally:\n",
    "    f_stop.close()\n",
    "\n",
    "f_stop_seg_list = f_stop_text.split('\\n')\n",
    "\n",
    "#print(f_stop_seg_list)\n",
    "\n",
    "doc = []\n",
    "for i in range(3):\n",
    "    doc.append([])\n",
    "    path = os.getcwd() + \"\\\\data\\\\1\"+str(i)+\".txt\"\n",
    "    f = open(path)\n",
    "    try:\n",
    "        f_text = f.read()\n",
    "\n",
    "    finally:\n",
    "        f.close()  \n",
    "    #print(f_text)\n",
    "    f_seg_list = jieba.cut(f_text)\n",
    "    \n",
    "   \n",
    "    word_id = 0    \n",
    "   \n",
    "    \n",
    "    for myword in f_seg_list:\n",
    "        \n",
    "        if not(myword.split() in f_stop_seg_list)and len(myword.strip())>1:\n",
    "            \n",
    "            all_words.setdefault(myword,0)\n",
    "            doc[i].append(myword)\n",
    "            #all_words[myword] += 1\n",
    "    term_id = 0\n",
    "    for key in all_words.keys():\n",
    "        all_words[key] = term_id\n",
    "        term_id += 1\n",
    "    \n",
    "#print(doc)\n",
    "print(len(all_words))\n",
    "print(all_words['政府部门'])\n",
    "print(all_words[doc[0][0]])\n",
    "nt = [[0 for t in range(10)] for term in range(704)]\n",
    "for m in range(3):\n",
    "    print(len(doc[m]))\n",
    "    for n in range(len(doc[m])):\n",
    "        nt.append(1)\n",
    "        term = all_words.get(doc[m][n])\n",
    "        #nt[term][topic] += 1\n",
    "        #print(term,end = \" \")\n",
    "    print(\"\\n\")\n",
    "#print(all_words)\n",
    "#print(nt[term])\n",
    "\n",
    "\n",
    "#print(all_words.values())    \n",
    "sortdic = list(all_words.items())\n",
    "sortdic.sort(key = lambda i:i[1],reverse = True)\n",
    "print(len(sortdic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-592ce6b15dc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'gensim' is not defined"
     ]
    }
   ],
   "source": [
    "help(gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\dell\\AppData\\Local\\Temp\\jieba.cache\n",
      "DEBUG:jieba:Loading model from cache C:\\Users\\dell\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.796 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.796 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading...\n",
      "working\n",
      "...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................{'5.2': 4, '97': 1, '文章': 1, '相关': 2, '1.5': 2, '问题': 1, '傻子': 1, '主持': 1, '出租车': 1, '几天': 1, '791': 1, '注意': 1, '下来': 1, '医疗': 3, '人士': 1, '总在': 1, '出处': 1, '所有': 1, '何况': 1, '作者': 1, '狠狠': 1, '实行': 2, '绝大多数': 1, '普通': 1, '保险公司': 4, '因素': 1, '现在': 2, '中国消费者协会': 1, '每车': 1, '票价': 1, '恐怕': 1, '数字': 1, '高收': 1, '时有': 1, '采访': 1, '两千多个': 1, '利润': 1, '保护': 1, '2000': 2, '工作': 1, '地挤': 1, '大限': 1, '赔偿': 4, '进一步': 1, '双方': 1, '不知情': 1, '计算': 1, '留神': 1, '根据': 2, '决不会': 1, '元搏': 1, '一半': 1, '听证会': 1, '仅收': 1, '初步': 1, '发生': 3, '价格': 2, '机动车': 3, '仅仅': 2, '之前': 1, '一经': 1, '不盈': 1, '只要': 1, '置评': 1, '3000': 1, '最大': 1, '武高汉': 1, '不管': 2, '保障': 1, '价钱': 1, '透露': 1, '万起': 2, '多收': 1, '说道': 1, '准备': 1, '不过': 2, '并存': 1, '已经': 2, '万人': 2, '缴纳': 2, '是不是': 2, '到位': 1, '否则': 2, '付出': 1, '所谓': 1, '记者': 1, '汽车': 3, '预期': 1, '认同': 1, '站不住脚': 1, '生产': 1, '共计': 1, '属于': 1, '传闻': 1, '虽然': 1, '还有': 1, '财产': 2, '就是': 2, '一定': 3, '更大': 1, '收取': 3, '当即': 1, '估算': 2, '由多收': 1, '赔付率': 2, '怎样': 1, '北京': 1, '猜测': 1, '接受': 1, '剩下': 1, '11': 1, '死伤': 1, '人数': 1, '保险条例': 1, '以上': 1, '三峡大坝': 1, '来说': 1, '只多缴': 1, '笔者': 1, '非要': 1, '哪怕': 1, '原则': 1, '一车': 1, '理由': 1, '不难': 1, '审批': 1, '结论': 1, '这个': 3, '保费': 14, '保险业': 1, '垫付': 1, '相当': 1, '本来': 1, '500': 4, '紧张': 1, '46': 2, '也就是说': 1, '听证': 1, '700': 1, '转载': 1, '车主': 5, '范围': 3, '1000': 1, '的确': 1, '油价': 1, '正常': 1, '随着': 1, '辟谣': 1, '保险费用': 1, '秘书长': 1, '死亡': 4, '注明': 1, '毕竟': 1, '水分': 1, '1800': 10, '更进一步': 1, '险种': 3, '总的来看': 1, '财产损失': 2, '无法': 1, '投保': 6, '区区': 1, '商业': 7, '有关': 2, '强制保险': 6, '过错': 2, '与此同时': 1, '45': 2, '为此': 1, '投保人': 1, '10': 4, '景点': 1, '辆车': 2, '抢银行': 3, '临近': 1, '大大的': 1, '最终': 1, '行为': 1, '财险': 1, '而且': 1, '事故': 4, '过去': 1, '一分': 1, '排除': 1, '有没有': 1, '全国': 2, '必须': 2, '国家': 2, '细节': 1, '标准': 1, '决定': 1, '损失赔偿': 2, '版权': 2, '保额': 1, '责任': 7, '100': 3, '险大': 1, '消息': 3, '类似': 1, '险是': 1, '考量': 1, '同时': 2, '予以': 1, '传出': 1, '不到': 1, '钱包': 1, '原来': 2, '至少': 1, '假想': 1, '要求': 1, '这么': 1, '亿辆': 1, '费用': 3, '2600': 1, '2700': 2, '18.8': 1, '对于': 1, '直接': 1, '盈利': 1, '法律': 1, '核算': 1, '保监会': 3, '不是': 4, '表示': 1, '婉拒': 1, '三者': 10, '价却': 1, '概率': 1, '当然': 1, '提高': 1, '这时': 1, '安全': 1, '部分': 1, '50': 3, '权益': 2, '增加': 2, '保有量': 1, '必将': 1, '搜狐': 3, '负担': 1, '即便': 1, '如此': 1, '多一点': 1, '大为': 1, '基金': 1, '本人': 1, '福音': 1, '30': 3, '去年': 1, '可以': 2, '仅为': 1, '最多': 1, '公里': 1, '不会': 1, '事情': 1, '这里': 1, '对此': 1, '日前': 1, '各位': 1, '整体': 1, '总额': 1, '1200': 1, '独家': 1, '应该': 5, '可是': 1, '公司': 2, '观点': 1, '以及': 1, '强制': 18, '限额': 9, '交通事故': 3, '可能': 1, '车险': 1, '1500': 1, '上面': 1, '先行': 1, '要待': 1, '保险费': 1, '吊销执照': 1, '管理局': 1, '2005': 1, '40': 1, '纷纷': 1, '负责人': 2, '亏损': 1, '受伤': 1, '踊跃': 1, '宣称': 1, '按照': 2, '持平': 1, '2006': 2, '非盈利': 1, '条款': 1, '噩耗': 1, '赔付': 4, '一个': 1, '伤残': 3, '发现': 2, '广大': 1, '日须': 1, '七种': 1, '29': 1, '传来': 1, '不同': 1, '超过': 2, '相距甚远': 1, '幅度': 1, '方面': 2, '同样': 1, '说法': 1, '我们': 3, '好处': 1, '赔偿金': 2, '60': 2, '不亏': 1, '后者': 1, '平均': 1, '稿件': 1, '进行': 1, '目前': 1, '最高': 4, '沉重': 1, '声明': 1, '可太多': 1, '涉及': 1, '好心': 1, '不止': 1, '无关': 1, '通过': 1, '合法': 1, '道路': 1, '显示': 1, '想到': 1, '优惠': 2, '如果': 5, '最低': 2, '天价': 3, '反而': 1, '计较': 1, '合理': 1, '驾驶员': 2, '追究': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n          \\n    mytest1_words = copy.deepcopy(test_words)\\n    for myword in ftest1_seg_list:\\n        print (\\'.\\',end=\"\")\\n        if not(myword.strip() in f_stop_seg_list):\\n            if myword in mytest1_words.keys():\\n            #if mytest1_words.has_key(myword):\\n                mytest1_words[myword]+= 1\\n    mytest2_words = copy.deepcopy(test_words)\\n    for myword in ftest2_seg_list:\\n        print (\\'.\\',end =\"\")\\n      \\n        if not(myword.strip() in f_stop_seg_list):\\n            if myword in mytest1_words.keys():\\n            #if mytest2_words.has_key(myword):\\n                mytest2_words[myword]+= 1\\n                \\n    sampdata=[]\\n    test1data=[]\\n    test2data=[]\\n    for key in all_words.keys():\\n        sampdata.append(all_words[key])\\n        test1data.append(mytest1_words[key])\\n        test2data.append(mytest2_words[key])\\n    test1simi = get_cossimi(sampdata,test1data)\\n    test2simi = get_cossimi(sampdata,test2data)\\n\\n    print(u\"\\n%s与样本[%s]的余弦相似度：%f\" % (ftest1fn,sampfn,test1simi))\\n    print(u\"%s与样本[%s]的余弦相似度：%f\" % (ftest2fn,sampfn,test2simi))  \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "import copy\n",
    "ftest1fn = '10.txt'\n",
    "ftest2fn = '11.txt'\n",
    "sampfn = '12.txt'\n",
    "\n",
    "def get_cossimi(x,y):\n",
    "    myx = np.array(x)\n",
    "    myy = np.array(y)\n",
    "    cos1 = np.sum(myx*myy)\n",
    "    cos21 =  np.sqrt(sum(myx*myx))\n",
    "    cos22 = np.sqrt(sum(myy*myy))\n",
    "    return cos1/float(cos21*cos22)\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "print()\n",
    "print('loading...')\n",
    "print('working')\n",
    "f1 = open(sampfn)\n",
    "try:\n",
    "    f1_text = f1.read()\n",
    "finally:\n",
    "    f1.close()\n",
    "f1_seg_list = jieba.cut(f1_text)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ftest1 = open(ftest1fn)\n",
    "try:\n",
    "    ftest1_text = ftest1.read()\n",
    "finally:\n",
    "    ftest1.close()\n",
    "ftest1_seg_list = jieba.cut(ftest1_text)\n",
    "\n",
    "\n",
    "\n",
    "ftest2 = open(ftest2fn)\n",
    "try:\n",
    "    ftest2_text = ftest2.read()\n",
    "finally:\n",
    "    ftest2.close()\n",
    "ftest2_seg_list = jieba.cut(ftest2_text)\n",
    "\n",
    "\"\"\"\n",
    "f_stop = open('stopwords.txt')\n",
    "try:\n",
    "    f_stop_text = f_stop.read()\n",
    "\n",
    "finally:\n",
    "    f_stop.close()\n",
    "\n",
    "f_stop_seg_list = f_stop_text.split('\\n')\n",
    "\n",
    "test_words = {}\n",
    "all_words = {}\n",
    "\n",
    "for myword in f1_seg_list:\n",
    "    print('.',end=\"\")\n",
    "    if not(myword.split() in f_stop_seg_list)and len(myword.strip())>1:\n",
    "        test_words.setdefault(myword,0)\n",
    "        all_words.setdefault(myword,0)\n",
    "        all_words[myword] += 1\n",
    "print(all_words)\n",
    "\"\"\"\n",
    "          \n",
    "    mytest1_words = copy.deepcopy(test_words)\n",
    "    for myword in ftest1_seg_list:\n",
    "        print ('.',end=\"\")\n",
    "        if not(myword.strip() in f_stop_seg_list):\n",
    "            if myword in mytest1_words.keys():\n",
    "            #if mytest1_words.has_key(myword):\n",
    "                mytest1_words[myword]+= 1\n",
    "    mytest2_words = copy.deepcopy(test_words)\n",
    "    for myword in ftest2_seg_list:\n",
    "        print ('.',end =\"\")\n",
    "      \n",
    "        if not(myword.strip() in f_stop_seg_list):\n",
    "            if myword in mytest1_words.keys():\n",
    "            #if mytest2_words.has_key(myword):\n",
    "                mytest2_words[myword]+= 1\n",
    "                \n",
    "    sampdata=[]\n",
    "    test1data=[]\n",
    "    test2data=[]\n",
    "    for key in all_words.keys():\n",
    "        sampdata.append(all_words[key])\n",
    "        test1data.append(mytest1_words[key])\n",
    "        test2data.append(mytest2_words[key])\n",
    "    test1simi = get_cossimi(sampdata,test1data)\n",
    "    test2simi = get_cossimi(sampdata,test2data)\n",
    "\n",
    "    print(u\"\\n%s与样本[%s]的余弦相似度：%f\" % (ftest1fn,sampfn,test1simi))\n",
    "    print(u\"%s与样本[%s]的余弦相似度：%f\" % (ftest2fn,sampfn,test2simi))  \n",
    "\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "L = []\n",
    "k = [1,2,1,2]\n",
    "for x in range(1, 11):\n",
    "    L.append(x * x)\n",
    "    L.append(k)\n",
    "print(L[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0]\n",
      "[3, 2, 1]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[[0, 0, 0], [3, 2, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
      "Help on built-in function sort:\n",
      "\n",
      "sort(...) method of builtins.list instance\n",
      "    L.sort(key=None, reverse=False) -> None -- stable sort *IN PLACE*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "theta = [[0 for t in range(3)] for d in range(5)]\n",
    "theta[1] = [1,2,3]\n",
    "for m in range(5):\n",
    "    theta[m].sort(reverse = True)\n",
    "    print(theta[m])\n",
    "\n",
    "print(theta)\n",
    "help(theta.sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#实例4: 对第二个关键字排序 \n",
    "\n",
    "L = [('b',6),('a',1),('c',3),('d',4)]\n",
    "\n",
    "L.sort(key=lambda x:x[1]) \n",
    "\n",
    "L\n",
    "\n",
    "[('a', 1), ('c', 3), ('d', 4), ('b', 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "L = [('b',6),('a',1),('c',3),('d',4)]\n",
    "\n",
    "L.sort(key=lambda x:x[1]) \n",
    "\n",
    "print(L[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "#遍历文件夹：\n",
    "import os\n",
    "want_dir = '.'\n",
    "for root, dirs, files in os.walk(want_dir):\n",
    "        for file in files:\n",
    "                if os.path.splitext(file)[1] == '.py':\n",
    "                        filename = os.path.join(root,file)\n",
    "                        print filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module sklearn.lda in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.lda - Linear Discriminant Analysis (LDA)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        LDA(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.base.TransformerMixin)\n",
      "    sklearn.base.TransformerMixin(builtins.object)\n",
      "        LDA(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.base.TransformerMixin)\n",
      "    sklearn.linear_model.base.LinearClassifierMixin(sklearn.base.ClassifierMixin)\n",
      "        LDA(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.base.TransformerMixin)\n",
      "    \n",
      "    class LDA(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.base.TransformerMixin)\n",
      "     |  Linear Discriminant Analysis (LDA).\n",
      "     |  \n",
      "     |  A classifier with a linear decision boundary, generated by fitting class\n",
      "     |  conditional densities to the data and using Bayes' rule.\n",
      "     |  \n",
      "     |  The model fits a Gaussian density to each class, assuming that all classes\n",
      "     |  share the same covariance matrix.\n",
      "     |  \n",
      "     |  The fitted model can also be used to reduce the dimensionality of the input\n",
      "     |  by projecting it to the most discriminative directions.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  solver : string, optional\n",
      "     |      Solver to use, possible values:\n",
      "     |        - 'svd': Singular value decomposition (default). Does not compute the\n",
      "     |              covariance matrix, therefore this solver is recommended for\n",
      "     |              data with a large number of features.\n",
      "     |        - 'lsqr': Least squares solution, can be combined with shrinkage.\n",
      "     |        - 'eigen': Eigenvalue decomposition, can be combined with shrinkage.\n",
      "     |  \n",
      "     |  shrinkage : string or float, optional\n",
      "     |      Shrinkage parameter, possible values:\n",
      "     |        - None: no shrinkage (default).\n",
      "     |        - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n",
      "     |        - float between 0 and 1: fixed shrinkage parameter.\n",
      "     |  \n",
      "     |      Note that shrinkage works only with 'lsqr' and 'eigen' solvers.\n",
      "     |  \n",
      "     |  priors : array, optional, shape (n_classes,)\n",
      "     |      Class priors.\n",
      "     |  \n",
      "     |  n_components : int, optional\n",
      "     |      Number of components (< n_classes - 1) for dimensionality reduction.\n",
      "     |  \n",
      "     |  store_covariance : bool, optional\n",
      "     |      Additionally compute class covariance matrix (default False).\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Threshold used for rank estimation in SVD solver.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,) or (n_classes, n_features)\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : array, shape (n_features,)\n",
      "     |      Intercept term.\n",
      "     |  \n",
      "     |  covariance_ : array-like, shape (n_features, n_features)\n",
      "     |      Covariance matrix (shared by all classes).\n",
      "     |  \n",
      "     |  means_ : array-like, shape (n_classes, n_features)\n",
      "     |      Class means.\n",
      "     |  \n",
      "     |  priors_ : array-like, shape (n_classes,)\n",
      "     |      Class priors (sum to 1).\n",
      "     |  \n",
      "     |  scalings_ : array-like, shape (rank, n_classes - 1)\n",
      "     |      Scaling of the features in the space spanned by the class centroids.\n",
      "     |  \n",
      "     |  xbar_ : array-like, shape (n_features,)\n",
      "     |      Overall mean.\n",
      "     |  \n",
      "     |  classes_ : array-like, shape (n_classes,)\n",
      "     |      Unique class labels.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  sklearn.qda.QDA: Quadratic discriminant analysis\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default solver is 'svd'. It can perform both classification and\n",
      "     |  transform, and it does not rely on the calculation of the covariance\n",
      "     |  matrix. This can be an advantage in situations where the number of features\n",
      "     |  is large. However, the 'svd' solver cannot be used with shrinkage.\n",
      "     |  \n",
      "     |  The 'lsqr' solver is an efficient algorithm that only works for\n",
      "     |  classification. It supports shrinkage.\n",
      "     |  \n",
      "     |  The 'eigen' solver is based on the optimization of the between class\n",
      "     |  scatter to within class scatter ratio. It can be used for both\n",
      "     |  classification and transform, and it supports shrinkage. However, the\n",
      "     |  'eigen' solver needs to compute the covariance matrix, so it might not be\n",
      "     |  suitable for situations with a high number of features.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.lda import LDA\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      "     |  >>> y = np.array([1, 1, 1, 2, 2, 2])\n",
      "     |  >>> clf = LDA()\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  LDA(n_components=None, priors=None, shrinkage=None, solver='svd',\n",
      "     |    store_covariance=False, tol=0.0001)\n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LDA\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, solver='svd', shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001)\n",
      "     |  \n",
      "     |  fit(self, X, y, store_covariance=False, tol=0.0001)\n",
      "     |      Fit LDA model according to the given training data and parameters.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array, shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Estimate log probability.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples, n_classes)\n",
      "     |          Estimated log probabilities.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Estimate probability.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples, n_classes)\n",
      "     |          Estimated probabilities.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Project data to maximize class separation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : array, shape (n_samples, n_components)\n",
      "     |          Transformed data.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep: boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The former have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['LDA']\n",
      "\n",
      "FILE\n",
      "    d:\\360downloads\\anaconda3\\lib\\site-packages\\sklearn\\lda.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "from sklearn import lda\n",
    "help(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on dict object:\n",
      "\n",
      "class dict(object)\n",
      " |  dict() -> new empty dictionary\n",
      " |  dict(mapping) -> new dictionary initialized from a mapping object's\n",
      " |      (key, value) pairs\n",
      " |  dict(iterable) -> new dictionary initialized as if via:\n",
      " |      d = {}\n",
      " |      for k, v in iterable:\n",
      " |          d[k] = v\n",
      " |  dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
      " |      in the keyword argument list.  For example:  dict(one=1, two=2)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if D has a key k, else False.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  copy(...)\n",
      " |      D.copy() -> a shallow copy of D\n",
      " |  \n",
      " |  fromkeys(iterable, value=None, /) from builtins.type\n",
      " |      Returns a new dict with keys from iterable and values equal to value.\n",
      " |  \n",
      " |  get(...)\n",
      " |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(...)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      " |      2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  setdefault(...)\n",
      " |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
      " |  \n",
      " |  update(...)\n",
      " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n",
      "{'two': 2, 'one': 1}\n",
      "{'two': 2, 'one': 1}\n"
     ]
    }
   ],
   "source": [
    "dic = {}\n",
    "help(dic)\n",
    "dic = dict(one=1, two =2)\n",
    "print(dic)\n",
    "\n",
    "(x for x in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
