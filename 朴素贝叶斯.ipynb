{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.算法描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）获取样本文件，将样本人工分类，并标记\n",
    "\n",
    "（2）每个类别的样本中文分词\n",
    "\n",
    "（3）去除样本中垃圾词条\n",
    "\n",
    "（4）合成特征组，分析计算词频信息\n",
    "\n",
    "（5）计算先验概率\n",
    "\n",
    "（6）读取未知样本，中文分词，去垃圾词，形成样本特征值\n",
    "\n",
    "（7）计算后验概率，得到最大概率所属类别即为文本所属类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'LiFeiteng'  \n",
    "02.# -*- coding: utf-8 -*-  \n",
    "03.import os  \n",
    "04.import  jieba  \n",
    "05.import nltk  \n",
    "06.  \n",
    "07.  \n",
    "08.## 由搜狗语料库 生成数据  \n",
    "09.folder_path = 'C:\\LIFEITENG\\SogouC.reduced\\\\Reduced'  \n",
    "10.#folder_path = 'C:\\LIFEITENG\\SogouC.mini\\Sample'  \n",
    "11.folder_list = os.listdir(folder_path)  \n",
    "12.class_list = [] ##由于乱码等问题 仅以数字[0,1,...]来代表文件分类  \n",
    "13.nClass = 0  \n",
    "14.N = 100 #每类文件 最多取 100 个样本 70%train 30%test  \n",
    "15.train_set = []  \n",
    "16.test_set = []  \n",
    "17.all_words = {}  \n",
    "18.import time  \n",
    "19.process_times = [] ## 统计处理每个文件的时间  \n",
    "20.for i in range(len(folder_list)):  \n",
    "21.    new_folder_path = folder_path + '\\\\' + folder_list[i]  \n",
    "22.    files = os.listdir(new_folder_path)  \n",
    "23.    class_list.append(nClass)  \n",
    "24.    nClass += 1  \n",
    "25.    j = 0  \n",
    "26.    nFile = min([len(files), N])  \n",
    "27.    for file in files:  \n",
    "28.        if j > N:  \n",
    "29.            break  \n",
    "30.        starttime = time.clock()  \n",
    "31.  \n",
    "32.        fobj = open(new_folder_path+'\\\\'+file, 'r')  \n",
    "33.        raw = fobj.read()  \n",
    "34.        word_cut = jieba.cut(raw, cut_all=False)  \n",
    "35.        word_list = list(word_cut)  \n",
    "36.        for word in word_list:  \n",
    "37.            if word in all_words.keys():  \n",
    "38.                all_words[word] += 1  \n",
    "39.            else:  \n",
    "40.                all_words[word] = 0  \n",
    "41.        if j > 0.3 * nFile:  \n",
    "42.            train_set.append((word_list, class_list[i]))  \n",
    "43.        else:  \n",
    "44.            test_set.append((word_list, class_list[i]))  \n",
    "45.        j += 1  \n",
    "46.        endtime = time.clock()  \n",
    "47.        process_times.append(endtime-starttime)  \n",
    "48.  \n",
    "49.        print \"Folder \",i,\"-file-\",j, \"all_words length = \", len(all_words.keys()),\\  \n",
    "50.            \"process time:\",(endtime-starttime)  \n",
    "51.  \n",
    "52.  \n",
    "53.print len(all_words)  \n",
    "54.  \n",
    "55.## 根据word的词频排序  \n",
    "56.all_words_list = sorted(all_words.items(), key=lambda e:e[1], reverse=True)  \n",
    "57.word_features = []  \n",
    "58.## 由于乱码的问题，没有正确使用 stopwords；简单去掉 前100个高频项  \n",
    "59.## word_features 是选用的 word-词典  \n",
    "60.for t in range(100, 1100, 1):  \n",
    "61.    word_features.append(all_words_list[t][0])  \n",
    "62.  \n",
    "63.def document_features(document):  \n",
    "64.    document_words = set(document)  \n",
    "65.    features = {}  \n",
    "66.    for word in word_features: ## 根据词典生成 每个document的feature True or False  \n",
    "67.        features['contains(%s)' % word] = (word in document_words)  \n",
    "68.    return features  \n",
    "69.  \n",
    "70.## 根据每个document 分词生成的 word_list 生成 feature  \n",
    "71.train_data = [(document_features(d), c) for (d,c) in train_set]  \n",
    "72.test_data = [(document_features(d), c) for (d,c) in test_set]  \n",
    "73.print \"train number:\",len(train_data),\"\\n test number:\",len(test_data)  \n",
    "74.  \n",
    "75.## 朴素贝叶斯分类器  \n",
    "76.classifier = nltk.NaiveBayesClassifier.train(train_data)  \n",
    "77.print \"test accuracy:\",nltk.classify.accuracy(classifier, test_data)  \n",
    "78.  \n",
    "79.## 处理每个文件所用的时间 可见到后面 处理单个文件的时间显著增长  \n",
    "80.## 原因 已查明  \n",
    "81.import pylab  \n",
    "82.pylab.plot(range(len(process_times)), process_times, 'b.')  \n",
    "83.pylab.show()  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
