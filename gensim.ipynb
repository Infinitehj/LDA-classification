{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "knn score: 0.801507537688\n",
    "svm score: 0.809603573423\n",
    "tree score: 0.721105527638\n",
    "bayes score: 0.796761585706\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the dataset there are 100 textual documents\n",
      "In the corpus there are 11490 unique tokens\n",
      "Running time: 3.0731047387125727 Seconds\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.WARNING)\n",
    "logging.root.level = logging.WARNING\n",
    "import os\n",
    "import jieba\n",
    "import codecs\n",
    "import time\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "start = time.clock()\n",
    "def read_documents(file):\n",
    "    documents = []\n",
    "    walk = os.walk(file)\n",
    "    for root ,dirs, files in walk:\n",
    "        for name in files:\n",
    "            filename = os.path.join(root, name)\n",
    "            try:\n",
    "                f = open(filename)\n",
    "                #f = codecs.open(filename,'r','utf-8','ignore')\n",
    "                text = f.read()\n",
    "            except:\n",
    "                f = codecs.open(filename,'r','gbk','ignore')    \n",
    "                text = f.read()\n",
    "            finally:\n",
    "                f.close()\n",
    "\n",
    "            documents.append(text)\n",
    "    return documents\n",
    "#documents = news_dataset.data\n",
    "documents = read_documents('data')\n",
    "print (\"In the dataset there are\", len(documents), \"textual documents\")\n",
    "#print (\"And this is the first one:\\n\", documents[0])\n",
    "        \n",
    "def load_stopwords():\n",
    "    f_stop = open('stopwords.txt')\n",
    "    try:\n",
    "        f_stop_text = f_stop.read()\n",
    "\n",
    "    finally:\n",
    "        f_stop.close()\n",
    "\n",
    "    f_stop_seg_list = f_stop_text.split('\\n')\n",
    "    return f_stop_seg_list\n",
    "\n",
    "def tokenize(text):\n",
    "    stopwords = load_stopwords()    \n",
    "    return [token for token in jieba.cut(text) if token.strip() not in stopwords and len(token.strip()) > 1]\n",
    "\n",
    "#print (\"After the tokenizer, the previous document becomes:\\n\", tokenize(documents[0]))\n",
    "\n",
    "\n",
    "processed_docs = [tokenize(doc) for doc in documents]\n",
    "word_count_dict = gensim.corpora.Dictionary(processed_docs)\n",
    "print (\"In the corpus there are\", len(word_count_dict), \"unique tokens\")\n",
    "\n",
    "#word_count_dict.filter_extremes(no_below=20, no_above=0.1)\n",
    "#print (\"After filtering, in the corpus there are only\", len(word_count_dict), \"unique tokens\")\n",
    "\n",
    "bag_of_words_corpus = [word_count_dict.doc2bow(pdoc) for pdoc in processed_docs]\n",
    "\n",
    "#bow_doc1 = bag_of_words_corpus[0]\n",
    "#print (\"Bag of words representation of the first document (tuples are composed by token_id and multiplicity):\\n\", bow_doc1)\n",
    "#print\n",
    "#for i in range(5):\n",
    "#    print (\"In the document, topic_id {} (word \\\"{}\\\") appears {} time[s]\".format(bow_doc1[i][0], word_count_dict[bow_doc1[i][0]], bow_doc1[i][1]))\n",
    "#print (\"...\")\n",
    "\n",
    "\n",
    "\n",
    "end = time.clock()\n",
    "print('Running time: %s Seconds'%(end-start))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LDA mono-core\n",
    "lda_model = gensim.models.LdaModel(bag_of_words_corpus, num_topics=100,iterations=1000, id2word=word_count_dict, passes=5,minimum_probability=0)\n",
    "\n",
    "# LDA multicore (in this configuration, defaulty, uses n_cores-1)\n",
    "# lda_model = gensim.models.LdaMulticore(bag_of_words_corpus, num_topics=10, id2word=word_count_dict, passes=5)\n",
    "\n",
    "#_ = lda_model.print_topics(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.clock()\n",
    "#for index, score in sorted(lda_model[bag_of_words_corpus[0]], key=lambda tup: -1*tup[1]):\n",
    "#    print (\"Score: {}\\t Index: {}\\t \\nTopic: {}\".format(score, index, lda_model.print_topic(index, 5)))\n",
    "\n",
    "\n",
    "unseen_documents = read_documents('test')\n",
    "print (\"In the test dataset there are\", len(unseen_documents), \"unseen textual documents\")\n",
    "\n",
    "processed_docs = [tokenize(doc) for doc in unseen_documents]\n",
    "#word_count_dict = gensim.corpora.Dictionary(processed_docs)\n",
    "#print (\"In the corpus there are\", len(word_count_dict), \"unique tokens\")\n",
    "\n",
    "#word_count_dict.filter_extremes(no_below=20, no_above=0.1)\n",
    "#print (\"After filtering, in the corpus there are only\", len(word_count_dict), \"unique tokens\")\n",
    "\n",
    "\n",
    "bow_vector = [word_count_dict.doc2bow(pdoc) for pdoc in processed_docs]\n",
    "\n",
    "\n",
    "end = time.clock()\n",
    "print('Running time: %s Seconds'%(end-start)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 11.32786949851743 Seconds\n",
      "Running time: 14.100854407528573 Seconds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running time: 16.646045065655926 Seconds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running time: 17.530466260282992 Seconds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running time: 19.897350404789563 Seconds\n",
      "[-10.369866605991547, -10.130334107830127, -10.443193078925614, -10.821319194182648, -11.26401110775145]\n"
     ]
    }
   ],
   "source": [
    "lper = []\n",
    "topiclist=[10,50,100,150,200]\n",
    "for ntopic in topiclist:\n",
    "    start = time.clock()\n",
    "    lda_model = gensim.models.LdaModel(bag_of_words_corpus, num_topics=ntopic, iterations=1000,id2word=word_count_dict, passes=5)\n",
    "    lper.append(lda_model.log_perplexity(bag_of_words_corpus))\n",
    "    end = time.clock()\n",
    "    print('Running time: %s Seconds'%(end-start))\n",
    "print(lper)\n",
    "# LDA multicore (in this configuration, defaulty, uses n_cores-1)\n",
    "# lda_model = gensim.models.LdaMulticore(bag_of_words_corpus, num_topics=10, id2word=word_count_dict, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 16.166576832021065 Seconds\n",
      "Running time: 16.225584031245262 Seconds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running time: 20.815404377006644 Seconds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running time: 24.928525421425775 Seconds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running time: 27.505073365416365 Seconds\n",
      "[-10.449027262906808, -10.212015838726003, -9.8670069794212605, -9.9090898258201268, -9.9135197508951673]\n"
     ]
    }
   ],
   "source": [
    "# LDA mono-core\n",
    "\n",
    "lper2 = []\n",
    "iteration =[50,100,500,1000,2000]\n",
    "for iternum in iteration:\n",
    "    start = time.clock()\n",
    "    lda_model = gensim.models.LdaModel(bag_of_words_corpus, num_topics=100, iterations=iternum,id2word=word_count_dict, passes=5)\n",
    "    lper2.append(lda_model.log_perplexity(bag_of_words_corpus))\n",
    "    end = time.clock()\n",
    "    print('Running time: %s Seconds'%(end-start))\n",
    "print(lper2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jieba\n",
    "import codecs\n",
    "from gensim import models\n",
    "from gensim import corpora,similarities\n",
    "\n",
    "def load_stopwords():\n",
    "    f_stop = open('stopwords.txt')\n",
    "    try:\n",
    "        f_stop_text = f_stop.read()\n",
    "\n",
    "    finally:\n",
    "        f_stop.close()\n",
    "\n",
    "    f_stop_seg_list = f_stop_text.split('\\n')\n",
    "    return f_stop_seg_list\n",
    "\n",
    "train_set = []\n",
    "walk = os.walk('data')\n",
    "for root ,dirs, files in walk:\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "        try:\n",
    "            f = open(filename)\n",
    "            #f = codecs.open(filename,'r','utf-8','ignore')\n",
    "            raw = f.read()\n",
    "        except:\n",
    "            f = codecs.open(filename,'r','gbk','ignore')    \n",
    "            raw = f.read()\n",
    "        finally:\n",
    "            f.close()\n",
    "        word_list = list(jieba.cut(raw, cut_all = False))\n",
    "        train_set.append(word_list)\n",
    "        \n",
    "\n",
    "    \n",
    "dic = corpora.Dictionary(train_set)\n",
    "corpus = [dic.doc2bow(text) for text in train_set]\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "lda = models.LdaModel(corpus_tfidf, id2word = dic, num_topics = 10,passes = 5)\n",
    "corpus_lda = lda[corpus_tfidf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_topics:  [10, 50, 100, 150, 200]\n",
      "log_perplexity:  [-8.40739493862402, -8.453508218120083, -8.475337133675147, -8.423509263752123, -8.410553528300738]\n"
     ]
    }
   ],
   "source": [
    "x = [10,50,100,150,200]\n",
    "y = [-8.4073949386240194, -8.4535082181200831, -8.4753371336751471, -8.4235092637521234, -8.4105535283007384]\n",
    "print(\"num_topics: \",x)\n",
    "print(\"log_perplexity: \",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1203\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "doc_type = np.arange(10746)\n",
    "\n",
    "doc_type[0:1194] = 1\n",
    "print(doc_type[1203])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList1=[0]*10\n",
    "myList1.extend([1]*10)\n",
    "for i in range(9):\n",
    "    myList.extend([i]*10)\n",
    "myList1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class list in module builtins:\n",
      "\n",
      "class list(object)\n",
      " |  list() -> new empty list\n",
      " |  list(iterable) -> new list initialized from iterable's items\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, value, /)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __iadd__(self, value, /)\n",
      " |      Implement self+=value.\n",
      " |  \n",
      " |  __imul__(self, value, /)\n",
      " |      Implement self*=value.\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mul__(self, value, /)\n",
      " |      Return self*value.n\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __reversed__(...)\n",
      " |      L.__reversed__() -- return a reverse iterator over the list\n",
      " |  \n",
      " |  __rmul__(self, value, /)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      L.__sizeof__() -- size of L in memory, in bytes\n",
      " |  \n",
      " |  append(...)\n",
      " |      L.append(object) -> None -- append object to end\n",
      " |  \n",
      " |  clear(...)\n",
      " |      L.clear() -> None -- remove all items from L\n",
      " |  \n",
      " |  copy(...)\n",
      " |      L.copy() -> list -- a shallow copy of L\n",
      " |  \n",
      " |  count(...)\n",
      " |      L.count(value) -> integer -- return number of occurrences of value\n",
      " |  \n",
      " |  extend(...)\n",
      " |      L.extend(iterable) -> None -- extend list by appending elements from the iterable\n",
      " |  \n",
      " |  index(...)\n",
      " |      L.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      " |      Raises ValueError if the value is not present.\n",
      " |  \n",
      " |  insert(...)\n",
      " |      L.insert(index, object) -- insert object before index\n",
      " |  \n",
      " |  pop(...)\n",
      " |      L.pop([index]) -> item -- remove and return item at index (default last).\n",
      " |      Raises IndexError if list is empty or index is out of range.\n",
      " |  \n",
      " |  remove(...)\n",
      " |      L.remove(value) -> None -- remove first occurrence of value.\n",
      " |      Raises ValueError if the value is not present.\n",
      " |  \n",
      " |  reverse(...)\n",
      " |      L.reverse() -- reverse *IN PLACE*\n",
      " |  \n",
      " |  sort(...)\n",
      " |      L.sort(key=None, reverse=False) -> None -- stable sort *IN PLACE*\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ndarray object:\n",
      "\n",
      "class ndarray(builtins.object)\n",
      " |  ndarray(shape, dtype=float, buffer=None, offset=0,\n",
      " |          strides=None, order=None)\n",
      " |  \n",
      " |  An array object represents a multidimensional, homogeneous array\n",
      " |  of fixed-size items.  An associated data-type object describes the\n",
      " |  format of each element in the array (its byte-order, how many bytes it\n",
      " |  occupies in memory, whether it is an integer, a floating point number,\n",
      " |  or something else, etc.)\n",
      " |  \n",
      " |  Arrays should be constructed using `array`, `zeros` or `empty` (refer\n",
      " |  to the See Also section below).  The parameters given here refer to\n",
      " |  a low-level method (`ndarray(...)`) for instantiating an array.\n",
      " |  \n",
      " |  For more information, refer to the `numpy` module and examine the\n",
      " |  the methods and attributes of an array.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  (for the __new__ method; see Notes below)\n",
      " |  \n",
      " |  shape : tuple of ints\n",
      " |      Shape of created array.\n",
      " |  dtype : data-type, optional\n",
      " |      Any object that can be interpreted as a numpy data type.\n",
      " |  buffer : object exposing buffer interface, optional\n",
      " |      Used to fill the array with data.\n",
      " |  offset : int, optional\n",
      " |      Offset of array data in buffer.\n",
      " |  strides : tuple of ints, optional\n",
      " |      Strides of data in memory.\n",
      " |  order : {'C', 'F'}, optional\n",
      " |      Row-major or column-major order.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  T : ndarray\n",
      " |      Transpose of the array.\n",
      " |  data : buffer\n",
      " |      The array's elements, in memory.\n",
      " |  dtype : dtype object\n",
      " |      Describes the format of the elements in the array.\n",
      " |  flags : dict\n",
      " |      Dictionary containing information related to memory use, e.g.,\n",
      " |      'C_CONTIGUOUS', 'OWNDATA', 'WRITEABLE', etc.\n",
      " |  flat : numpy.flatiter object\n",
      " |      Flattened version of the array as an iterator.  The iterator\n",
      " |      allows assignments, e.g., ``x.flat = 3`` (See `ndarray.flat` for\n",
      " |      assignment examples; TODO).\n",
      " |  imag : ndarray\n",
      " |      Imaginary part of the array.\n",
      " |  real : ndarray\n",
      " |      Real part of the array.\n",
      " |  size : int\n",
      " |      Number of elements in the array.\n",
      " |  itemsize : int\n",
      " |      The memory use of each array element in bytes.\n",
      " |  nbytes : int\n",
      " |      The total number of bytes required to store the array data,\n",
      " |      i.e., ``itemsize * size``.\n",
      " |  ndim : int\n",
      " |      The array's number of dimensions.\n",
      " |  shape : tuple of ints\n",
      " |      Shape of the array.\n",
      " |  strides : tuple of ints\n",
      " |      The step-size required to move from one element to the next in\n",
      " |      memory. For example, a contiguous ``(3, 4)`` array of type\n",
      " |      ``int16`` in C-order has strides ``(8, 2)``.  This implies that\n",
      " |      to move from element to element in memory requires jumps of 2 bytes.\n",
      " |      To move from row-to-row, one needs to jump 8 bytes at a time\n",
      " |      (``2 * 4``).\n",
      " |  ctypes : ctypes object\n",
      " |      Class containing properties of the array needed for interaction\n",
      " |      with ctypes.\n",
      " |  base : ndarray\n",
      " |      If the array is a view into another array, that array is its `base`\n",
      " |      (unless that array is also a view).  The `base` array is where the\n",
      " |      array data is actually stored.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  array : Construct an array.\n",
      " |  zeros : Create an array, each element of which is zero.\n",
      " |  empty : Create an array, but leave its allocated memory unchanged (i.e.,\n",
      " |          it contains \"garbage\").\n",
      " |  dtype : Create a data-type.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  There are two modes of creating an array using ``__new__``:\n",
      " |  \n",
      " |  1. If `buffer` is None, then only `shape`, `dtype`, and `order`\n",
      " |     are used.\n",
      " |  2. If `buffer` is an object exposing the buffer interface, then\n",
      " |     all keywords are interpreted.\n",
      " |  \n",
      " |  No ``__init__`` method is needed because the array is fully initialized\n",
      " |  after the ``__new__`` method.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  These examples illustrate the low-level `ndarray` constructor.  Refer\n",
      " |  to the `See Also` section above for easier ways of constructing an\n",
      " |  ndarray.\n",
      " |  \n",
      " |  First mode, `buffer` is None:\n",
      " |  \n",
      " |  >>> np.ndarray(shape=(2,2), dtype=float, order='F')\n",
      " |  array([[ -1.13698227e+002,   4.25087011e-303],\n",
      " |         [  2.88528414e-306,   3.27025015e-309]])         #random\n",
      " |  \n",
      " |  Second mode:\n",
      " |  \n",
      " |  >>> np.ndarray((2,), buffer=np.array([1,2,3]),\n",
      " |  ...            offset=np.int_().itemsize,\n",
      " |  ...            dtype=int) # offset = 1*itemsize, i.e. skip first element\n",
      " |  array([2, 3])\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __abs__(self, /)\n",
      " |      abs(self)\n",
      " |  \n",
      " |  __add__(self, value, /)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __and__(self, value, /)\n",
      " |      Return self&value.\n",
      " |  \n",
      " |  __array__(...)\n",
      " |      a.__array__(|dtype) -> reference if type unchanged, copy otherwise.\n",
      " |      \n",
      " |      Returns either a new reference to self if dtype is not given or a new array\n",
      " |      of provided data type if dtype is different from the current dtype of the\n",
      " |      array.\n",
      " |  \n",
      " |  __array_prepare__(...)\n",
      " |      a.__array_prepare__(obj) -> Object of same type as ndarray object obj.\n",
      " |  \n",
      " |  __array_wrap__(...)\n",
      " |      a.__array_wrap__(obj) -> Object of same type as ndarray object a.\n",
      " |  \n",
      " |  __bool__(self, /)\n",
      " |      self != 0\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __copy__(...)\n",
      " |      a.__copy__([order])\n",
      " |      \n",
      " |      Return a copy of the array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      order : {'C', 'F', 'A'}, optional\n",
      " |          If order is 'C' (False) then the result is contiguous (default).\n",
      " |          If order is 'Fortran' (True) then the result has fortran order.\n",
      " |          If order is 'Any' (None) then the result has fortran order\n",
      " |          only if the array already is in fortran order.\n",
      " |  \n",
      " |  __deepcopy__(...)\n",
      " |      a.__deepcopy__() -> Deep copy of array.\n",
      " |      \n",
      " |      Used if copy.deepcopy is called on an array.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __divmod__(self, value, /)\n",
      " |      Return divmod(self, value).\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __float__(self, /)\n",
      " |      float(self)\n",
      " |  \n",
      " |  __floordiv__(self, value, /)\n",
      " |      Return self//value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getitem__(self, key, /)\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __iadd__(self, value, /)\n",
      " |      Return self+=value.\n",
      " |  \n",
      " |  __iand__(self, value, /)\n",
      " |      Return self&=value.\n",
      " |  \n",
      " |  __ifloordiv__(self, value, /)\n",
      " |      Return self//=value.\n",
      " |  \n",
      " |  __ilshift__(self, value, /)\n",
      " |      Return self<<=value.\n",
      " |  \n",
      " |  __imod__(self, value, /)\n",
      " |      Return self%=value.\n",
      " |  \n",
      " |  __imul__(self, value, /)\n",
      " |      Return self*=value.\n",
      " |  \n",
      " |  __index__(self, /)\n",
      " |      Return self converted to an integer, if self is suitable for use as an index into a list.\n",
      " |  \n",
      " |  __int__(self, /)\n",
      " |      int(self)\n",
      " |  \n",
      " |  __invert__(self, /)\n",
      " |      ~self\n",
      " |  \n",
      " |  __ior__(self, value, /)\n",
      " |      Return self|=value.\n",
      " |  \n",
      " |  __ipow__(self, value, /)\n",
      " |      Return self**=value.\n",
      " |  \n",
      " |  __irshift__(self, value, /)\n",
      " |      Return self>>=value.\n",
      " |  \n",
      " |  __isub__(self, value, /)\n",
      " |      Return self-=value.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __itruediv__(self, value, /)\n",
      " |      Return self/=value.\n",
      " |  \n",
      " |  __ixor__(self, value, /)\n",
      " |      Return self^=value.\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lshift__(self, value, /)\n",
      " |      Return self<<value.\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mod__(self, value, /)\n",
      " |      Return self%value.\n",
      " |  \n",
      " |  __mul__(self, value, /)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __neg__(self, /)\n",
      " |      -self\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __or__(self, value, /)\n",
      " |      Return self|value.\n",
      " |  \n",
      " |  __pos__(self, /)\n",
      " |      +self\n",
      " |  \n",
      " |  __pow__(self, value, mod=None, /)\n",
      " |      Return pow(self, value, mod).\n",
      " |  \n",
      " |  __radd__(self, value, /)\n",
      " |      Return value+self.\n",
      " |  \n",
      " |  __rand__(self, value, /)\n",
      " |      Return value&self.\n",
      " |  \n",
      " |  __rdivmod__(self, value, /)\n",
      " |      Return divmod(value, self).\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      a.__reduce__()\n",
      " |      \n",
      " |      For pickling.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rfloordiv__(self, value, /)\n",
      " |      Return value//self.\n",
      " |  \n",
      " |  __rlshift__(self, value, /)\n",
      " |      Return value<<self.\n",
      " |  \n",
      " |  __rmod__(self, value, /)\n",
      " |      Return value%self.\n",
      " |  \n",
      " |  __rmul__(self, value, /)\n",
      " |      Return value*self.\n",
      " |  \n",
      " |  __ror__(self, value, /)\n",
      " |      Return value|self.\n",
      " |  \n",
      " |  __rpow__(self, value, mod=None, /)\n",
      " |      Return pow(value, self, mod).\n",
      " |  \n",
      " |  __rrshift__(self, value, /)\n",
      " |      Return value>>self.\n",
      " |  \n",
      " |  __rshift__(self, value, /)\n",
      " |      Return self>>value.\n",
      " |  \n",
      " |  __rsub__(self, value, /)\n",
      " |      Return value-self.\n",
      " |  \n",
      " |  __rtruediv__(self, value, /)\n",
      " |      Return value/self.\n",
      " |  \n",
      " |  __rxor__(self, value, /)\n",
      " |      Return value^self.\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __setstate__(...)\n",
      " |      a.__setstate__(version, shape, dtype, isfortran, rawdata)\n",
      " |      \n",
      " |      For unpickling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      version : int\n",
      " |          optional pickle version. If omitted defaults to 0.\n",
      " |      shape : tuple\n",
      " |      dtype : data-type\n",
      " |      isFortran : bool\n",
      " |      rawdata : string or list\n",
      " |          a binary string with the data (or a list if 'a' is an object array)\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __sub__(self, value, /)\n",
      " |      Return self-value.\n",
      " |  \n",
      " |  __truediv__(self, value, /)\n",
      " |      Return self/value.\n",
      " |  \n",
      " |  __xor__(self, value, /)\n",
      " |      Return self^value.\n",
      " |  \n",
      " |  all(...)\n",
      " |      a.all(axis=None, out=None)\n",
      " |      \n",
      " |      Returns True if all elements evaluate to True.\n",
      " |      \n",
      " |      Refer to `numpy.all` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.all : equivalent function\n",
      " |  \n",
      " |  any(...)\n",
      " |      a.any(axis=None, out=None)\n",
      " |      \n",
      " |      Returns True if any of the elements of `a` evaluate to True.\n",
      " |      \n",
      " |      Refer to `numpy.any` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.any : equivalent function\n",
      " |  \n",
      " |  argmax(...)\n",
      " |      a.argmax(axis=None, out=None)\n",
      " |      \n",
      " |      Return indices of the maximum values along the given axis.\n",
      " |      \n",
      " |      Refer to `numpy.argmax` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.argmax : equivalent function\n",
      " |  \n",
      " |  argmin(...)\n",
      " |      a.argmin(axis=None, out=None)\n",
      " |      \n",
      " |      Return indices of the minimum values along the given axis of `a`.\n",
      " |      \n",
      " |      Refer to `numpy.argmin` for detailed documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.argmin : equivalent function\n",
      " |  \n",
      " |  argpartition(...)\n",
      " |      a.argpartition(kth, axis=-1, kind='introselect', order=None)\n",
      " |      \n",
      " |      Returns the indices that would partition this array.\n",
      " |      \n",
      " |      Refer to `numpy.argpartition` for full documentation.\n",
      " |      \n",
      " |      .. versionadded:: 1.8.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.argpartition : equivalent function\n",
      " |  \n",
      " |  argsort(...)\n",
      " |      a.argsort(axis=-1, kind='quicksort', order=None)\n",
      " |      \n",
      " |      Returns the indices that would sort this array.\n",
      " |      \n",
      " |      Refer to `numpy.argsort` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.argsort : equivalent function\n",
      " |  \n",
      " |  astype(...)\n",
      " |      a.astype(dtype, order='K', casting='unsafe', subok=True, copy=True)\n",
      " |      \n",
      " |      Copy of the array, cast to a specified type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : str or dtype\n",
      " |          Typecode or data-type to which the array is cast.\n",
      " |      order : {'C', 'F', 'A', 'K'}, optional\n",
      " |          Controls the memory layout order of the result.\n",
      " |          'C' means C order, 'F' means Fortran order, 'A'\n",
      " |          means 'F' order if all the arrays are Fortran contiguous,\n",
      " |          'C' order otherwise, and 'K' means as close to the\n",
      " |          order the array elements appear in memory as possible.\n",
      " |          Default is 'K'.\n",
      " |      casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n",
      " |          Controls what kind of data casting may occur. Defaults to 'unsafe'\n",
      " |          for backwards compatibility.\n",
      " |      \n",
      " |            * 'no' means the data types should not be cast at all.\n",
      " |            * 'equiv' means only byte-order changes are allowed.\n",
      " |            * 'safe' means only casts which can preserve values are allowed.\n",
      " |            * 'same_kind' means only safe casts or casts within a kind,\n",
      " |              like float64 to float32, are allowed.\n",
      " |            * 'unsafe' means any data conversions may be done.\n",
      " |      subok : bool, optional\n",
      " |          If True, then sub-classes will be passed-through (default), otherwise\n",
      " |          the returned array will be forced to be a base-class array.\n",
      " |      copy : bool, optional\n",
      " |          By default, astype always returns a newly allocated array. If this\n",
      " |          is set to false, and the `dtype`, `order`, and `subok`\n",
      " |          requirements are satisfied, the input array is returned instead\n",
      " |          of a copy.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      arr_t : ndarray\n",
      " |          Unless `copy` is False and the other conditions for returning the input\n",
      " |          array are satisfied (see description for `copy` input paramter), `arr_t`\n",
      " |          is a new array of the same shape as the input array, with dtype, order\n",
      " |          given by `dtype`, `order`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Starting in NumPy 1.9, astype method now returns an error if the string\n",
      " |      dtype to cast to is not long enough in 'safe' casting mode to hold the max\n",
      " |      value of integer/float array that is being casted. Previously the casting\n",
      " |      was allowed even if the result was truncated.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ComplexWarning\n",
      " |          When casting from complex to float or int. To avoid this,\n",
      " |          one should use ``a.real.astype(t)``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.array([1, 2, 2.5])\n",
      " |      >>> x\n",
      " |      array([ 1. ,  2. ,  2.5])\n",
      " |      \n",
      " |      >>> x.astype(int)\n",
      " |      array([1, 2, 2])\n",
      " |  \n",
      " |  byteswap(...)\n",
      " |      a.byteswap(inplace)\n",
      " |      \n",
      " |      Swap the bytes of the array elements\n",
      " |      \n",
      " |      Toggle between low-endian and big-endian data representation by\n",
      " |      returning a byteswapped array, optionally swapped in-place.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      inplace : bool, optional\n",
      " |          If ``True``, swap bytes in-place, default is ``False``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      out : ndarray\n",
      " |          The byteswapped array. If `inplace` is ``True``, this is\n",
      " |          a view to self.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> A = np.array([1, 256, 8755], dtype=np.int16)\n",
      " |      >>> map(hex, A)\n",
      " |      ['0x1', '0x100', '0x2233']\n",
      " |      >>> A.byteswap(True)\n",
      " |      array([  256,     1, 13090], dtype=int16)\n",
      " |      >>> map(hex, A)\n",
      " |      ['0x100', '0x1', '0x3322']\n",
      " |      \n",
      " |      Arrays of strings are not swapped\n",
      " |      \n",
      " |      >>> A = np.array(['ceg', 'fac'])\n",
      " |      >>> A.byteswap()\n",
      " |      array(['ceg', 'fac'],\n",
      " |            dtype='|S3')\n",
      " |  \n",
      " |  choose(...)\n",
      " |      a.choose(choices, out=None, mode='raise')\n",
      " |      \n",
      " |      Use an index array to construct a new array from a set of choices.\n",
      " |      \n",
      " |      Refer to `numpy.choose` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.choose : equivalent function\n",
      " |  \n",
      " |  clip(...)\n",
      " |      a.clip(a_min, a_max, out=None)\n",
      " |      \n",
      " |      Return an array whose values are limited to ``[a_min, a_max]``.\n",
      " |      \n",
      " |      Refer to `numpy.clip` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.clip : equivalent function\n",
      " |  \n",
      " |  compress(...)\n",
      " |      a.compress(condition, axis=None, out=None)\n",
      " |      \n",
      " |      Return selected slices of this array along given axis.\n",
      " |      \n",
      " |      Refer to `numpy.compress` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.compress : equivalent function\n",
      " |  \n",
      " |  conj(...)\n",
      " |      a.conj()\n",
      " |      \n",
      " |      Complex-conjugate all elements.\n",
      " |      \n",
      " |      Refer to `numpy.conjugate` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.conjugate : equivalent function\n",
      " |  \n",
      " |  conjugate(...)\n",
      " |      a.conjugate()\n",
      " |      \n",
      " |      Return the complex conjugate, element-wise.\n",
      " |      \n",
      " |      Refer to `numpy.conjugate` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.conjugate : equivalent function\n",
      " |  \n",
      " |  copy(...)\n",
      " |      a.copy(order='C')\n",
      " |      \n",
      " |      Return a copy of the array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      order : {'C', 'F', 'A', 'K'}, optional\n",
      " |          Controls the memory layout of the copy. 'C' means C-order,\n",
      " |          'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n",
      " |          'C' otherwise. 'K' means match the layout of `a` as closely\n",
      " |          as possible. (Note that this function and :func:numpy.copy are very\n",
      " |          similar, but have different default values for their order=\n",
      " |          arguments.)\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      numpy.copy\n",
      " |      numpy.copyto\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.array([[1,2,3],[4,5,6]], order='F')\n",
      " |      \n",
      " |      >>> y = x.copy()\n",
      " |      \n",
      " |      >>> x.fill(0)\n",
      " |      \n",
      " |      >>> x\n",
      " |      array([[0, 0, 0],\n",
      " |             [0, 0, 0]])\n",
      " |      \n",
      " |      >>> y\n",
      " |      array([[1, 2, 3],\n",
      " |             [4, 5, 6]])\n",
      " |      \n",
      " |      >>> y.flags['C_CONTIGUOUS']\n",
      " |      True\n",
      " |  \n",
      " |  cumprod(...)\n",
      " |      a.cumprod(axis=None, dtype=None, out=None)\n",
      " |      \n",
      " |      Return the cumulative product of the elements along the given axis.\n",
      " |      \n",
      " |      Refer to `numpy.cumprod` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.cumprod : equivalent function\n",
      " |  \n",
      " |  cumsum(...)\n",
      " |      a.cumsum(axis=None, dtype=None, out=None)\n",
      " |      \n",
      " |      Return the cumulative sum of the elements along the given axis.\n",
      " |      \n",
      " |      Refer to `numpy.cumsum` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.cumsum : equivalent function\n",
      " |  \n",
      " |  diagonal(...)\n",
      " |      a.diagonal(offset=0, axis1=0, axis2=1)\n",
      " |      \n",
      " |      Return specified diagonals. In NumPy 1.9 the returned array is a\n",
      " |      read-only view instead of a copy as in previous NumPy versions.  In\n",
      " |      NumPy 1.10 the read-only restriction will be removed.\n",
      " |      \n",
      " |      Refer to :func:`numpy.diagonal` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.diagonal : equivalent function\n",
      " |  \n",
      " |  dot(...)\n",
      " |      a.dot(b, out=None)\n",
      " |      \n",
      " |      Dot product of two arrays.\n",
      " |      \n",
      " |      Refer to `numpy.dot` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.dot : equivalent function\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> a = np.eye(2)\n",
      " |      >>> b = np.ones((2, 2)) * 2\n",
      " |      >>> a.dot(b)\n",
      " |      array([[ 2.,  2.],\n",
      " |             [ 2.,  2.]])\n",
      " |      \n",
      " |      This array method can be conveniently chained:\n",
      " |      \n",
      " |      >>> a.dot(b).dot(b)\n",
      " |      array([[ 8.,  8.],\n",
      " |             [ 8.,  8.]])\n",
      " |  \n",
      " |  dump(...)\n",
      " |      a.dump(file)\n",
      " |      \n",
      " |      Dump a pickle of the array to the specified file.\n",
      " |      The array can be read back with pickle.load or numpy.load.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      file : str\n",
      " |          A string naming the dump file.\n",
      " |  \n",
      " |  dumps(...)\n",
      " |      a.dumps()\n",
      " |      \n",
      " |      Returns the pickle of the array as a string.\n",
      " |      pickle.loads or numpy.loads will convert the string back to an array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      None\n",
      " |  \n",
      " |  fill(...)\n",
      " |      a.fill(value)\n",
      " |      \n",
      " |      Fill the array with a scalar value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : scalar\n",
      " |          All elements of `a` will be assigned this value.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> a = np.array([1, 2])\n",
      " |      >>> a.fill(0)\n",
      " |      >>> a\n",
      " |      array([0, 0])\n",
      " |      >>> a = np.empty(2)\n",
      " |      >>> a.fill(1)\n",
      " |      >>> a\n",
      " |      array([ 1.,  1.])\n",
      " |  \n",
      " |  flatten(...)\n",
      " |      a.flatten(order='C')\n",
      " |      \n",
      " |      Return a copy of the array collapsed into one dimension.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      order : {'C', 'F', 'A'}, optional\n",
      " |          Whether to flatten in C (row-major), Fortran (column-major) order,\n",
      " |          or preserve the C/Fortran ordering from `a`.\n",
      " |          The default is 'C'.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray\n",
      " |          A copy of the input array, flattened to one dimension.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      ravel : Return a flattened array.\n",
      " |      flat : A 1-D flat iterator over the array.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> a = np.array([[1,2], [3,4]])\n",
      " |      >>> a.flatten()\n",
      " |      array([1, 2, 3, 4])\n",
      " |      >>> a.flatten('F')\n",
      " |      array([1, 3, 2, 4])\n",
      " |  \n",
      " |  getfield(...)\n",
      " |      a.getfield(dtype, offset=0)\n",
      " |      \n",
      " |      Returns a field of the given array as a certain type.\n",
      " |      \n",
      " |      A field is a view of the array data with a given data-type. The values in\n",
      " |      the view are determined by the given type and the offset into the current\n",
      " |      array in bytes. The offset needs to be such that the view dtype fits in the\n",
      " |      array dtype; for example an array of dtype complex128 has 16-byte elements.\n",
      " |      If taking a view with a 32-bit integer (4 bytes), the offset needs to be\n",
      " |      between 0 and 12 bytes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : str or dtype\n",
      " |          The data type of the view. The dtype size of the view can not be larger\n",
      " |          than that of the array itself.\n",
      " |      offset : int\n",
      " |          Number of bytes to skip before beginning the element view.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.diag([1.+1.j]*2)\n",
      " |      >>> x[1, 1] = 2 + 4.j\n",
      " |      >>> x\n",
      " |      array([[ 1.+1.j,  0.+0.j],\n",
      " |             [ 0.+0.j,  2.+4.j]])\n",
      " |      >>> x.getfield(np.float64)\n",
      " |      array([[ 1.,  0.],\n",
      " |             [ 0.,  2.]])\n",
      " |      \n",
      " |      By choosing an offset of 8 bytes we can select the complex part of the\n",
      " |      array for our view:\n",
      " |      \n",
      " |      >>> x.getfield(np.float64, offset=8)\n",
      " |      array([[ 1.,  0.],\n",
      " |         [ 0.,  4.]])\n",
      " |  \n",
      " |  item(...)\n",
      " |      a.item(*args)\n",
      " |      \n",
      " |      Copy an element of an array to a standard Python scalar and return it.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \\*args : Arguments (variable number and type)\n",
      " |      \n",
      " |          * none: in this case, the method only works for arrays\n",
      " |            with one element (`a.size == 1`), which element is\n",
      " |            copied into a standard Python scalar object and returned.\n",
      " |      \n",
      " |          * int_type: this argument is interpreted as a flat index into\n",
      " |            the array, specifying which element to copy and return.\n",
      " |      \n",
      " |          * tuple of int_types: functions as does a single int_type argument,\n",
      " |            except that the argument is interpreted as an nd-index into the\n",
      " |            array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      z : Standard Python scalar object\n",
      " |          A copy of the specified element of the array as a suitable\n",
      " |          Python scalar\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      When the data type of `a` is longdouble or clongdouble, item() returns\n",
      " |      a scalar array object because there is no available Python scalar that\n",
      " |      would not lose information. Void arrays return a buffer object for item(),\n",
      " |      unless fields are defined, in which case a tuple is returned.\n",
      " |      \n",
      " |      `item` is very similar to a[args], except, instead of an array scalar,\n",
      " |      a standard Python scalar is returned. This can be useful for speeding up\n",
      " |      access to elements of the array and doing arithmetic on elements of the\n",
      " |      array using Python's optimized math.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.random.randint(9, size=(3, 3))\n",
      " |      >>> x\n",
      " |      array([[3, 1, 7],\n",
      " |             [2, 8, 3],\n",
      " |             [8, 5, 3]])\n",
      " |      >>> x.item(3)\n",
      " |      2\n",
      " |      >>> x.item(7)\n",
      " |      5\n",
      " |      >>> x.item((0, 1))\n",
      " |      1\n",
      " |      >>> x.item((2, 2))\n",
      " |      3\n",
      " |  \n",
      " |  itemset(...)\n",
      " |      a.itemset(*args)\n",
      " |      \n",
      " |      Insert scalar into an array (scalar is cast to array's dtype, if possible)\n",
      " |      \n",
      " |      There must be at least 1 argument, and define the last argument\n",
      " |      as *item*.  Then, ``a.itemset(*args)`` is equivalent to but faster\n",
      " |      than ``a[args] = item``.  The item should be a scalar value and `args`\n",
      " |      must select a single item in the array `a`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \\*args : Arguments\n",
      " |          If one argument: a scalar, only used in case `a` is of size 1.\n",
      " |          If two arguments: the last argument is the value to be set\n",
      " |          and must be a scalar, the first argument specifies a single array\n",
      " |          element location. It is either an int or a tuple.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Compared to indexing syntax, `itemset` provides some speed increase\n",
      " |      for placing a scalar into a particular location in an `ndarray`,\n",
      " |      if you must do this.  However, generally this is discouraged:\n",
      " |      among other problems, it complicates the appearance of the code.\n",
      " |      Also, when using `itemset` (and `item`) inside a loop, be sure\n",
      " |      to assign the methods to a local variable to avoid the attribute\n",
      " |      look-up at each loop iteration.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.random.randint(9, size=(3, 3))\n",
      " |      >>> x\n",
      " |      array([[3, 1, 7],\n",
      " |             [2, 8, 3],\n",
      " |             [8, 5, 3]])\n",
      " |      >>> x.itemset(4, 0)\n",
      " |      >>> x.itemset((2, 2), 9)\n",
      " |      >>> x\n",
      " |      array([[3, 1, 7],\n",
      " |             [2, 0, 3],\n",
      " |             [8, 5, 9]])\n",
      " |  \n",
      " |  max(...)\n",
      " |      a.max(axis=None, out=None)\n",
      " |      \n",
      " |      Return the maximum along a given axis.\n",
      " |      \n",
      " |      Refer to `numpy.amax` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.amax : equivalent function\n",
      " |  \n",
      " |  mean(...)\n",
      " |      a.mean(axis=None, dtype=None, out=None)\n",
      " |      \n",
      " |      Returns the average of the array elements along given axis.\n",
      " |      \n",
      " |      Refer to `numpy.mean` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.mean : equivalent function\n",
      " |  \n",
      " |  min(...)\n",
      " |      a.min(axis=None, out=None)\n",
      " |      \n",
      " |      Return the minimum along a given axis.\n",
      " |      \n",
      " |      Refer to `numpy.amin` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.amin : equivalent function\n",
      " |  \n",
      " |  newbyteorder(...)\n",
      " |      arr.newbyteorder(new_order='S')\n",
      " |      \n",
      " |      Return the array with the same data viewed with a different byte order.\n",
      " |      \n",
      " |      Equivalent to::\n",
      " |      \n",
      " |          arr.view(arr.dtype.newbytorder(new_order))\n",
      " |      \n",
      " |      Changes are also made in all fields and sub-arrays of the array data\n",
      " |      type.\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      new_order : string, optional\n",
      " |          Byte order to force; a value from the byte order specifications\n",
      " |          above. `new_order` codes can be any of::\n",
      " |      \n",
      " |           * 'S' - swap dtype from current to opposite endian\n",
      " |           * {'<', 'L'} - little endian\n",
      " |           * {'>', 'B'} - big endian\n",
      " |           * {'=', 'N'} - native order\n",
      " |           * {'|', 'I'} - ignore (no change to byte order)\n",
      " |      \n",
      " |          The default value ('S') results in swapping the current\n",
      " |          byte order. The code does a case-insensitive check on the first\n",
      " |          letter of `new_order` for the alternatives above.  For example,\n",
      " |          any of 'B' or 'b' or 'biggish' are valid to specify big-endian.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      new_arr : array\n",
      " |          New array object with the dtype reflecting given change to the\n",
      " |          byte order.\n",
      " |  \n",
      " |  nonzero(...)\n",
      " |      a.nonzero()\n",
      " |      \n",
      " |      Return the indices of the elements that are non-zero.\n",
      " |      \n",
      " |      Refer to `numpy.nonzero` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.nonzero : equivalent function\n",
      " |  \n",
      " |  partition(...)\n",
      " |      a.partition(kth, axis=-1, kind='introselect', order=None)\n",
      " |      \n",
      " |      Rearranges the elements in the array in such a way that value of the\n",
      " |      element in kth position is in the position it would be in a sorted array.\n",
      " |      All elements smaller than the kth element are moved before this element and\n",
      " |      all equal or greater are moved behind it. The ordering of the elements in\n",
      " |      the two partitions is undefined.\n",
      " |      \n",
      " |      .. versionadded:: 1.8.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      kth : int or sequence of ints\n",
      " |          Element index to partition by. The kth element value will be in its\n",
      " |          final sorted position and all smaller elements will be moved before it\n",
      " |          and all equal or greater elements behind it.\n",
      " |          The order all elements in the partitions is undefined.\n",
      " |          If provided with a sequence of kth it will partition all elements\n",
      " |          indexed by kth of them into their sorted position at once.\n",
      " |      axis : int, optional\n",
      " |          Axis along which to sort. Default is -1, which means sort along the\n",
      " |          last axis.\n",
      " |      kind : {'introselect'}, optional\n",
      " |          Selection algorithm. Default is 'introselect'.\n",
      " |      order : list, optional\n",
      " |          When `a` is an array with fields defined, this argument specifies\n",
      " |          which fields to compare first, second, etc.  Not all fields need be\n",
      " |          specified.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.partition : Return a parititioned copy of an array.\n",
      " |      argpartition : Indirect partition.\n",
      " |      sort : Full sort.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      See ``np.partition`` for notes on the different algorithms.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> a = np.array([3, 4, 2, 1])\n",
      " |      >>> a.partition(a, 3)\n",
      " |      >>> a\n",
      " |      array([2, 1, 3, 4])\n",
      " |      \n",
      " |      >>> a.partition((1, 3))\n",
      " |      array([1, 2, 3, 4])\n",
      " |  \n",
      " |  prod(...)\n",
      " |      a.prod(axis=None, dtype=None, out=None)\n",
      " |      \n",
      " |      Return the product of the array elements over the given axis\n",
      " |      \n",
      " |      Refer to `numpy.prod` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.prod : equivalent function\n",
      " |  \n",
      " |  ptp(...)\n",
      " |      a.ptp(axis=None, out=None)\n",
      " |      \n",
      " |      Peak to peak (maximum - minimum) value along a given axis.\n",
      " |      \n",
      " |      Refer to `numpy.ptp` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.ptp : equivalent function\n",
      " |  \n",
      " |  put(...)\n",
      " |      a.put(indices, values, mode='raise')\n",
      " |      \n",
      " |      Set ``a.flat[n] = values[n]`` for all `n` in indices.\n",
      " |      \n",
      " |      Refer to `numpy.put` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.put : equivalent function\n",
      " |  \n",
      " |  ravel(...)\n",
      " |      a.ravel([order])\n",
      " |      \n",
      " |      Return a flattened array.\n",
      " |      \n",
      " |      Refer to `numpy.ravel` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.ravel : equivalent function\n",
      " |      \n",
      " |      ndarray.flat : a flat iterator on the array.\n",
      " |  \n",
      " |  repeat(...)\n",
      " |      a.repeat(repeats, axis=None)\n",
      " |      \n",
      " |      Repeat elements of an array.\n",
      " |      \n",
      " |      Refer to `numpy.repeat` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.repeat : equivalent function\n",
      " |  \n",
      " |  reshape(...)\n",
      " |      a.reshape(shape, order='C')\n",
      " |      \n",
      " |      Returns an array containing the same data with a new shape.\n",
      " |      \n",
      " |      Refer to `numpy.reshape` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.reshape : equivalent function\n",
      " |  \n",
      " |  resize(...)\n",
      " |      a.resize(new_shape, refcheck=True)\n",
      " |      \n",
      " |      Change shape and size of array in-place.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      new_shape : tuple of ints, or `n` ints\n",
      " |          Shape of resized array.\n",
      " |      refcheck : bool, optional\n",
      " |          If False, reference count will not be checked. Default is True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          If `a` does not own its own data or references or views to it exist,\n",
      " |          and the data memory must be changed.\n",
      " |      \n",
      " |      SystemError\n",
      " |          If the `order` keyword argument is specified. This behaviour is a\n",
      " |          bug in NumPy.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      resize : Return a new array with the specified shape.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This reallocates space for the data area if necessary.\n",
      " |      \n",
      " |      Only contiguous arrays (data elements consecutive in memory) can be\n",
      " |      resized.\n",
      " |      \n",
      " |      The purpose of the reference count check is to make sure you\n",
      " |      do not use this array as a buffer for another Python object and then\n",
      " |      reallocate the memory. However, reference counts can increase in\n",
      " |      other ways so if you are sure that you have not shared the memory\n",
      " |      for this array with another Python object, then you may safely set\n",
      " |      `refcheck` to False.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Shrinking an array: array is flattened (in the order that the data are\n",
      " |      stored in memory), resized, and reshaped:\n",
      " |      \n",
      " |      >>> a = np.array([[0, 1], [2, 3]], order='C')\n",
      " |      >>> a.resize((2, 1))\n",
      " |      >>> a\n",
      " |      array([[0],\n",
      " |             [1]])\n",
      " |      \n",
      " |      >>> a = np.array([[0, 1], [2, 3]], order='F')\n",
      " |      >>> a.resize((2, 1))\n",
      " |      >>> a\n",
      " |      array([[0],\n",
      " |             [2]])\n",
      " |      \n",
      " |      Enlarging an array: as above, but missing entries are filled with zeros:\n",
      " |      \n",
      " |      >>> b = np.array([[0, 1], [2, 3]])\n",
      " |      >>> b.resize(2, 3) # new_shape parameter doesn't have to be a tuple\n",
      " |      >>> b\n",
      " |      array([[0, 1, 2],\n",
      " |             [3, 0, 0]])\n",
      " |      \n",
      " |      Referencing an array prevents resizing...\n",
      " |      \n",
      " |      >>> c = a\n",
      " |      >>> a.resize((1, 1))\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      ValueError: cannot resize an array that has been referenced ...\n",
      " |      \n",
      " |      Unless `refcheck` is False:\n",
      " |      \n",
      " |      >>> a.resize((1, 1), refcheck=False)\n",
      " |      >>> a\n",
      " |      array([[0]])\n",
      " |      >>> c\n",
      " |      array([[0]])\n",
      " |  \n",
      " |  round(...)\n",
      " |      a.round(decimals=0, out=None)\n",
      " |      \n",
      " |      Return `a` with each element rounded to the given number of decimals.\n",
      " |      \n",
      " |      Refer to `numpy.around` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.around : equivalent function\n",
      " |  \n",
      " |  searchsorted(...)\n",
      " |      a.searchsorted(v, side='left', sorter=None)\n",
      " |      \n",
      " |      Find indices where elements of v should be inserted in a to maintain order.\n",
      " |      \n",
      " |      For full documentation, see `numpy.searchsorted`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.searchsorted : equivalent function\n",
      " |  \n",
      " |  setfield(...)\n",
      " |      a.setfield(val, dtype, offset=0)\n",
      " |      \n",
      " |      Put a value into a specified place in a field defined by a data-type.\n",
      " |      \n",
      " |      Place `val` into `a`'s field defined by `dtype` and beginning `offset`\n",
      " |      bytes into the field.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      val : object\n",
      " |          Value to be placed in field.\n",
      " |      dtype : dtype object\n",
      " |          Data-type of the field in which to place `val`.\n",
      " |      offset : int, optional\n",
      " |          The number of bytes into the field at which to place `val`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      getfield\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.eye(3)\n",
      " |      >>> x.getfield(np.float64)\n",
      " |      array([[ 1.,  0.,  0.],\n",
      " |             [ 0.,  1.,  0.],\n",
      " |             [ 0.,  0.,  1.]])\n",
      " |      >>> x.setfield(3, np.int32)\n",
      " |      >>> x.getfield(np.int32)\n",
      " |      array([[3, 3, 3],\n",
      " |             [3, 3, 3],\n",
      " |             [3, 3, 3]])\n",
      " |      >>> x\n",
      " |      array([[  1.00000000e+000,   1.48219694e-323,   1.48219694e-323],\n",
      " |             [  1.48219694e-323,   1.00000000e+000,   1.48219694e-323],\n",
      " |             [  1.48219694e-323,   1.48219694e-323,   1.00000000e+000]])\n",
      " |      >>> x.setfield(np.eye(3), np.int32)\n",
      " |      >>> x\n",
      " |      array([[ 1.,  0.,  0.],\n",
      " |             [ 0.,  1.,  0.],\n",
      " |             [ 0.,  0.,  1.]])\n",
      " |  \n",
      " |  setflags(...)\n",
      " |      a.setflags(write=None, align=None, uic=None)\n",
      " |      \n",
      " |      Set array flags WRITEABLE, ALIGNED, and UPDATEIFCOPY, respectively.\n",
      " |      \n",
      " |      These Boolean-valued flags affect how numpy interprets the memory\n",
      " |      area used by `a` (see Notes below). The ALIGNED flag can only\n",
      " |      be set to True if the data is actually aligned according to the type.\n",
      " |      The UPDATEIFCOPY flag can never be set to True. The flag WRITEABLE\n",
      " |      can only be set to True if the array owns its own memory, or the\n",
      " |      ultimate owner of the memory exposes a writeable buffer interface,\n",
      " |      or is a string. (The exception for string is made so that unpickling\n",
      " |      can be done without copying memory.)\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      write : bool, optional\n",
      " |          Describes whether or not `a` can be written to.\n",
      " |      align : bool, optional\n",
      " |          Describes whether or not `a` is aligned properly for its type.\n",
      " |      uic : bool, optional\n",
      " |          Describes whether or not `a` is a copy of another \"base\" array.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Array flags provide information about how the memory area used\n",
      " |      for the array is to be interpreted. There are 6 Boolean flags\n",
      " |      in use, only three of which can be changed by the user:\n",
      " |      UPDATEIFCOPY, WRITEABLE, and ALIGNED.\n",
      " |      \n",
      " |      WRITEABLE (W) the data area can be written to;\n",
      " |      \n",
      " |      ALIGNED (A) the data and strides are aligned appropriately for the hardware\n",
      " |      (as determined by the compiler);\n",
      " |      \n",
      " |      UPDATEIFCOPY (U) this array is a copy of some other array (referenced\n",
      " |      by .base). When this array is deallocated, the base array will be\n",
      " |      updated with the contents of this array.\n",
      " |      \n",
      " |      All flags can be accessed using their first (upper case) letter as well\n",
      " |      as the full name.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> y\n",
      " |      array([[3, 1, 7],\n",
      " |             [2, 0, 0],\n",
      " |             [8, 5, 9]])\n",
      " |      >>> y.flags\n",
      " |        C_CONTIGUOUS : True\n",
      " |        F_CONTIGUOUS : False\n",
      " |        OWNDATA : True\n",
      " |        WRITEABLE : True\n",
      " |        ALIGNED : True\n",
      " |        UPDATEIFCOPY : False\n",
      " |      >>> y.setflags(write=0, align=0)\n",
      " |      >>> y.flags\n",
      " |        C_CONTIGUOUS : True\n",
      " |        F_CONTIGUOUS : False\n",
      " |        OWNDATA : True\n",
      " |        WRITEABLE : False\n",
      " |        ALIGNED : False\n",
      " |        UPDATEIFCOPY : False\n",
      " |      >>> y.setflags(uic=1)\n",
      " |      Traceback (most recent call last):\n",
      " |        File \"<stdin>\", line 1, in <module>\n",
      " |      ValueError: cannot set UPDATEIFCOPY flag to True\n",
      " |  \n",
      " |  sort(...)\n",
      " |      a.sort(axis=-1, kind='quicksort', order=None)\n",
      " |      \n",
      " |      Sort an array, in-place.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : int, optional\n",
      " |          Axis along which to sort. Default is -1, which means sort along the\n",
      " |          last axis.\n",
      " |      kind : {'quicksort', 'mergesort', 'heapsort'}, optional\n",
      " |          Sorting algorithm. Default is 'quicksort'.\n",
      " |      order : list, optional\n",
      " |          When `a` is an array with fields defined, this argument specifies\n",
      " |          which fields to compare first, second, etc.  Not all fields need be\n",
      " |          specified.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.sort : Return a sorted copy of an array.\n",
      " |      argsort : Indirect sort.\n",
      " |      lexsort : Indirect stable sort on multiple keys.\n",
      " |      searchsorted : Find elements in sorted array.\n",
      " |      partition: Partial sort.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      See ``sort`` for notes on the different sorting algorithms.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> a = np.array([[1,4], [3,1]])\n",
      " |      >>> a.sort(axis=1)\n",
      " |      >>> a\n",
      " |      array([[1, 4],\n",
      " |             [1, 3]])\n",
      " |      >>> a.sort(axis=0)\n",
      " |      >>> a\n",
      " |      array([[1, 3],\n",
      " |             [1, 4]])\n",
      " |      \n",
      " |      Use the `order` keyword to specify a field to use when sorting a\n",
      " |      structured array:\n",
      " |      \n",
      " |      >>> a = np.array([('a', 2), ('c', 1)], dtype=[('x', 'S1'), ('y', int)])\n",
      " |      >>> a.sort(order='y')\n",
      " |      >>> a\n",
      " |      array([('c', 1), ('a', 2)],\n",
      " |            dtype=[('x', '|S1'), ('y', '<i4')])\n",
      " |  \n",
      " |  squeeze(...)\n",
      " |      a.squeeze(axis=None)\n",
      " |      \n",
      " |      Remove single-dimensional entries from the shape of `a`.\n",
      " |      \n",
      " |      Refer to `numpy.squeeze` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.squeeze : equivalent function\n",
      " |  \n",
      " |  std(...)\n",
      " |      a.std(axis=None, dtype=None, out=None, ddof=0)\n",
      " |      \n",
      " |      Returns the standard deviation of the array elements along given axis.\n",
      " |      \n",
      " |      Refer to `numpy.std` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.std : equivalent function\n",
      " |  \n",
      " |  sum(...)\n",
      " |      a.sum(axis=None, dtype=None, out=None)\n",
      " |      \n",
      " |      Return the sum of the array elements over the given axis.\n",
      " |      \n",
      " |      Refer to `numpy.sum` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.sum : equivalent function\n",
      " |  \n",
      " |  swapaxes(...)\n",
      " |      a.swapaxes(axis1, axis2)\n",
      " |      \n",
      " |      Return a view of the array with `axis1` and `axis2` interchanged.\n",
      " |      \n",
      " |      Refer to `numpy.swapaxes` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.swapaxes : equivalent function\n",
      " |  \n",
      " |  take(...)\n",
      " |      a.take(indices, axis=None, out=None, mode='raise')\n",
      " |      \n",
      " |      Return an array formed from the elements of `a` at the given indices.\n",
      " |      \n",
      " |      Refer to `numpy.take` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.take : equivalent function\n",
      " |  \n",
      " |  tobytes(...)\n",
      " |      a.tobytes(order='C')\n",
      " |      \n",
      " |      Construct Python bytes containing the raw data bytes in the array.\n",
      " |      \n",
      " |      Constructs Python bytes showing a copy of the raw contents of\n",
      " |      data memory. The bytes object can be produced in either 'C' or 'Fortran',\n",
      " |      or 'Any' order (the default is 'C'-order). 'Any' order means C-order\n",
      " |      unless the F_CONTIGUOUS flag in the array is set, in which case it\n",
      " |      means 'Fortran' order.\n",
      " |      \n",
      " |      .. versionadded:: 1.9.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      order : {'C', 'F', None}, optional\n",
      " |          Order of the data for multidimensional arrays:\n",
      " |          C, Fortran, or the same as for the original array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      s : bytes\n",
      " |          Python bytes exhibiting a copy of `a`'s raw data.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.array([[0, 1], [2, 3]])\n",
      " |      >>> x.tobytes()\n",
      " |      b'\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x03\\x00\\x00\\x00'\n",
      " |      >>> x.tobytes('C') == x.tobytes()\n",
      " |      True\n",
      " |      >>> x.tobytes('F')\n",
      " |      b'\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x03\\x00\\x00\\x00'\n",
      " |  \n",
      " |  tofile(...)\n",
      " |      a.tofile(fid, sep=\"\", format=\"%s\")\n",
      " |      \n",
      " |      Write array to a file as text or binary (default).\n",
      " |      \n",
      " |      Data is always written in 'C' order, independent of the order of `a`.\n",
      " |      The data produced by this method can be recovered using the function\n",
      " |      fromfile().\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fid : file or str\n",
      " |          An open file object, or a string containing a filename.\n",
      " |      sep : str\n",
      " |          Separator between array items for text output.\n",
      " |          If \"\" (empty), a binary file is written, equivalent to\n",
      " |          ``file.write(a.tobytes())``.\n",
      " |      format : str\n",
      " |          Format string for text file output.\n",
      " |          Each entry in the array is formatted to text by first converting\n",
      " |          it to the closest Python type, and then using \"format\" % item.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is a convenience function for quick storage of array data.\n",
      " |      Information on endianness and precision is lost, so this method is not a\n",
      " |      good choice for files intended to archive data or transport data between\n",
      " |      machines with different endianness. Some of these problems can be overcome\n",
      " |      by outputting the data as text files, at the expense of speed and file\n",
      " |      size.\n",
      " |  \n",
      " |  tolist(...)\n",
      " |      a.tolist()\n",
      " |      \n",
      " |      Return the array as a (possibly nested) list.\n",
      " |      \n",
      " |      Return a copy of the array data as a (nested) Python list.\n",
      " |      Data items are converted to the nearest compatible Python type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      none\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : list\n",
      " |          The possibly nested list of array elements.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The array may be recreated, ``a = np.array(a.tolist())``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> a = np.array([1, 2])\n",
      " |      >>> a.tolist()\n",
      " |      [1, 2]\n",
      " |      >>> a = np.array([[1, 2], [3, 4]])\n",
      " |      >>> list(a)\n",
      " |      [array([1, 2]), array([3, 4])]\n",
      " |      >>> a.tolist()\n",
      " |      [[1, 2], [3, 4]]\n",
      " |  \n",
      " |  tostring(...)\n",
      " |      a.tostring(order='C')\n",
      " |      \n",
      " |      Construct Python bytes containing the raw data bytes in the array.\n",
      " |      \n",
      " |      Constructs Python bytes showing a copy of the raw contents of\n",
      " |      data memory. The bytes object can be produced in either 'C' or 'Fortran',\n",
      " |      or 'Any' order (the default is 'C'-order). 'Any' order means C-order\n",
      " |      unless the F_CONTIGUOUS flag in the array is set, in which case it\n",
      " |      means 'Fortran' order.\n",
      " |      \n",
      " |      This function is a compatibility alias for tobytes. Despite its name it returns bytes not strings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      order : {'C', 'F', None}, optional\n",
      " |          Order of the data for multidimensional arrays:\n",
      " |          C, Fortran, or the same as for the original array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      s : bytes\n",
      " |          Python bytes exhibiting a copy of `a`'s raw data.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.array([[0, 1], [2, 3]])\n",
      " |      >>> x.tobytes()\n",
      " |      b'\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x03\\x00\\x00\\x00'\n",
      " |      >>> x.tobytes('C') == x.tobytes()\n",
      " |      True\n",
      " |      >>> x.tobytes('F')\n",
      " |      b'\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x03\\x00\\x00\\x00'\n",
      " |  \n",
      " |  trace(...)\n",
      " |      a.trace(offset=0, axis1=0, axis2=1, dtype=None, out=None)\n",
      " |      \n",
      " |      Return the sum along diagonals of the array.\n",
      " |      \n",
      " |      Refer to `numpy.trace` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.trace : equivalent function\n",
      " |  \n",
      " |  transpose(...)\n",
      " |      a.transpose(*axes)\n",
      " |      \n",
      " |      Returns a view of the array with axes transposed.\n",
      " |      \n",
      " |      For a 1-D array, this has no effect. (To change between column and\n",
      " |      row vectors, first cast the 1-D array into a matrix object.)\n",
      " |      For a 2-D array, this is the usual matrix transpose.\n",
      " |      For an n-D array, if axes are given, their order indicates how the\n",
      " |      axes are permuted (see Examples). If axes are not provided and\n",
      " |      ``a.shape = (i[0], i[1], ... i[n-2], i[n-1])``, then\n",
      " |      ``a.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0])``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axes : None, tuple of ints, or `n` ints\n",
      " |      \n",
      " |       * None or no argument: reverses the order of the axes.\n",
      " |      \n",
      " |       * tuple of ints: `i` in the `j`-th place in the tuple means `a`'s\n",
      " |         `i`-th axis becomes `a.transpose()`'s `j`-th axis.\n",
      " |      \n",
      " |       * `n` ints: same as an n-tuple of the same ints (this form is\n",
      " |         intended simply as a \"convenience\" alternative to the tuple form)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      out : ndarray\n",
      " |          View of `a`, with axes suitably permuted.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      ndarray.T : Array property returning the array transposed.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> a = np.array([[1, 2], [3, 4]])\n",
      " |      >>> a\n",
      " |      array([[1, 2],\n",
      " |             [3, 4]])\n",
      " |      >>> a.transpose()\n",
      " |      array([[1, 3],\n",
      " |             [2, 4]])\n",
      " |      >>> a.transpose((1, 0))\n",
      " |      array([[1, 3],\n",
      " |             [2, 4]])\n",
      " |      >>> a.transpose(1, 0)\n",
      " |      array([[1, 3],\n",
      " |             [2, 4]])\n",
      " |  \n",
      " |  var(...)\n",
      " |      a.var(axis=None, dtype=None, out=None, ddof=0)\n",
      " |      \n",
      " |      Returns the variance of the array elements, along given axis.\n",
      " |      \n",
      " |      Refer to `numpy.var` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.var : equivalent function\n",
      " |  \n",
      " |  view(...)\n",
      " |      a.view(dtype=None, type=None)\n",
      " |      \n",
      " |      New view of array with the same data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : data-type or ndarray sub-class, optional\n",
      " |          Data-type descriptor of the returned view, e.g., float32 or int16. The\n",
      " |          default, None, results in the view having the same data-type as `a`.\n",
      " |          This argument can also be specified as an ndarray sub-class, which\n",
      " |          then specifies the type of the returned object (this is equivalent to\n",
      " |          setting the ``type`` parameter).\n",
      " |      type : Python type, optional\n",
      " |          Type of the returned view, e.g., ndarray or matrix.  Again, the\n",
      " |          default None results in type preservation.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      ``a.view()`` is used two different ways:\n",
      " |      \n",
      " |      ``a.view(some_dtype)`` or ``a.view(dtype=some_dtype)`` constructs a view\n",
      " |      of the array's memory with a different data-type.  This can cause a\n",
      " |      reinterpretation of the bytes of memory.\n",
      " |      \n",
      " |      ``a.view(ndarray_subclass)`` or ``a.view(type=ndarray_subclass)`` just\n",
      " |      returns an instance of `ndarray_subclass` that looks at the same array\n",
      " |      (same shape, dtype, etc.)  This does not cause a reinterpretation of the\n",
      " |      memory.\n",
      " |      \n",
      " |      For ``a.view(some_dtype)``, if ``some_dtype`` has a different number of\n",
      " |      bytes per entry than the previous dtype (for example, converting a\n",
      " |      regular array to a structured array), then the behavior of the view\n",
      " |      cannot be predicted just from the superficial appearance of ``a`` (shown\n",
      " |      by ``print(a)``). It also depends on exactly how ``a`` is stored in\n",
      " |      memory. Therefore if ``a`` is C-ordered versus fortran-ordered, versus\n",
      " |      defined as a slice or transpose, etc., the view may give different\n",
      " |      results.\n",
      " |      \n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.array([(1, 2)], dtype=[('a', np.int8), ('b', np.int8)])\n",
      " |      \n",
      " |      Viewing array data using a different type and dtype:\n",
      " |      \n",
      " |      >>> y = x.view(dtype=np.int16, type=np.matrix)\n",
      " |      >>> y\n",
      " |      matrix([[513]], dtype=int16)\n",
      " |      >>> print type(y)\n",
      " |      <class 'numpy.matrixlib.defmatrix.matrix'>\n",
      " |      \n",
      " |      Creating a view on a structured array so it can be used in calculations\n",
      " |      \n",
      " |      >>> x = np.array([(1, 2),(3,4)], dtype=[('a', np.int8), ('b', np.int8)])\n",
      " |      >>> xv = x.view(dtype=np.int8).reshape(-1,2)\n",
      " |      >>> xv\n",
      " |      array([[1, 2],\n",
      " |             [3, 4]], dtype=int8)\n",
      " |      >>> xv.mean(0)\n",
      " |      array([ 2.,  3.])\n",
      " |      \n",
      " |      Making changes to the view changes the underlying array\n",
      " |      \n",
      " |      >>> xv[0,1] = 20\n",
      " |      >>> print x\n",
      " |      [(1, 20) (3, 4)]\n",
      " |      \n",
      " |      Using a view to convert an array to a record array:\n",
      " |      \n",
      " |      >>> z = x.view(np.recarray)\n",
      " |      >>> z.a\n",
      " |      array([1], dtype=int8)\n",
      " |      \n",
      " |      Views share data:\n",
      " |      \n",
      " |      >>> x[0] = (9, 10)\n",
      " |      >>> z[0]\n",
      " |      (9, 10)\n",
      " |      \n",
      " |      Views that change the dtype size (bytes per entry) should normally be\n",
      " |      avoided on arrays defined by slices, transposes, fortran-ordering, etc.:\n",
      " |      \n",
      " |      >>> x = np.array([[1,2,3],[4,5,6]], dtype=np.int16)\n",
      " |      >>> y = x[:, 0:2]\n",
      " |      >>> y\n",
      " |      array([[1, 2],\n",
      " |             [4, 5]], dtype=int16)\n",
      " |      >>> y.view(dtype=[('width', np.int16), ('length', np.int16)])\n",
      " |      Traceback (most recent call last):\n",
      " |        File \"<stdin>\", line 1, in <module>\n",
      " |      ValueError: new type not compatible with array.\n",
      " |      >>> z = y.copy()\n",
      " |      >>> z.view(dtype=[('width', np.int16), ('length', np.int16)])\n",
      " |      array([[(1, 2)],\n",
      " |             [(4, 5)]], dtype=[('width', '<i2'), ('length', '<i2')])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  T\n",
      " |      Same as self.transpose(), except that self is returned if\n",
      " |      self.ndim < 2.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.array([[1.,2.],[3.,4.]])\n",
      " |      >>> x\n",
      " |      array([[ 1.,  2.],\n",
      " |             [ 3.,  4.]])\n",
      " |      >>> x.T\n",
      " |      array([[ 1.,  3.],\n",
      " |             [ 2.,  4.]])\n",
      " |      >>> x = np.array([1.,2.,3.,4.])\n",
      " |      >>> x\n",
      " |      array([ 1.,  2.,  3.,  4.])\n",
      " |      >>> x.T\n",
      " |      array([ 1.,  2.,  3.,  4.])\n",
      " |  \n",
      " |  __array_finalize__\n",
      " |      None.\n",
      " |  \n",
      " |  __array_interface__\n",
      " |      Array protocol: Python side.\n",
      " |  \n",
      " |  __array_priority__\n",
      " |      Array priority.\n",
      " |  \n",
      " |  __array_struct__\n",
      " |      Array protocol: C-struct side.\n",
      " |  \n",
      " |  base\n",
      " |      Base object if memory is from some other object.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      The base of an array that owns its memory is None:\n",
      " |      \n",
      " |      >>> x = np.array([1,2,3,4])\n",
      " |      >>> x.base is None\n",
      " |      True\n",
      " |      \n",
      " |      Slicing creates a view, whose memory is shared with x:\n",
      " |      \n",
      " |      >>> y = x[2:]\n",
      " |      >>> y.base is x\n",
      " |      True\n",
      " |  \n",
      " |  ctypes\n",
      " |      An object to simplify the interaction of the array with the ctypes\n",
      " |      module.\n",
      " |      \n",
      " |      This attribute creates an object that makes it easier to use arrays\n",
      " |      when calling shared libraries with the ctypes module. The returned\n",
      " |      object has, among others, data, shape, and strides attributes (see\n",
      " |      Notes below) which themselves return ctypes objects that can be used\n",
      " |      as arguments to a shared library.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      None\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      c : Python object\n",
      " |          Possessing attributes data, shape, strides, etc.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.ctypeslib\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Below are the public attributes of this object which were documented\n",
      " |      in \"Guide to NumPy\" (we have omitted undocumented public attributes,\n",
      " |      as well as documented private attributes):\n",
      " |      \n",
      " |      * data: A pointer to the memory area of the array as a Python integer.\n",
      " |        This memory area may contain data that is not aligned, or not in correct\n",
      " |        byte-order. The memory area may not even be writeable. The array\n",
      " |        flags and data-type of this array should be respected when passing this\n",
      " |        attribute to arbitrary C-code to avoid trouble that can include Python\n",
      " |        crashing. User Beware! The value of this attribute is exactly the same\n",
      " |        as self._array_interface_['data'][0].\n",
      " |      \n",
      " |      * shape (c_intp*self.ndim): A ctypes array of length self.ndim where\n",
      " |        the basetype is the C-integer corresponding to dtype('p') on this\n",
      " |        platform. This base-type could be c_int, c_long, or c_longlong\n",
      " |        depending on the platform. The c_intp type is defined accordingly in\n",
      " |        numpy.ctypeslib. The ctypes array contains the shape of the underlying\n",
      " |        array.\n",
      " |      \n",
      " |      * strides (c_intp*self.ndim): A ctypes array of length self.ndim where\n",
      " |        the basetype is the same as for the shape attribute. This ctypes array\n",
      " |        contains the strides information from the underlying array. This strides\n",
      " |        information is important for showing how many bytes must be jumped to\n",
      " |        get to the next element in the array.\n",
      " |      \n",
      " |      * data_as(obj): Return the data pointer cast to a particular c-types object.\n",
      " |        For example, calling self._as_parameter_ is equivalent to\n",
      " |        self.data_as(ctypes.c_void_p). Perhaps you want to use the data as a\n",
      " |        pointer to a ctypes array of floating-point data:\n",
      " |        self.data_as(ctypes.POINTER(ctypes.c_double)).\n",
      " |      \n",
      " |      * shape_as(obj): Return the shape tuple as an array of some other c-types\n",
      " |        type. For example: self.shape_as(ctypes.c_short).\n",
      " |      \n",
      " |      * strides_as(obj): Return the strides tuple as an array of some other\n",
      " |        c-types type. For example: self.strides_as(ctypes.c_longlong).\n",
      " |      \n",
      " |      Be careful using the ctypes attribute - especially on temporary\n",
      " |      arrays or arrays constructed on the fly. For example, calling\n",
      " |      ``(a+b).ctypes.data_as(ctypes.c_void_p)`` returns a pointer to memory\n",
      " |      that is invalid because the array created as (a+b) is deallocated\n",
      " |      before the next Python statement. You can avoid this problem using\n",
      " |      either ``c=a+b`` or ``ct=(a+b).ctypes``. In the latter case, ct will\n",
      " |      hold a reference to the array until ct is deleted or re-assigned.\n",
      " |      \n",
      " |      If the ctypes module is not available, then the ctypes attribute\n",
      " |      of array objects still returns something useful, but ctypes objects\n",
      " |      are not returned and errors may be raised instead. In particular,\n",
      " |      the object will still have the as parameter attribute which will\n",
      " |      return an integer equal to the data attribute.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import ctypes\n",
      " |      >>> x\n",
      " |      array([[0, 1],\n",
      " |             [2, 3]])\n",
      " |      >>> x.ctypes.data\n",
      " |      30439712\n",
      " |      >>> x.ctypes.data_as(ctypes.POINTER(ctypes.c_long))\n",
      " |      <ctypes.LP_c_long object at 0x01F01300>\n",
      " |      >>> x.ctypes.data_as(ctypes.POINTER(ctypes.c_long)).contents\n",
      " |      c_long(0)\n",
      " |      >>> x.ctypes.data_as(ctypes.POINTER(ctypes.c_longlong)).contents\n",
      " |      c_longlong(4294967296L)\n",
      " |      >>> x.ctypes.shape\n",
      " |      <numpy.core._internal.c_long_Array_2 object at 0x01FFD580>\n",
      " |      >>> x.ctypes.shape_as(ctypes.c_long)\n",
      " |      <numpy.core._internal.c_long_Array_2 object at 0x01FCE620>\n",
      " |      >>> x.ctypes.strides\n",
      " |      <numpy.core._internal.c_long_Array_2 object at 0x01FCE620>\n",
      " |      >>> x.ctypes.strides_as(ctypes.c_longlong)\n",
      " |      <numpy.core._internal.c_longlong_Array_2 object at 0x01F01300>\n",
      " |  \n",
      " |  data\n",
      " |      Python buffer object pointing to the start of the array's data.\n",
      " |  \n",
      " |  dtype\n",
      " |      Data-type of the array's elements.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      None\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      d : numpy dtype object\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.dtype\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x\n",
      " |      array([[0, 1],\n",
      " |             [2, 3]])\n",
      " |      >>> x.dtype\n",
      " |      dtype('int32')\n",
      " |      >>> type(x.dtype)\n",
      " |      <type 'numpy.dtype'>\n",
      " |  \n",
      " |  flags\n",
      " |      Information about the memory layout of the array.\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      C_CONTIGUOUS (C)\n",
      " |          The data is in a single, C-style contiguous segment.\n",
      " |      F_CONTIGUOUS (F)\n",
      " |          The data is in a single, Fortran-style contiguous segment.\n",
      " |      OWNDATA (O)\n",
      " |          The array owns the memory it uses or borrows it from another object.\n",
      " |      WRITEABLE (W)\n",
      " |          The data area can be written to.  Setting this to False locks\n",
      " |          the data, making it read-only.  A view (slice, etc.) inherits WRITEABLE\n",
      " |          from its base array at creation time, but a view of a writeable\n",
      " |          array may be subsequently locked while the base array remains writeable.\n",
      " |          (The opposite is not true, in that a view of a locked array may not\n",
      " |          be made writeable.  However, currently, locking a base object does not\n",
      " |          lock any views that already reference it, so under that circumstance it\n",
      " |          is possible to alter the contents of a locked array via a previously\n",
      " |          created writeable view onto it.)  Attempting to change a non-writeable\n",
      " |          array raises a RuntimeError exception.\n",
      " |      ALIGNED (A)\n",
      " |          The data and all elements are aligned appropriately for the hardware.\n",
      " |      UPDATEIFCOPY (U)\n",
      " |          This array is a copy of some other array. When this array is\n",
      " |          deallocated, the base array will be updated with the contents of\n",
      " |          this array.\n",
      " |      FNC\n",
      " |          F_CONTIGUOUS and not C_CONTIGUOUS.\n",
      " |      FORC\n",
      " |          F_CONTIGUOUS or C_CONTIGUOUS (one-segment test).\n",
      " |      BEHAVED (B)\n",
      " |          ALIGNED and WRITEABLE.\n",
      " |      CARRAY (CA)\n",
      " |          BEHAVED and C_CONTIGUOUS.\n",
      " |      FARRAY (FA)\n",
      " |          BEHAVED and F_CONTIGUOUS and not C_CONTIGUOUS.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The `flags` object can be accessed dictionary-like (as in ``a.flags['WRITEABLE']``),\n",
      " |      or by using lowercased attribute names (as in ``a.flags.writeable``). Short flag\n",
      " |      names are only supported in dictionary access.\n",
      " |      \n",
      " |      Only the UPDATEIFCOPY, WRITEABLE, and ALIGNED flags can be changed by\n",
      " |      the user, via direct assignment to the attribute or dictionary entry,\n",
      " |      or by calling `ndarray.setflags`.\n",
      " |      \n",
      " |      The array flags cannot be set arbitrarily:\n",
      " |      \n",
      " |      - UPDATEIFCOPY can only be set ``False``.\n",
      " |      - ALIGNED can only be set ``True`` if the data is truly aligned.\n",
      " |      - WRITEABLE can only be set ``True`` if the array owns its own memory\n",
      " |        or the ultimate owner of the memory exposes a writeable buffer\n",
      " |        interface or is a string.\n",
      " |      \n",
      " |      Arrays can be both C-style and Fortran-style contiguous simultaneously.\n",
      " |      This is clear for 1-dimensional arrays, but can also be true for higher\n",
      " |      dimensional arrays.\n",
      " |      \n",
      " |      Even for contiguous arrays a stride for a given dimension\n",
      " |      ``arr.strides[dim]`` may be *arbitrary* if ``arr.shape[dim] == 1``\n",
      " |      or the array has no elements.\n",
      " |      It does *not* generally hold that ``self.strides[-1] == self.itemsize``\n",
      " |      for C-style contiguous arrays or ``self.strides[0] == self.itemsize`` for\n",
      " |      Fortran-style contiguous arrays is true.\n",
      " |  \n",
      " |  flat\n",
      " |      A 1-D iterator over the array.\n",
      " |      \n",
      " |      This is a `numpy.flatiter` instance, which acts similarly to, but is not\n",
      " |      a subclass of, Python's built-in iterator object.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      flatten : Return a copy of the array collapsed into one dimension.\n",
      " |      \n",
      " |      flatiter\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.arange(1, 7).reshape(2, 3)\n",
      " |      >>> x\n",
      " |      array([[1, 2, 3],\n",
      " |             [4, 5, 6]])\n",
      " |      >>> x.flat[3]\n",
      " |      4\n",
      " |      >>> x.T\n",
      " |      array([[1, 4],\n",
      " |             [2, 5],\n",
      " |             [3, 6]])\n",
      " |      >>> x.T.flat[3]\n",
      " |      5\n",
      " |      >>> type(x.flat)\n",
      " |      <type 'numpy.flatiter'>\n",
      " |      \n",
      " |      An assignment example:\n",
      " |      \n",
      " |      >>> x.flat = 3; x\n",
      " |      array([[3, 3, 3],\n",
      " |             [3, 3, 3]])\n",
      " |      >>> x.flat[[1,4]] = 1; x\n",
      " |      array([[3, 1, 3],\n",
      " |             [3, 1, 3]])\n",
      " |  \n",
      " |  imag\n",
      " |      The imaginary part of the array.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.sqrt([1+0j, 0+1j])\n",
      " |      >>> x.imag\n",
      " |      array([ 0.        ,  0.70710678])\n",
      " |      >>> x.imag.dtype\n",
      " |      dtype('float64')\n",
      " |  \n",
      " |  itemsize\n",
      " |      Length of one array element in bytes.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.array([1,2,3], dtype=np.float64)\n",
      " |      >>> x.itemsize\n",
      " |      8\n",
      " |      >>> x = np.array([1,2,3], dtype=np.complex128)\n",
      " |      >>> x.itemsize\n",
      " |      16\n",
      " |  \n",
      " |  nbytes\n",
      " |      Total bytes consumed by the elements of the array.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Does not include memory consumed by non-element attributes of the\n",
      " |      array object.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.zeros((3,5,2), dtype=np.complex128)\n",
      " |      >>> x.nbytes\n",
      " |      480\n",
      " |      >>> np.prod(x.shape) * x.itemsize\n",
      " |      480\n",
      " |  \n",
      " |  ndim\n",
      " |      Number of array dimensions.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.array([1, 2, 3])\n",
      " |      >>> x.ndim\n",
      " |      1\n",
      " |      >>> y = np.zeros((2, 3, 4))\n",
      " |      >>> y.ndim\n",
      " |      3\n",
      " |  \n",
      " |  real\n",
      " |      The real part of the array.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.sqrt([1+0j, 0+1j])\n",
      " |      >>> x.real\n",
      " |      array([ 1.        ,  0.70710678])\n",
      " |      >>> x.real.dtype\n",
      " |      dtype('float64')\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.real : equivalent function\n",
      " |  \n",
      " |  shape\n",
      " |      Tuple of array dimensions.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      May be used to \"reshape\" the array, as long as this would not\n",
      " |      require a change in the total number of elements\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.array([1, 2, 3, 4])\n",
      " |      >>> x.shape\n",
      " |      (4,)\n",
      " |      >>> y = np.zeros((2, 3, 4))\n",
      " |      >>> y.shape\n",
      " |      (2, 3, 4)\n",
      " |      >>> y.shape = (3, 8)\n",
      " |      >>> y\n",
      " |      array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      " |             [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      " |             [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      " |      >>> y.shape = (3, 6)\n",
      " |      Traceback (most recent call last):\n",
      " |        File \"<stdin>\", line 1, in <module>\n",
      " |      ValueError: total size of new array must be unchanged\n",
      " |  \n",
      " |  size\n",
      " |      Number of elements in the array.\n",
      " |      \n",
      " |      Equivalent to ``np.prod(a.shape)``, i.e., the product of the array's\n",
      " |      dimensions.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.zeros((3, 5, 2), dtype=np.complex128)\n",
      " |      >>> x.size\n",
      " |      30\n",
      " |      >>> np.prod(x.shape)\n",
      " |      30\n",
      " |  \n",
      " |  strides\n",
      " |      Tuple of bytes to step in each dimension when traversing an array.\n",
      " |      \n",
      " |      The byte offset of element ``(i[0], i[1], ..., i[n])`` in an array `a`\n",
      " |      is::\n",
      " |      \n",
      " |          offset = sum(np.array(i) * a.strides)\n",
      " |      \n",
      " |      A more detailed explanation of strides can be found in the\n",
      " |      \"ndarray.rst\" file in the NumPy reference guide.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Imagine an array of 32-bit integers (each 4 bytes)::\n",
      " |      \n",
      " |        x = np.array([[0, 1, 2, 3, 4],\n",
      " |                      [5, 6, 7, 8, 9]], dtype=np.int32)\n",
      " |      \n",
      " |      This array is stored in memory as 40 bytes, one after the other\n",
      " |      (known as a contiguous block of memory).  The strides of an array tell\n",
      " |      us how many bytes we have to skip in memory to move to the next position\n",
      " |      along a certain axis.  For example, we have to skip 4 bytes (1 value) to\n",
      " |      move to the next column, but 20 bytes (5 values) to get to the same\n",
      " |      position in the next row.  As such, the strides for the array `x` will be\n",
      " |      ``(20, 4)``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.lib.stride_tricks.as_strided\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> y = np.reshape(np.arange(2*3*4), (2,3,4))\n",
      " |      >>> y\n",
      " |      array([[[ 0,  1,  2,  3],\n",
      " |              [ 4,  5,  6,  7],\n",
      " |              [ 8,  9, 10, 11]],\n",
      " |             [[12, 13, 14, 15],\n",
      " |              [16, 17, 18, 19],\n",
      " |              [20, 21, 22, 23]]])\n",
      " |      >>> y.strides\n",
      " |      (48, 16, 4)\n",
      " |      >>> y[1,1,1]\n",
      " |      17\n",
      " |      >>> offset=sum(y.strides * np.array((1,1,1)))\n",
      " |      >>> offset/y.itemsize\n",
      " |      17\n",
      " |      \n",
      " |      >>> x = np.reshape(np.arange(5*6*7*8), (5,6,7,8)).transpose(2,3,1,0)\n",
      " |      >>> x.strides\n",
      " |      (32, 4, 224, 1344)\n",
      " |      >>> i = np.array([3,5,2,2])\n",
      " |      >>> offset = sum(i * x.strides)\n",
      " |      >>> x[3,5,2,2]\n",
      " |      813\n",
      " |      >>> offset / x.itemsize\n",
      " |      813\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(doc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.000*利苑 + 0.000*辽宁队 + 0.000*实验室 + 0.000*联想 + 0.000*医药 + 0.000*降价 + 0.000*VS + 0.000*作战 + 0.000*本场 + 0.000*初盘'),\n",
       " (1,\n",
       "  '0.001*０ + 0.000*雷克萨斯 + 0.000*奇瑞 + 0.000*２ + 0.000*元老 + 0.000*１ + 0.000*围棋 + 0.000*销量 + 0.000*人口 + 0.000*止痛药'),\n",
       " (2,\n",
       "  '0.001*& + 0.001*nbsp + 0.001*; + 0.000*耿大勇 + 0.000*新飞 + 0.000*戴尔公司 + 0.000*坦克 + 0.000*邮票 + 0.000*股骨头 + 0.000*宁波'),\n",
       " (3,\n",
       "  '0.001*\\x00 + 0.001*nbsp + 0.001*& + 0.000*新浪 + 0.000*黑车 + 0.000*; + 0.000*游客 + 0.000*机票 + 0.000*联想 + 0.000*医务人员'),\n",
       " (4,\n",
       "  '0.001*袁 + 0.001*旅游 + 0.001*游客 + 0.001*小 + 0.001*你 + 0.001*我 + 0.001*公司 + 0.001*东莞 + 0.001*他 + 0.001*支付'),\n",
       " (5,\n",
       "  '0.001*新浪 + 0.001*\\r\\n + 0.000*易趣网 + 0.000*股东 + 0.000*H股 + 0.000*海上 + 0.000*升值 + 0.000*拦截 + 0.000*自卫队 + 0.000*作战'),\n",
       " (6,\n",
       "  '0.001*认证 + 0.000*车贷险 + 0.000*词汇 + 0.000*保险公司 + 0.000*肇庆 + 0.000*牙膏 + 0.000*还款 + 0.000*护理 + 0.000*句子 + 0.000*走廊'),\n",
       " (7,\n",
       "  '0.001*女兵 + 0.000*兵器 + 0.000*考生 + 0.000*低腰裤 + 0.000*志愿 + 0.000*三九生化 + 0.000*银行 + 0.000*我 + 0.000*词汇 + 0.000*家教'),\n",
       " (8,\n",
       "  '0.001*考古 + 0.001*  + 0.001*两会 + 0.000*沈阳市 + 0.000*手机 + 0.000*分期付款 + 0.000*王治郅 + 0.000*我 + 0.000*辅导班 + 0.000*您'),\n",
       " (9,\n",
       "  '0.002*& + 0.002*nbsp + 0.001*; + 0.001*志愿 + 0.001*  + 0.000*拳击 + 0.000*00 + 0.000*场位 + 0.000*填报 + 0.000*公斤')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = lda.print_topics(-1)\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6636686621953762\t Index:2\t Topic: 0.001*& + 0.001*nbsp + 0.001*; + 0.000*耿大勇 + 0.000*新飞 + 0.000*戴尔公司 + 0.000*坦克 + 0.000*邮票 + 0.000*股骨头 + 0.000*宁波\n",
      "Score: 0.273083399085053\t Index:4\t Topic: 0.001*袁 + 0.001*旅游 + 0.001*游客 + 0.001*小 + 0.001*你 + 0.001*我 + 0.001*公司 + 0.001*东莞 + 0.001*他 + 0.001*支付\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda[corpus_tfidf[0]], key=lambda tup: -1*tup[1]):\n",
    "    print (\"Score: {}\\t Index:{}\\t Topic: {}\".format(score, index, lda.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6635615306801939\t Index:2\t Topic: 0.001*& + 0.001*nbsp + 0.001*; + 0.000*耿大勇 + 0.000*新飞 + 0.000*戴尔公司 + 0.000*坦克 + 0.000*邮票 + 0.000*股骨头 + 0.000*宁波\n",
      "\n",
      "Score: 0.27319036084839016\t Index:4\t Topic: 0.001*袁 + 0.001*旅游 + 0.001*游客 + 0.001*小 + 0.001*你 + 0.001*我 + 0.001*公司 + 0.001*东莞 + 0.001*他 + 0.001*支付\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, score in lda[corpus_tfidf[0]]:\n",
    "    print (\"Score: {}\\t Index:{}\\t Topic: {}\\n\".format(score, index, lda.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.interfaces.TransformedCorpus object at 0x000000000C4896D8>\n"
     ]
    }
   ],
   "source": [
    "#     >>> lda = LdaModel(corpus, num_topics=100)  # train model\n",
    "lda[corpus_tfidf] # get topic probability distribution for a document\n",
    "#lda.update(corpus2) # update the LDA model with additional documents\n",
    "#print(lda[doc_bow])\n",
    "#lda = LdaModel(corpus, num_topics=50, alpha='auto', eval_every=5)  # train asymmetric alpha from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unseen document is composed by the following text: In my spare time I either play badmington or drive my car\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_count_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-fea7333118db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mbow_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_count_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munseen_document\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbow_vector\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Score: {}\\t Topic: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_topic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_count_dict' is not defined"
     ]
    }
   ],
   "source": [
    "unseen_document = \"In my spare time I either play badmington or drive my car\"\n",
    "print (\"The unseen document is composed by the following text:\", unseen_document)\n",
    "print\n",
    "\n",
    "bow_vector = word_count_dict.doc2bow(tokenize(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print (\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'CorePyPro'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-7794d19cbe59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspatial\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mCorePyPro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimeStump\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtotalTime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'CorePyPro'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "__title__ = 'topic model - build lda - 20news dataset'\n",
    "__author__ = 'pi'\n",
    "__mtime__ = '12/26/2014-026'\n",
    "# code is far away from bugs with the god animal protecting\n",
    "    I love animals. They taste delicious.\n",
    "              ┏┓      ┏┓\n",
    "            ┏┛┻━━━┛┻┓\n",
    "            ┃      ☃      ┃\n",
    "            ┃  ┳┛  ┗┳  ┃\n",
    "            ┃      ┻      ┃\n",
    "            ┗━┓      ┏━┛\n",
    "                ┃      ┗━━━┓\n",
    "                ┃  神兽保佑    ┣┓\n",
    "                ┃　永无BUG！   ┏┛\n",
    "                ┗┓┓┏━┳┓┏┛\n",
    "                  ┃┫┫  ┃┫┫\n",
    "                  ┗┻┛  ┗┻┛\n",
    "\"\"\"\n",
    "#from Colors import *\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import datetime\n",
    "from sklearn import datasets\n",
    "import nltk\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from CorePyPro.Fun.TimeStump import totalTime\n",
    "\n",
    "\n",
    "def load_texts(dataset_type='train', groups=None):\n",
    "    \"\"\"\n",
    "    load datasets to bytes list\n",
    "    :return:train_dataset_bunch.data bytes list\n",
    "    \"\"\"\n",
    "    if groups == 'small':\n",
    "        groups = ['comp.graphics', 'comp.os.ms-windows.misc']  # 仅用于小数据测试时用, #1368\n",
    "    elif groups == 'medium':\n",
    "        groups = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.ma c.hardware',\n",
    "                  'comp.windows.x', 'sci.space']  # 中量数据时用    #3414\n",
    "    train_dataset_bunch = datasets.load_mlcomp('20news-18828', dataset_type, mlcomp_root='./datasets',\n",
    "                                               categories=groups)  # 13180\n",
    "    return train_dataset_bunch.data\n",
    "\n",
    "\n",
    "def preprocess_texts(texts, test_doc_id=1):\n",
    "    \"\"\"\n",
    "    texts preprocessing\n",
    "    :param texts: bytes list\n",
    "    :return:bytes list\n",
    "    \"\"\"\n",
    "    texts = [t.decode(errors='ignore') for t in texts]  # bytes2str\n",
    "    # print(REDH, 'original texts[%d]: ' % test_doc_id, DEFAULT, '\\n',texts[test_doc_id])\n",
    "    # split_texts = [t.lower().split() for t in texts]\n",
    "    # print(REDH, 'split texts[%d]: #%d' % (test_doc_id, len(split_texts)), DEFAULT, '\\n',split_texts[test_doc_id])\n",
    "\n",
    "\n",
    "    # lower str & split str 2 word list with sep=... & delete None\n",
    "    SEPS = '[\\s()-/,:.?!]\\s*'\n",
    "    texts = [re.split(SEPS, t.lower()) for t in texts]\n",
    "    for t in texts:\n",
    "        while '' in t:\n",
    "            t.remove('')\n",
    "    # print(REDH, 'texts[%d] lower & split(seps= %s) & delete None: #%d' % (test_doc_id, SEPS, len(texts[test_doc_id])), DEFAULT, '\\n',texts[test_doc_id])\n",
    "\n",
    "\n",
    "    # nltk.download()   #then choose the corpus.stopwords\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))  # #127\n",
    "    stopwords.update(['from', 'subject', 'writes'])  # #129\n",
    "    word_usage = defaultdict(int)\n",
    "    for t in texts:\n",
    "        for w in t:\n",
    "            word_usage[w] += 1\n",
    "    COMMON_LINE = len(texts) / 10\n",
    "    too_common_words = [w for w in t if word_usage[w] > COMMON_LINE]  # set(too_common_words)\n",
    "    # print('too_common_words: #', len(too_common_words), '\\n', too_common_words)   #68\n",
    "    stopwords.update(too_common_words)\n",
    "    # print('stopwords: #', len(stopwords), '\\n', stopwords)  #   #147\n",
    "\n",
    "    english_stemmer = nltk.SnowballStemmer('english')\n",
    "    MIN_WORD_LEN = 3  # 4\n",
    "    texts = [[english_stemmer.stem(w) for w in t if\n",
    "              not set(w) & set('@+>0123456789*') and w not in stopwords and len(w) >= MIN_WORD_LEN] for t in\n",
    "             texts]  # set('+-.?!()>@0123456789*/')\n",
    "    # print(REDH, 'texts[%d] delete ^alphanum & stopwords & len<%d & stemmed: #' % (test_doc_id, MIN_WORD_LEN),\n",
    "    # len(texts[test_doc_id]), DEFAULT, '\\n', texts[test_doc_id])\n",
    "    return texts\n",
    "\n",
    "\n",
    "def build_corpus(texts):\n",
    "    \"\"\"\n",
    "    build corpora\n",
    "    :param texts: bytes list\n",
    "    :return: corpus DirectTextCorpus(corpora.TextCorpus)\n",
    "    \"\"\"\n",
    "\n",
    "    class DirectTextCorpus(corpora.TextCorpus): \n",
    "        def get_texts(self):\n",
    "            return self.input\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.input)\n",
    "\n",
    "    corpus = DirectTextCorpus(texts)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def build_id2word(corpus):\n",
    "    \"\"\"\n",
    "    from corpus build id2word=dict\n",
    "    :param corpus:\n",
    "    :return:dict = corpus.dictionary\n",
    "    \"\"\"\n",
    "    dict = corpus.dictionary  # gensim.corpora.dictionary.Dictionary\n",
    "    # print(dict.id2token)\n",
    "    try:\n",
    "        dict['anything']\n",
    "    except:\n",
    "        pass\n",
    "        # print(\"dict.id2token is not {} now\")\n",
    "    # print(dict.id2token)\n",
    "    return dict\n",
    "\n",
    "\n",
    "def save_corpus_dict(dict, corpus, dictDir='./LDA/id_word.dict', corpusDir='./LDA/corpus.mm'):\n",
    "    dict.save(dictDir)\n",
    "    print(GREENL, 'dict saved into %s successfully ...' % dictDir, DEFAULT)\n",
    "    corpora.MmCorpus.serialize(corpusDir, corpus)\n",
    "    print(GREENL, 'corpus saved into %s successfully ...' % corpusDir, DEFAULT)\n",
    "    # corpus.save(fname='./LDA/corpus.mm')  # stores only the (tiny) iteration object\n",
    "\n",
    "\n",
    "def load_ldamodel(modelDir='./lda.pkl'):\n",
    "    model = models.LdaModel.load(fname=modelDir)\n",
    "    print(GREENL, 'ldamodel load from %s successfully ...' % modelDir, DEFAULT)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_corpus_dict(dictDir='./LDA/id_word.dict', corpusDir='./LDA/corpus.mm'):\n",
    "    dict = corpora.Dictionary.load(fname=dictDir)\n",
    "    print(GREENL, 'dict load from %s successfully ...' % dictDir, DEFAULT)\n",
    "    # dict = corpora.Dictionary.load_from_text('./id_word.txt')\n",
    "    corpus = corpora.MmCorpus(corpusDir)  # corpora.mmcorpus.MmCorpus\n",
    "    print(GREENL, 'corpus load from %s successfully ...' % corpusDir, DEFAULT)\n",
    "    return dict, corpus\n",
    "\n",
    "\n",
    "def build_doc_word_mat(corpus, model, num_topics):\n",
    "    \"\"\"\n",
    "    build doc_word_mat in topic space\n",
    "    :param corpus:\n",
    "    :param model:\n",
    "    :param num_topics: int\n",
    "    :return:doc_word_mat np.array (len(topics) * num_topics)\n",
    "    \"\"\"\n",
    "    topics = [model[c] for c in corpus]  # (word_id, weight) list\n",
    "    doc_word_mat = np.zeros((len(topics), num_topics))\n",
    "    for doc, topic in enumerate(topics):\n",
    "        for word_id, weight in topic:\n",
    "            doc_word_mat[doc, word_id] += weight\n",
    "    return doc_word_mat\n",
    "\n",
    "\n",
    "def compute_pairwise_dist(doc_word_mat):\n",
    "    \"\"\"\n",
    "    compute pairwise dist\n",
    "    :param doc_word_mat: np.array (len(topics) * num_topics)\n",
    "    :return:pairwise_dist <class 'numpy.ndarray'>\n",
    "    \"\"\"\n",
    "    pairwise_dist = spatial.distance.squareform(spatial.distance.pdist(doc_word_mat))\n",
    "    max_weight = pairwise_dist.max() + 1\n",
    "    for i in list(range(len(pairwise_dist))):\n",
    "        pairwise_dist[i, i] = max_weight\n",
    "    return pairwise_dist\n",
    "\n",
    "\n",
    "def closest_texts(corpus, model, num_topics, test_doc_id=1, topn=5):\n",
    "    \"\"\"\n",
    "    find the closest_doc_ids for  doc[test_doc_id]\n",
    "    :param corpus:\n",
    "    :param model:\n",
    "    :param num_topics:\n",
    "    :param test_doc_id:\n",
    "    :param topn:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    doc_word_mat = build_doc_word_mat(corpus, model, num_topics)\n",
    "    pairwise_dist = compute_pairwise_dist(doc_word_mat)\n",
    "    # print(REDH, 'original texts[%d]: ' % test_doc_id, DEFAULT, '\\n', original_texts[test_doc_id])\n",
    "    closest_doc_ids = pairwise_dist[test_doc_id].argsort()\n",
    "    # return closest_doc_ids[:topn]\n",
    "    for closest_doc_id in closest_doc_ids[:topn]:\n",
    "        print(RED, 'closest doc[%d]' % closest_doc_id, DEFAULT, '\\n', original_texts[closest_doc_id])\n",
    "\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"\n",
    "    計算模型在test data的Perplexity\n",
    "    :param model:\n",
    "    :return:model.log_perplexity float\n",
    "    \"\"\"\n",
    "    test_texts = load_texts(dataset_type='test', groups='small')\n",
    "    test_texts = preprocess_texts(test_texts)\n",
    "    test_corpus = build_corpus(test_texts)\n",
    "    return model.log_perplexity(test_corpus)\n",
    "\n",
    "\n",
    "def test_num_topics():\n",
    "    dict, corpus = load_corpus_dict()\n",
    "    print(\"#corpus_items:\", len(corpus))\n",
    "    for num_topics in [3, 5, 10, 30, 50, 100, 150, 200, 300]:\n",
    "        start_time = datetime.datetime.now()\n",
    "        model = models.LdaModel(corpus, num_topics=num_topics, id2word=dict)\n",
    "        end_time = datetime.datetime.now()\n",
    "        print(\"total running time = \", end_time - start_time)\n",
    "        print(REDL, 'model.log_perplexity for test_texts with num_topics=%d : ' % num_topics, evaluate_model(model),\n",
    "              DEFAULT)\n",
    "\n",
    "\n",
    "def test():\n",
    "    texts = load_texts(dataset_type='train', groups='small')\n",
    "    original_texts = texts\n",
    "    test_doc_id = 1\n",
    "\n",
    "    # texts = preprocess_texts(texts, test_doc_id=test_doc_id)\n",
    "    # corpus = build_corpus(texts=texts)  # corpus DirectTextCorpus(corpora.TextCorpus)\n",
    "    # dict = build_id2word(corpus)\n",
    "    # save_corpus_dict(dict, corpus)\n",
    "    dict, corpus = load_corpus_dict()\n",
    "    # print(len(corpus))\n",
    "\n",
    "    num_topics = 100\n",
    "    model = models.LdaModel(corpus, num_topics=num_topics, id2word=dict)  # 每次结果不同\n",
    "    model.show_topic(0)\n",
    "    # model.save(fname='./lda.pkl')\n",
    "\n",
    "    # model = load_ldamodel()\n",
    "    # closest_texts(corpus, model, num_topics, test_doc_id=1, topn=3)\n",
    "\n",
    "    print(REDL, 'model.log_perplexity for test_texts', evaluate_model(model), DEFAULT)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()\n",
    "    # test_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced consumes0bytes in 0 non-directory files\n",
      "Reduced\\C000008 consumes4749812bytes in 1990 non-directory files\n",
      "Reduced\\C000010 consumes3539444bytes in 1990 non-directory files\n",
      "Reduced\\C000013 consumes5078678bytes in 1990 non-directory files\n",
      "Reduced\\C000014 consumes3213181bytes in 1990 non-directory files\n",
      "Reduced\\C000016 consumes4155989bytes in 1990 non-directory files\n",
      "Reduced\\C000020 consumes6791509bytes in 1990 non-directory files\n",
      "Reduced\\C000022 consumes6696483bytes in 1990 non-directory files\n",
      "Reduced\\C000023 consumes11397015bytes in 1990 non-directory files\n",
      "Reduced\\C000024 consumes4961840bytes in 1990 non-directory files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join, getsize\n",
    "for root, dirs, files in os.walk('Reduced'):\n",
    "    print(root, \"consumes\", end=\"\")\n",
    "    print(sum([getsize(join(root, name)) for name in files]), end=\"\")\n",
    "    print(\"bytes in\", len(files), \"non-directory files\")\n",
    "    if 'CVS' in dirs:\n",
    "        dirs.remove('CVS')  # don't visit CVS directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\r\n",
      "跳空过大关 双高仍可期\r\n",
      "北 京 首 证\r\n",
      "\r\n",
      "两市周二再次跳空高开并呈极明显的强势上攻态势，股指尾市再创新高并收于次高点，涨跌家数之比显示市场依然保持全线普涨的壮观景象，成交总额更是高达700亿元，较前激增约180亿元或34％左右，去掉权证因素的话，日成交总额也达到590亿元以上，这无疑是近年来的天量。\r\n",
      "周二两市之所以继续保持强势上行之势，关键就在于市场的做多热情已经并正在不断提高，涨跌家数之比的变化便充分表明了这一点，且不论涨幅在5％以上的个股比比皆是，仅以涨停家数而言，沪深两市就比周一又一次有所增加。值得注意的是，作为市场的中流砥柱，大盘蓝筹板块内部再次出现有序的轮动格局，两市各自的第一航母即中国石化(行情,论坛)、G中兴(行情,论坛)多数时间内保持强势整固姿态，G宝钢(行情,论坛)、G长电(行情,论坛)、G招行(行情,论坛)和G万科等的短线涨幅也弱于股指，一定程度显示多方主力目前并不想大幅推高股指，而G华能(行情,论坛)、G申能(行情,论坛)和G华侨城(行情,论坛)等的表现相对较强，且G五粮液(行情,论坛)的身影还出现在涨停行列中，尾市深发展等大盘银行股也出现大幅上扬之势，也就是说，目前既有一部分超级航母直接参与推升大盘，也有一部分权重很大的蓝筹龙头品种引而不发，这就在相当大程度上确保了市场近日将会继续强攻。\r\n",
      "盘面观察可发现，两市绝大多数个股与板块都已经形成强攻态势，市场的热点由此被完全激活，换言之，市场热点并非以往的以某一类型的个股为主线，而是形成了“满园春色关不?钡木跋螅∑渲校绕渲档靡惶岬氖牵谛矶喔龉沙中耐保ㄖて分值谋硐指欠绻馕尴蓿扔形辶竃GC1在一个月左右的时间内大涨500％以上，也有在一天内暴涨61.59％的万华HXP1！这种效应对市场的影响力无疑是相当大的，很大程度上也表明市场继续强势上攻的可能依然相当大。\r\n",
      "从股指运行看，两市大盘周二再次跳空高开高走，日线走势图上由此也产生了连续的第二个上升缺口，其中上证综指的还恰好出现在千五整数和心理大关处，这对市场也会产生很大的积极影响，而且成交额也创出了近阶段的新高，单日成交近600亿元（历史天量的64％左右）也表明增量资金入市热情仍然非常高涨！因此，两市近日很可能将会继续强势上行，“双高”即股价和成交额的新高纪录也将在强攻过程中不断被刷新。 \r\n",
      "\r\n",
      "\r\n",
      "在本机构所知情的范围内，本机构及财产上的利害关系人与所分析的大盘观点。所评价或推荐的证券无任何利害关系，观点仅供参考，据此入市，风险自担！\r\n",
      "\r\n",
      "北京首证投资顾问有限公司 E-mail: capitalstock@vip.sina.com \r\n",
      "联系电话：010－51690108；010－51690109\r\n",
      "地址：北京市东城区东四十条甲22号南新仓商务大厦B座1021室\r\n",
      "\r\n",
      "作者声明：在本机构、本人所知情的范围内,本机构、本人以及财产上的利害关系人与所述文章内容没有利害关系。本版文章纯属个人观点,仅供参考,文责自负。读者据此入市,风险自担。\n"
     ]
    }
   ],
   "source": [
    "import codecs,sys\n",
    "\n",
    "f = codecs.open('1107.txt','r','utf-8')\n",
    "try:\n",
    "    text = f.read()\n",
    "    #text =unicode(text,'utf-8')\n",
    "finally:\n",
    "    f.close()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "help(codecs)\n",
    "import os\n",
    "help(os.walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package gensim:\n",
      "\n",
      "NAME\n",
      "    gensim\n",
      "\n",
      "DESCRIPTION\n",
      "    This package contains interfaces and functionality to compute pair-wise document\n",
      "    similarities within a corpus of documents.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    corpora (package)\n",
      "    interfaces\n",
      "    matutils\n",
      "    models (package)\n",
      "    nosy\n",
      "    parsing (package)\n",
      "    scripts (package)\n",
      "    similarities (package)\n",
      "    summarization (package)\n",
      "    test (package)\n",
      "    utils\n",
      "\n",
      "CLASSES\n",
      "    logging.Handler(logging.Filterer)\n",
      "        NullHandler\n",
      "    \n",
      "    class NullHandler(logging.Handler)\n",
      "     |  For python versions <= 2.6; same as `logging.NullHandler` in 2.7.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NullHandler\n",
      "     |      logging.Handler\n",
      "     |      logging.Filterer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  emit(self, record)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from logging.Handler:\n",
      "     |  \n",
      "     |  __init__(self, level=0)\n",
      "     |      Initializes the instance - basically setting the formatter to None\n",
      "     |      and the filter list to empty.\n",
      "     |  \n",
      "     |  acquire(self)\n",
      "     |      Acquire the I/O thread lock.\n",
      "     |  \n",
      "     |  close(self)\n",
      "     |      Tidy up any resources used by the handler.\n",
      "     |      \n",
      "     |      This version removes the handler from an internal map of handlers,\n",
      "     |      _handlers, which is used for handler lookup by name. Subclasses\n",
      "     |      should ensure that this gets called from overridden close()\n",
      "     |      methods.\n",
      "     |  \n",
      "     |  createLock(self)\n",
      "     |      Acquire a thread lock for serializing access to the underlying I/O.\n",
      "     |  \n",
      "     |  flush(self)\n",
      "     |      Ensure all logging output has been flushed.\n",
      "     |      \n",
      "     |      This version does nothing and is intended to be implemented by\n",
      "     |      subclasses.\n",
      "     |  \n",
      "     |  format(self, record)\n",
      "     |      Format the specified record.\n",
      "     |      \n",
      "     |      If a formatter is set, use it. Otherwise, use the default formatter\n",
      "     |      for the module.\n",
      "     |  \n",
      "     |  get_name(self)\n",
      "     |  \n",
      "     |  handle(self, record)\n",
      "     |      Conditionally emit the specified logging record.\n",
      "     |      \n",
      "     |      Emission depends on filters which may have been added to the handler.\n",
      "     |      Wrap the actual emission of the record with acquisition/release of\n",
      "     |      the I/O thread lock. Returns whether the filter passed the record for\n",
      "     |      emission.\n",
      "     |  \n",
      "     |  handleError(self, record)\n",
      "     |      Handle errors which occur during an emit() call.\n",
      "     |      \n",
      "     |      This method should be called from handlers when an exception is\n",
      "     |      encountered during an emit() call. If raiseExceptions is false,\n",
      "     |      exceptions get silently ignored. This is what is mostly wanted\n",
      "     |      for a logging system - most users will not care about errors in\n",
      "     |      the logging system, they are more interested in application errors.\n",
      "     |      You could, however, replace this with a custom handler if you wish.\n",
      "     |      The record which was being processed is passed in to this method.\n",
      "     |  \n",
      "     |  release(self)\n",
      "     |      Release the I/O thread lock.\n",
      "     |  \n",
      "     |  setFormatter(self, fmt)\n",
      "     |      Set the formatter for this handler.\n",
      "     |  \n",
      "     |  setLevel(self, level)\n",
      "     |      Set the logging level of this handler.  level must be an int or a str.\n",
      "     |  \n",
      "     |  set_name(self, name)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from logging.Handler:\n",
      "     |  \n",
      "     |  name\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from logging.Filterer:\n",
      "     |  \n",
      "     |  addFilter(self, filter)\n",
      "     |      Add the specified filter to this handler.\n",
      "     |  \n",
      "     |  filter(self, record)\n",
      "     |      Determine if a record is loggable by consulting all the filters.\n",
      "     |      \n",
      "     |      The default is to allow the record to be logged; any filter can veto\n",
      "     |      this and the record is then dropped. Returns a zero value if a record\n",
      "     |      is to be dropped, else non-zero.\n",
      "     |      \n",
      "     |      .. versionchanged: 3.2\n",
      "     |      \n",
      "     |         Allow filters to be just callables.\n",
      "     |  \n",
      "     |  removeFilter(self, filter)\n",
      "     |      Remove the specified filter from this handler.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from logging.Filterer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    logger = <logging.Logger object>\n",
      "\n",
      "VERSION\n",
      "    0.12.4\n",
      "\n",
      "FILE\n",
      "    d:\\360downloads\\anaconda3\\lib\\site-packages\\gensim-0.12.4-py3.4-win-amd64.egg\\gensim\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "help(gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module gensim.test.test_ldamodel in gensim.test:\n",
      "\n",
      "NAME\n",
      "    gensim.test.test_ldamodel - Automated tests for checking transformation algorithms (the models package).\n",
      "\n",
      "CLASSES\n",
      "    unittest.case.TestCase(builtins.object)\n",
      "        TestLdaModel\n",
      "            TestLdaMulticore\n",
      "    \n",
      "    class TestLdaModel(unittest.case.TestCase)\n",
      "     |  Method resolution order:\n",
      "     |      TestLdaModel\n",
      "     |      unittest.case.TestCase\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  setUp(self)\n",
      "     |  \n",
      "     |  testAlpha(self)\n",
      "     |  \n",
      "     |  testAlphaAuto(self)\n",
      "     |  \n",
      "     |  testEta(self)\n",
      "     |  \n",
      "     |  testEtaAuto(self)\n",
      "     |  \n",
      "     |  testGetDocumentTopics(self)\n",
      "     |  \n",
      "     |  testGetTopicTerms(self)\n",
      "     |  \n",
      "     |  testLargeMmap(self)\n",
      "     |  \n",
      "     |  testLargeMmapCompressed(self)\n",
      "     |  \n",
      "     |  testPasses(self)\n",
      "     |  \n",
      "     |  testPersistence(self)\n",
      "     |  \n",
      "     |  testPersistenceCompressed(self)\n",
      "     |  \n",
      "     |  testPersistenceIgnore(self)\n",
      "     |  \n",
      "     |  testShowTopic(self)\n",
      "     |  \n",
      "     |  testShowTopics(self)\n",
      "     |  \n",
      "     |  testTopTopics(self)\n",
      "     |  \n",
      "     |  testTransform(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from unittest.case.TestCase:\n",
      "     |  \n",
      "     |  __call__(self, *args, **kwds)\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |  \n",
      "     |  __init__(self, methodName='runTest')\n",
      "     |      Create an instance of the class that will use the named test\n",
      "     |      method when executed. Raises a ValueError if the instance does\n",
      "     |      not have a method with the specified name.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |  \n",
      "     |  addCleanup(self, function, *args, **kwargs)\n",
      "     |      Add a function, with arguments, to be called when the test is\n",
      "     |      completed. Functions added are called on a LIFO basis and are\n",
      "     |      called after tearDown on test failure or success.\n",
      "     |      \n",
      "     |      Cleanup items are called even if setUp fails (unlike tearDown).\n",
      "     |  \n",
      "     |  addTypeEqualityFunc(self, typeobj, function)\n",
      "     |      Add a type specific assertEqual style function to compare a type.\n",
      "     |      \n",
      "     |      This method is for use by TestCase subclasses that need to register\n",
      "     |      their own type equality functions to provide nicer error messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          typeobj: The data type to call this function on when both values\n",
      "     |                  are of the same type in assertEqual().\n",
      "     |          function: The callable taking two arguments and an optional\n",
      "     |                  msg= argument that raises self.failureException with a\n",
      "     |                  useful error message when the two arguments are not equal.\n",
      "     |  \n",
      "     |  assertAlmostEqual(self, first, second, places=None, msg=None, delta=None)\n",
      "     |      Fail if the two objects are unequal as determined by their\n",
      "     |      difference rounded to the given number of decimal places\n",
      "     |      (default 7) and comparing to zero, or by comparing that the\n",
      "     |      between the two objects is more than the given delta.\n",
      "     |      \n",
      "     |      Note that decimal places (from zero) are usually not the same\n",
      "     |      as significant digits (measured from the most signficant digit).\n",
      "     |      \n",
      "     |      If the two objects compare equal then they will automatically\n",
      "     |      compare almost equal.\n",
      "     |  \n",
      "     |  assertAlmostEquals = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  assertCountEqual(self, first, second, msg=None)\n",
      "     |      An unordered sequence comparison asserting that the same elements,\n",
      "     |      regardless of order.  If the same element occurs more than once,\n",
      "     |      it verifies that the elements occur the same number of times.\n",
      "     |      \n",
      "     |          self.assertEqual(Counter(list(first)),\n",
      "     |                           Counter(list(second)))\n",
      "     |      \n",
      "     |       Example:\n",
      "     |          - [0, 1, 1] and [1, 0, 1] compare equal.\n",
      "     |          - [0, 0, 1] and [0, 1] compare unequal.\n",
      "     |  \n",
      "     |  assertDictContainsSubset(self, subset, dictionary, msg=None)\n",
      "     |      Checks whether dictionary is a superset of subset.\n",
      "     |  \n",
      "     |  assertDictEqual(self, d1, d2, msg=None)\n",
      "     |  \n",
      "     |  assertEqual(self, first, second, msg=None)\n",
      "     |      Fail if the two objects are unequal as determined by the '=='\n",
      "     |      operator.\n",
      "     |  \n",
      "     |  assertEquals = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  assertFalse(self, expr, msg=None)\n",
      "     |      Check that the expression is false.\n",
      "     |  \n",
      "     |  assertGreater(self, a, b, msg=None)\n",
      "     |      Just like self.assertTrue(a > b), but with a nicer default message.\n",
      "     |  \n",
      "     |  assertGreaterEqual(self, a, b, msg=None)\n",
      "     |      Just like self.assertTrue(a >= b), but with a nicer default message.\n",
      "     |  \n",
      "     |  assertIn(self, member, container, msg=None)\n",
      "     |      Just like self.assertTrue(a in b), but with a nicer default message.\n",
      "     |  \n",
      "     |  assertIs(self, expr1, expr2, msg=None)\n",
      "     |      Just like self.assertTrue(a is b), but with a nicer default message.\n",
      "     |  \n",
      "     |  assertIsInstance(self, obj, cls, msg=None)\n",
      "     |      Same as self.assertTrue(isinstance(obj, cls)), with a nicer\n",
      "     |      default message.\n",
      "     |  \n",
      "     |  assertIsNone(self, obj, msg=None)\n",
      "     |      Same as self.assertTrue(obj is None), with a nicer default message.\n",
      "     |  \n",
      "     |  assertIsNot(self, expr1, expr2, msg=None)\n",
      "     |      Just like self.assertTrue(a is not b), but with a nicer default message.\n",
      "     |  \n",
      "     |  assertIsNotNone(self, obj, msg=None)\n",
      "     |      Included for symmetry with assertIsNone.\n",
      "     |  \n",
      "     |  assertLess(self, a, b, msg=None)\n",
      "     |      Just like self.assertTrue(a < b), but with a nicer default message.\n",
      "     |  \n",
      "     |  assertLessEqual(self, a, b, msg=None)\n",
      "     |      Just like self.assertTrue(a <= b), but with a nicer default message.\n",
      "     |  \n",
      "     |  assertListEqual(self, list1, list2, msg=None)\n",
      "     |      A list-specific equality assertion.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          list1: The first list to compare.\n",
      "     |          list2: The second list to compare.\n",
      "     |          msg: Optional message to use on failure instead of a list of\n",
      "     |                  differences.\n",
      "     |  \n",
      "     |  assertLogs(self, logger=None, level=None)\n",
      "     |      Fail unless a log message of level *level* or higher is emitted\n",
      "     |      on *logger_name* or its children.  If omitted, *level* defaults to\n",
      "     |      INFO and *logger* defaults to the root logger.\n",
      "     |      \n",
      "     |      This method must be used as a context manager, and will yield\n",
      "     |      a recording object with two attributes: `output` and `records`.\n",
      "     |      At the end of the context manager, the `output` attribute will\n",
      "     |      be a list of the matching formatted log messages and the\n",
      "     |      `records` attribute will be a list of the corresponding LogRecord\n",
      "     |      objects.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          with self.assertLogs('foo', level='INFO') as cm:\n",
      "     |              logging.getLogger('foo').info('first message')\n",
      "     |              logging.getLogger('foo.bar').error('second message')\n",
      "     |          self.assertEqual(cm.output, ['INFO:foo:first message',\n",
      "     |                                       'ERROR:foo.bar:second message'])\n",
      "     |  \n",
      "     |  assertMultiLineEqual(self, first, second, msg=None)\n",
      "     |      Assert that two multi-line strings are equal.\n",
      "     |  \n",
      "     |  assertNotAlmostEqual(self, first, second, places=None, msg=None, delta=None)\n",
      "     |      Fail if the two objects are equal as determined by their\n",
      "     |      difference rounded to the given number of decimal places\n",
      "     |      (default 7) and comparing to zero, or by comparing that the\n",
      "     |      between the two objects is less than the given delta.\n",
      "     |      \n",
      "     |      Note that decimal places (from zero) are usually not the same\n",
      "     |      as significant digits (measured from the most signficant digit).\n",
      "     |      \n",
      "     |      Objects that are equal automatically fail.\n",
      "     |  \n",
      "     |  assertNotAlmostEquals = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  assertNotEqual(self, first, second, msg=None)\n",
      "     |      Fail if the two objects are equal as determined by the '!='\n",
      "     |      operator.\n",
      "     |  \n",
      "     |  assertNotEquals = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  assertNotIn(self, member, container, msg=None)\n",
      "     |      Just like self.assertTrue(a not in b), but with a nicer default message.\n",
      "     |  \n",
      "     |  assertNotIsInstance(self, obj, cls, msg=None)\n",
      "     |      Included for symmetry with assertIsInstance.\n",
      "     |  \n",
      "     |  assertNotRegex(self, text, unexpected_regex, msg=None)\n",
      "     |      Fail the test if the text matches the regular expression.\n",
      "     |  \n",
      "     |  assertRaises(self, excClass, callableObj=None, *args, **kwargs)\n",
      "     |      Fail unless an exception of class excClass is raised\n",
      "     |      by callableObj when invoked with arguments args and keyword\n",
      "     |      arguments kwargs. If a different type of exception is\n",
      "     |      raised, it will not be caught, and the test case will be\n",
      "     |      deemed to have suffered an error, exactly as for an\n",
      "     |      unexpected exception.\n",
      "     |      \n",
      "     |      If called with callableObj omitted or None, will return a\n",
      "     |      context object used like this::\n",
      "     |      \n",
      "     |           with self.assertRaises(SomeException):\n",
      "     |               do_something()\n",
      "     |      \n",
      "     |      An optional keyword argument 'msg' can be provided when assertRaises\n",
      "     |      is used as a context object.\n",
      "     |      \n",
      "     |      The context manager keeps a reference to the exception as\n",
      "     |      the 'exception' attribute. This allows you to inspect the\n",
      "     |      exception after the assertion::\n",
      "     |      \n",
      "     |          with self.assertRaises(SomeException) as cm:\n",
      "     |              do_something()\n",
      "     |          the_exception = cm.exception\n",
      "     |          self.assertEqual(the_exception.error_code, 3)\n",
      "     |  \n",
      "     |  assertRaisesRegex(self, expected_exception, expected_regex, callable_obj=None, *args, **kwargs)\n",
      "     |      Asserts that the message in a raised exception matches a regex.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expected_exception: Exception class expected to be raised.\n",
      "     |          expected_regex: Regex (re pattern object or string) expected\n",
      "     |                  to be found in error message.\n",
      "     |          callable_obj: Function to be called.\n",
      "     |          msg: Optional message used in case of failure. Can only be used\n",
      "     |                  when assertRaisesRegex is used as a context manager.\n",
      "     |          args: Extra args.\n",
      "     |          kwargs: Extra kwargs.\n",
      "     |  \n",
      "     |  assertRaisesRegexp = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  assertRegex(self, text, expected_regex, msg=None)\n",
      "     |      Fail the test unless the text matches the regular expression.\n",
      "     |  \n",
      "     |  assertRegexpMatches = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  assertSequenceEqual(self, seq1, seq2, msg=None, seq_type=None)\n",
      "     |      An equality assertion for ordered sequences (like lists and tuples).\n",
      "     |      \n",
      "     |      For the purposes of this function, a valid ordered sequence type is one\n",
      "     |      which can be indexed, has a length, and has an equality operator.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          seq1: The first sequence to compare.\n",
      "     |          seq2: The second sequence to compare.\n",
      "     |          seq_type: The expected datatype of the sequences, or None if no\n",
      "     |                  datatype should be enforced.\n",
      "     |          msg: Optional message to use on failure instead of a list of\n",
      "     |                  differences.\n",
      "     |  \n",
      "     |  assertSetEqual(self, set1, set2, msg=None)\n",
      "     |      A set-specific equality assertion.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set1: The first set to compare.\n",
      "     |          set2: The second set to compare.\n",
      "     |          msg: Optional message to use on failure instead of a list of\n",
      "     |                  differences.\n",
      "     |      \n",
      "     |      assertSetEqual uses ducktyping to support different types of sets, and\n",
      "     |      is optimized for sets specifically (parameters must support a\n",
      "     |      difference method).\n",
      "     |  \n",
      "     |  assertTrue(self, expr, msg=None)\n",
      "     |      Check that the expression is true.\n",
      "     |  \n",
      "     |  assertTupleEqual(self, tuple1, tuple2, msg=None)\n",
      "     |      A tuple-specific equality assertion.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          tuple1: The first tuple to compare.\n",
      "     |          tuple2: The second tuple to compare.\n",
      "     |          msg: Optional message to use on failure instead of a list of\n",
      "     |                  differences.\n",
      "     |  \n",
      "     |  assertWarns(self, expected_warning, callable_obj=None, *args, **kwargs)\n",
      "     |      Fail unless a warning of class warnClass is triggered\n",
      "     |      by callable_obj when invoked with arguments args and keyword\n",
      "     |      arguments kwargs.  If a different type of warning is\n",
      "     |      triggered, it will not be handled: depending on the other\n",
      "     |      warning filtering rules in effect, it might be silenced, printed\n",
      "     |      out, or raised as an exception.\n",
      "     |      \n",
      "     |      If called with callable_obj omitted or None, will return a\n",
      "     |      context object used like this::\n",
      "     |      \n",
      "     |           with self.assertWarns(SomeWarning):\n",
      "     |               do_something()\n",
      "     |      \n",
      "     |      An optional keyword argument 'msg' can be provided when assertWarns\n",
      "     |      is used as a context object.\n",
      "     |      \n",
      "     |      The context manager keeps a reference to the first matching\n",
      "     |      warning as the 'warning' attribute; similarly, the 'filename'\n",
      "     |      and 'lineno' attributes give you information about the line\n",
      "     |      of Python code from which the warning was triggered.\n",
      "     |      This allows you to inspect the warning after the assertion::\n",
      "     |      \n",
      "     |          with self.assertWarns(SomeWarning) as cm:\n",
      "     |              do_something()\n",
      "     |          the_warning = cm.warning\n",
      "     |          self.assertEqual(the_warning.some_attribute, 147)\n",
      "     |  \n",
      "     |  assertWarnsRegex(self, expected_warning, expected_regex, callable_obj=None, *args, **kwargs)\n",
      "     |      Asserts that the message in a triggered warning matches a regexp.\n",
      "     |      Basic functioning is similar to assertWarns() with the addition\n",
      "     |      that only warnings whose messages also match the regular expression\n",
      "     |      are considered successful matches.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expected_warning: Warning class expected to be triggered.\n",
      "     |          expected_regex: Regex (re pattern object or string) expected\n",
      "     |                  to be found in error message.\n",
      "     |          callable_obj: Function to be called.\n",
      "     |          msg: Optional message used in case of failure. Can only be used\n",
      "     |                  when assertWarnsRegex is used as a context manager.\n",
      "     |          args: Extra args.\n",
      "     |          kwargs: Extra kwargs.\n",
      "     |  \n",
      "     |  assert_ = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  countTestCases(self)\n",
      "     |  \n",
      "     |  debug(self)\n",
      "     |      Run the test without collecting errors in a TestResult\n",
      "     |  \n",
      "     |  defaultTestResult(self)\n",
      "     |  \n",
      "     |  doCleanups(self)\n",
      "     |      Execute all cleanup functions. Normally called for you after\n",
      "     |      tearDown.\n",
      "     |  \n",
      "     |  fail(self, msg=None)\n",
      "     |      Fail immediately, with the given message.\n",
      "     |  \n",
      "     |  failIf = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  failIfAlmostEqual = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  failIfEqual = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  failUnless = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  failUnlessAlmostEqual = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  failUnlessEqual = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  failUnlessRaises = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  id(self)\n",
      "     |  \n",
      "     |  run(self, result=None)\n",
      "     |  \n",
      "     |  shortDescription(self)\n",
      "     |      Returns a one-line description of the test, or None if no\n",
      "     |      description has been provided.\n",
      "     |      \n",
      "     |      The default implementation of this method returns the first line of\n",
      "     |      the specified test method's docstring.\n",
      "     |  \n",
      "     |  skipTest(self, reason)\n",
      "     |      Skip this test.\n",
      "     |  \n",
      "     |  subTest(self, msg=None, **params)\n",
      "     |      Return a context manager that will return the enclosed block\n",
      "     |      of code in a subtest identified by the optional message and\n",
      "     |      keyword parameters.  A failure in the subtest marks the test\n",
      "     |      case as failed but resumes execution at the end of the enclosed\n",
      "     |      block, allowing further test code to be executed.\n",
      "     |  \n",
      "     |  tearDown(self)\n",
      "     |      Hook method for deconstructing the test fixture after testing it.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from unittest.case.TestCase:\n",
      "     |  \n",
      "     |  setUpClass() from builtins.type\n",
      "     |      Hook method for setting up class fixture before running tests in the class.\n",
      "     |  \n",
      "     |  tearDownClass() from builtins.type\n",
      "     |      Hook method for deconstructing the class fixture after running all tests in the class.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from unittest.case.TestCase:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from unittest.case.TestCase:\n",
      "     |  \n",
      "     |  failureException = <class 'AssertionError'>\n",
      "     |      Assertion failed.\n",
      "     |  \n",
      "     |  longMessage = True\n",
      "     |  \n",
      "     |  maxDiff = 640\n",
      "    \n",
      "    class TestLdaMulticore(TestLdaModel)\n",
      "     |  Method resolution order:\n",
      "     |      TestLdaMulticore\n",
      "     |      TestLdaModel\n",
      "     |      unittest.case.TestCase\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  setUp(self)\n",
      "     |  \n",
      "     |  testAlphaAuto(self)\n",
      "     |      # override LdaModel because multicore does not allow alpha=auto\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from TestLdaModel:\n",
      "     |  \n",
      "     |  testAlpha(self)\n",
      "     |  \n",
      "     |  testEta(self)\n",
      "     |  \n",
      "     |  testEtaAuto(self)\n",
      "     |  \n",
      "     |  testGetDocumentTopics(self)\n",
      "     |  \n",
      "     |  testGetTopicTerms(self)\n",
      "     |  \n",
      "     |  testLargeMmap(self)\n",
      "     |  \n",
      "     |  testLargeMmapCompressed(self)\n",
      "     |  \n",
      "     |  testPasses(self)\n",
      "     |  \n",
      "     |  testPersistence(self)\n",
      "     |  \n",
      "     |  testPersistenceCompressed(self)\n",
      "     |  \n",
      "     |  testPersistenceIgnore(self)\n",
      "     |  \n",
      "     |  testShowTopic(self)\n",
      "     |  \n",
      "     |  testShowTopics(self)\n",
      "     |  \n",
      "     |  testTopTopics(self)\n",
      "     |  \n",
      "     |  testTransform(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from unittest.case.TestCase:\n",
      "     |  \n",
      "     |  __call__(self, *args, **kwds)\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |  \n",
      "     |  __init__(self, methodName='runTest')\n",
      "     |      Create an instance of the class that will use the named test\n",
      "     |      method when executed. Raises a ValueError if the instance does\n",
      "     |      not have a method with the specified name.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |  \n",
      "     |  addCleanup(self, function, *args, **kwargs)\n",
      "     |      Add a function, with arguments, to be called when the test is\n",
      "     |      completed. Functions added are called on a LIFO basis and are\n",
      "     |      called after tearDown on test failure or success.\n",
      "     |      \n",
      "     |      Cleanup items are called even if setUp fails (unlike tearDown).\n",
      "     |  \n",
      "     |  addTypeEqualityFunc(self, typeobj, function)\n",
      "     |      Add a type specific assertEqual style function to compare a type.\n",
      "     |      \n",
      "     |      This method is for use by TestCase subclasses that need to register\n",
      "     |      their own type equality functions to provide nicer error messages.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          typeobj: The data type to call this function on when both values\n",
      "     |                  are of the same type in assertEqual().\n",
      "     |          function: The callable taking two arguments and an optional\n",
      "     |                  msg= argument that raises self.failureException with a\n",
      "     |                  useful error message when the two arguments are not equal.\n",
      "     |  \n",
      "     |  assertAlmostEqual(self, first, second, places=None, msg=None, delta=None)\n",
      "     |      Fail if the two objects are unequal as determined by their\n",
      "     |      difference rounded to the given number of decimal places\n",
      "     |      (default 7) and comparing to zero, or by comparing that the\n",
      "     |      between the two objects is more than the given delta.\n",
      "     |      \n",
      "     |      Note that decimal places (from zero) are usually not the same\n",
      "     |      as significant digits (measured from the most signficant digit).\n",
      "     |      \n",
      "     |      If the two objects compare equal then they will automatically\n",
      "     |      compare almost equal.\n",
      "     |  \n",
      "     |  assertAlmostEquals = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  assertCountEqual(self, first, second, msg=None)\n",
      "     |      An unordered sequence comparison asserting that the same elements,\n",
      "     |      regardless of order.  If the same element occurs more than once,\n",
      "     |      it verifies that the elements occur the same number of times.\n",
      "     |      \n",
      "     |          self.assertEqual(Counter(list(first)),\n",
      "     |                           Counter(list(second)))\n",
      "     |      \n",
      "     |       Example:\n",
      "     |          - [0, 1, 1] and [1, 0, 1] compare equal.\n",
      "     |          - [0, 0, 1] and [0, 1] compare unequal.\n",
      "     |  \n",
      "     |  assertDictContainsSubset(self, subset, dictionary, msg=None)\n",
      "     |      Checks whether dictionary is a superset of subset.\n",
      "     |  \n",
      "     |  assertDictEqual(self, d1, d2, msg=None)\n",
      "     |  \n",
      "     |  assertEqual(self, first, second, msg=None)\n",
      "     |      Fail if the two objects are unequal as determined by the '=='\n",
      "     |      operator.\n",
      "     |  \n",
      "     |  assertEquals = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  assertFalse(self, expr, msg=None)\n",
      "     |      Check that the expression is false.\n",
      "     |  \n",
      "     |  assertGreater(self, a, b, msg=None)\n",
      "     |      Just like self.assertTrue(a > b), but with a nicer default message.\n",
      "     |  \n",
      "     |  assertGreaterEqual(self, a, b, msg=None)\n",
      "     |      Just like self.assertTrue(a >= b), but with a nicer default message.\n",
      "     |  \n",
      "     |  assertIn(self, member, container, msg=None)\n",
      "     |      Just like self.assertTrue(a in b), but with a nicer default message.\n",
      "     |  \n",
      "     |  assertIs(self, expr1, expr2, msg=None)\n",
      "     |      Just like self.assertTrue(a is b), but with a nicer default message.\n",
      "     |  \n",
      "     |  assertIsInstance(self, obj, cls, msg=None)\n",
      "     |      Same as self.assertTrue(isinstance(obj, cls)), with a nicer\n",
      "     |      default message.\n",
      "     |  \n",
      "     |  assertIsNone(self, obj, msg=None)\n",
      "     |      Same as self.assertTrue(obj is None), with a nicer default message.\n",
      "     |  \n",
      "     |  assertIsNot(self, expr1, expr2, msg=None)\n",
      "     |      Just like self.assertTrue(a is not b), but with a nicer default message.\n",
      "     |  \n",
      "     |  assertIsNotNone(self, obj, msg=None)\n",
      "     |      Included for symmetry with assertIsNone.\n",
      "     |  \n",
      "     |  assertLess(self, a, b, msg=None)\n",
      "     |      Just like self.assertTrue(a < b), but with a nicer default message.\n",
      "     |  \n",
      "     |  assertLessEqual(self, a, b, msg=None)\n",
      "     |      Just like self.assertTrue(a <= b), but with a nicer default message.\n",
      "     |  \n",
      "     |  assertListEqual(self, list1, list2, msg=None)\n",
      "     |      A list-specific equality assertion.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          list1: The first list to compare.\n",
      "     |          list2: The second list to compare.\n",
      "     |          msg: Optional message to use on failure instead of a list of\n",
      "     |                  differences.\n",
      "     |  \n",
      "     |  assertLogs(self, logger=None, level=None)\n",
      "     |      Fail unless a log message of level *level* or higher is emitted\n",
      "     |      on *logger_name* or its children.  If omitted, *level* defaults to\n",
      "     |      INFO and *logger* defaults to the root logger.\n",
      "     |      \n",
      "     |      This method must be used as a context manager, and will yield\n",
      "     |      a recording object with two attributes: `output` and `records`.\n",
      "     |      At the end of the context manager, the `output` attribute will\n",
      "     |      be a list of the matching formatted log messages and the\n",
      "     |      `records` attribute will be a list of the corresponding LogRecord\n",
      "     |      objects.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          with self.assertLogs('foo', level='INFO') as cm:\n",
      "     |              logging.getLogger('foo').info('first message')\n",
      "     |              logging.getLogger('foo.bar').error('second message')\n",
      "     |          self.assertEqual(cm.output, ['INFO:foo:first message',\n",
      "     |                                       'ERROR:foo.bar:second message'])\n",
      "     |  \n",
      "     |  assertMultiLineEqual(self, first, second, msg=None)\n",
      "     |      Assert that two multi-line strings are equal.\n",
      "     |  \n",
      "     |  assertNotAlmostEqual(self, first, second, places=None, msg=None, delta=None)\n",
      "     |      Fail if the two objects are equal as determined by their\n",
      "     |      difference rounded to the given number of decimal places\n",
      "     |      (default 7) and comparing to zero, or by comparing that the\n",
      "     |      between the two objects is less than the given delta.\n",
      "     |      \n",
      "     |      Note that decimal places (from zero) are usually not the same\n",
      "     |      as significant digits (measured from the most signficant digit).\n",
      "     |      \n",
      "     |      Objects that are equal automatically fail.\n",
      "     |  \n",
      "     |  assertNotAlmostEquals = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  assertNotEqual(self, first, second, msg=None)\n",
      "     |      Fail if the two objects are equal as determined by the '!='\n",
      "     |      operator.\n",
      "     |  \n",
      "     |  assertNotEquals = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  assertNotIn(self, member, container, msg=None)\n",
      "     |      Just like self.assertTrue(a not in b), but with a nicer default message.\n",
      "     |  \n",
      "     |  assertNotIsInstance(self, obj, cls, msg=None)\n",
      "     |      Included for symmetry with assertIsInstance.\n",
      "     |  \n",
      "     |  assertNotRegex(self, text, unexpected_regex, msg=None)\n",
      "     |      Fail the test if the text matches the regular expression.\n",
      "     |  \n",
      "     |  assertRaises(self, excClass, callableObj=None, *args, **kwargs)\n",
      "     |      Fail unless an exception of class excClass is raised\n",
      "     |      by callableObj when invoked with arguments args and keyword\n",
      "     |      arguments kwargs. If a different type of exception is\n",
      "     |      raised, it will not be caught, and the test case will be\n",
      "     |      deemed to have suffered an error, exactly as for an\n",
      "     |      unexpected exception.\n",
      "     |      \n",
      "     |      If called with callableObj omitted or None, will return a\n",
      "     |      context object used like this::\n",
      "     |      \n",
      "     |           with self.assertRaises(SomeException):\n",
      "     |               do_something()\n",
      "     |      \n",
      "     |      An optional keyword argument 'msg' can be provided when assertRaises\n",
      "     |      is used as a context object.\n",
      "     |      \n",
      "     |      The context manager keeps a reference to the exception as\n",
      "     |      the 'exception' attribute. This allows you to inspect the\n",
      "     |      exception after the assertion::\n",
      "     |      \n",
      "     |          with self.assertRaises(SomeException) as cm:\n",
      "     |              do_something()\n",
      "     |          the_exception = cm.exception\n",
      "     |          self.assertEqual(the_exception.error_code, 3)\n",
      "     |  \n",
      "     |  assertRaisesRegex(self, expected_exception, expected_regex, callable_obj=None, *args, **kwargs)\n",
      "     |      Asserts that the message in a raised exception matches a regex.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expected_exception: Exception class expected to be raised.\n",
      "     |          expected_regex: Regex (re pattern object or string) expected\n",
      "     |                  to be found in error message.\n",
      "     |          callable_obj: Function to be called.\n",
      "     |          msg: Optional message used in case of failure. Can only be used\n",
      "     |                  when assertRaisesRegex is used as a context manager.\n",
      "     |          args: Extra args.\n",
      "     |          kwargs: Extra kwargs.\n",
      "     |  \n",
      "     |  assertRaisesRegexp = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  assertRegex(self, text, expected_regex, msg=None)\n",
      "     |      Fail the test unless the text matches the regular expression.\n",
      "     |  \n",
      "     |  assertRegexpMatches = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  assertSequenceEqual(self, seq1, seq2, msg=None, seq_type=None)\n",
      "     |      An equality assertion for ordered sequences (like lists and tuples).\n",
      "     |      \n",
      "     |      For the purposes of this function, a valid ordered sequence type is one\n",
      "     |      which can be indexed, has a length, and has an equality operator.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          seq1: The first sequence to compare.\n",
      "     |          seq2: The second sequence to compare.\n",
      "     |          seq_type: The expected datatype of the sequences, or None if no\n",
      "     |                  datatype should be enforced.\n",
      "     |          msg: Optional message to use on failure instead of a list of\n",
      "     |                  differences.\n",
      "     |  \n",
      "     |  assertSetEqual(self, set1, set2, msg=None)\n",
      "     |      A set-specific equality assertion.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set1: The first set to compare.\n",
      "     |          set2: The second set to compare.\n",
      "     |          msg: Optional message to use on failure instead of a list of\n",
      "     |                  differences.\n",
      "     |      \n",
      "     |      assertSetEqual uses ducktyping to support different types of sets, and\n",
      "     |      is optimized for sets specifically (parameters must support a\n",
      "     |      difference method).\n",
      "     |  \n",
      "     |  assertTrue(self, expr, msg=None)\n",
      "     |      Check that the expression is true.\n",
      "     |  \n",
      "     |  assertTupleEqual(self, tuple1, tuple2, msg=None)\n",
      "     |      A tuple-specific equality assertion.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          tuple1: The first tuple to compare.\n",
      "     |          tuple2: The second tuple to compare.\n",
      "     |          msg: Optional message to use on failure instead of a list of\n",
      "     |                  differences.\n",
      "     |  \n",
      "     |  assertWarns(self, expected_warning, callable_obj=None, *args, **kwargs)\n",
      "     |      Fail unless a warning of class warnClass is triggered\n",
      "     |      by callable_obj when invoked with arguments args and keyword\n",
      "     |      arguments kwargs.  If a different type of warning is\n",
      "     |      triggered, it will not be handled: depending on the other\n",
      "     |      warning filtering rules in effect, it might be silenced, printed\n",
      "     |      out, or raised as an exception.\n",
      "     |      \n",
      "     |      If called with callable_obj omitted or None, will return a\n",
      "     |      context object used like this::\n",
      "     |      \n",
      "     |           with self.assertWarns(SomeWarning):\n",
      "     |               do_something()\n",
      "     |      \n",
      "     |      An optional keyword argument 'msg' can be provided when assertWarns\n",
      "     |      is used as a context object.\n",
      "     |      \n",
      "     |      The context manager keeps a reference to the first matching\n",
      "     |      warning as the 'warning' attribute; similarly, the 'filename'\n",
      "     |      and 'lineno' attributes give you information about the line\n",
      "     |      of Python code from which the warning was triggered.\n",
      "     |      This allows you to inspect the warning after the assertion::\n",
      "     |      \n",
      "     |          with self.assertWarns(SomeWarning) as cm:\n",
      "     |              do_something()\n",
      "     |          the_warning = cm.warning\n",
      "     |          self.assertEqual(the_warning.some_attribute, 147)\n",
      "     |  \n",
      "     |  assertWarnsRegex(self, expected_warning, expected_regex, callable_obj=None, *args, **kwargs)\n",
      "     |      Asserts that the message in a triggered warning matches a regexp.\n",
      "     |      Basic functioning is similar to assertWarns() with the addition\n",
      "     |      that only warnings whose messages also match the regular expression\n",
      "     |      are considered successful matches.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expected_warning: Warning class expected to be triggered.\n",
      "     |          expected_regex: Regex (re pattern object or string) expected\n",
      "     |                  to be found in error message.\n",
      "     |          callable_obj: Function to be called.\n",
      "     |          msg: Optional message used in case of failure. Can only be used\n",
      "     |                  when assertWarnsRegex is used as a context manager.\n",
      "     |          args: Extra args.\n",
      "     |          kwargs: Extra kwargs.\n",
      "     |  \n",
      "     |  assert_ = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  countTestCases(self)\n",
      "     |  \n",
      "     |  debug(self)\n",
      "     |      Run the test without collecting errors in a TestResult\n",
      "     |  \n",
      "     |  defaultTestResult(self)\n",
      "     |  \n",
      "     |  doCleanups(self)\n",
      "     |      Execute all cleanup functions. Normally called for you after\n",
      "     |      tearDown.\n",
      "     |  \n",
      "     |  fail(self, msg=None)\n",
      "     |      Fail immediately, with the given message.\n",
      "     |  \n",
      "     |  failIf = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  failIfAlmostEqual = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  failIfEqual = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  failUnless = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  failUnlessAlmostEqual = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  failUnlessEqual = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  failUnlessRaises = deprecated_func(*args, **kwargs)\n",
      "     |  \n",
      "     |  id(self)\n",
      "     |  \n",
      "     |  run(self, result=None)\n",
      "     |  \n",
      "     |  shortDescription(self)\n",
      "     |      Returns a one-line description of the test, or None if no\n",
      "     |      description has been provided.\n",
      "     |      \n",
      "     |      The default implementation of this method returns the first line of\n",
      "     |      the specified test method's docstring.\n",
      "     |  \n",
      "     |  skipTest(self, reason)\n",
      "     |      Skip this test.\n",
      "     |  \n",
      "     |  subTest(self, msg=None, **params)\n",
      "     |      Return a context manager that will return the enclosed block\n",
      "     |      of code in a subtest identified by the optional message and\n",
      "     |      keyword parameters.  A failure in the subtest marks the test\n",
      "     |      case as failed but resumes execution at the end of the enclosed\n",
      "     |      block, allowing further test code to be executed.\n",
      "     |  \n",
      "     |  tearDown(self)\n",
      "     |      Hook method for deconstructing the test fixture after testing it.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from unittest.case.TestCase:\n",
      "     |  \n",
      "     |  setUpClass() from builtins.type\n",
      "     |      Hook method for setting up class fixture before running tests in the class.\n",
      "     |  \n",
      "     |  tearDownClass() from builtins.type\n",
      "     |      Hook method for deconstructing the class fixture after running all tests in the class.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from unittest.case.TestCase:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from unittest.case.TestCase:\n",
      "     |  \n",
      "     |  failureException = <class 'AssertionError'>\n",
      "     |      Assertion failed.\n",
      "     |  \n",
      "     |  longMessage = True\n",
      "     |  \n",
      "     |  maxDiff = 640\n",
      "\n",
      "FUNCTIONS\n",
      "    datapath lambda fname\n",
      "    \n",
      "    testfile()\n",
      "\n",
      "DATA\n",
      "    corpus = [[(0, 1), (1, 1), (2, 1)], [(1, 1), (3, 1), (4, 1), (5, 1), (...\n",
      "    dictionary = <gensim.corpora.dictionary.Dictionary object>\n",
      "    module_path = r'D:\\360Downloads\\Anaconda3\\lib\\site-packages\\gensim-0.1...\n",
      "    texts = [['human', 'interface', 'computer'], ['survey', 'user', 'compu...\n",
      "\n",
      "FILE\n",
      "    d:\\360downloads\\anaconda3\\lib\\site-packages\\gensim-0.12.4-py3.4-win-amd64.egg\\gensim\\test\\test_ldamodel.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.test import test_ldamodel\n",
    "\n",
    "help(test_ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package gensim.models in gensim:\n",
      "\n",
      "NAME\n",
      "    gensim.models\n",
      "\n",
      "DESCRIPTION\n",
      "    This package contains algorithms for extracting document representations from their raw\n",
      "    bag-of-word counts.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    doc2vec\n",
      "    hdpmodel\n",
      "    lda_dispatcher\n",
      "    lda_worker\n",
      "    ldamodel\n",
      "    ldamulticore\n",
      "    logentropy_model\n",
      "    lsi_dispatcher\n",
      "    lsi_worker\n",
      "    lsimodel\n",
      "    phrases\n",
      "    rpmodel\n",
      "    tfidfmodel\n",
      "    word2vec\n",
      "    wrappers (package)\n",
      "\n",
      "CLASSES\n",
      "    gensim.interfaces.TransformationABC(gensim.utils.SaveLoad)\n",
      "        VocabTransform\n",
      "    \n",
      "    class VocabTransform(gensim.interfaces.TransformationABC)\n",
      "     |  Remap feature ids to new values.\n",
      "     |  \n",
      "     |  Given a mapping between old ids and new ids (some old ids may be missing = these\n",
      "     |  features are to be discarded), this will wrap a corpus so that iterating over\n",
      "     |  `VocabTransform[corpus]` returns the same vectors but with the new ids.\n",
      "     |  \n",
      "     |  Old features that have no counterpart in the new ids are discarded. This\n",
      "     |  can be used to filter vocabulary of a corpus \"online\"::\n",
      "     |  \n",
      "     |  >>> old2new = dict((oldid, newid) for newid, oldid in enumerate(ids_you_want_to_keep))\n",
      "     |  >>> vt = VocabTransform(old2new)\n",
      "     |  >>> for vec_with_new_ids in vt[corpus_with_old_ids]:\n",
      "     |  >>>     ...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VocabTransform\n",
      "     |      gensim.interfaces.TransformationABC\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, bow)\n",
      "     |      Return representation with the ids transformed.\n",
      "     |  \n",
      "     |  __init__(self, old2new, id2token=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=2)\n",
      "     |      Save the object to file (also see `load`).\n",
      "     |      \n",
      "     |      `fname_or_handle` is either a string specifying the file name to\n",
      "     |      save to, or an open file-like object which can be written to. If\n",
      "     |      the object is a file handle, no special array handling will be\n",
      "     |      performed; all attributes will be saved to the same file.\n",
      "     |      \n",
      "     |      If `separately` is None, automatically detect large\n",
      "     |      numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |      them into separate files. This avoids pickle memory errors and\n",
      "     |      allows mmap'ing large arrays back on load efficiently.\n",
      "     |      \n",
      "     |      You can also set `separately` manually, in which case it must be\n",
      "     |      a list of attribute names to be stored in separate files. The\n",
      "     |      automatic check is not performed in this case.\n",
      "     |      \n",
      "     |      `ignore` is a set of attribute names to *not* serialize (file\n",
      "     |      handles, caches etc). On subsequent load() these attributes will\n",
      "     |      be set to None.\n",
      "     |      \n",
      "     |      `pickle_protocol` defaults to 2 so the pickled object can be imported\n",
      "     |      in both Python 2 and 3.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load a previously saved object from file (also see `save`).\n",
      "     |      \n",
      "     |      If the object was saved with large arrays stored separately, you can load\n",
      "     |      these arrays via mmap (shared memory) using `mmap='r'`. Default: don't use\n",
      "     |      mmap, load large arrays as normal objects.\n",
      "     |      \n",
      "     |      If the file being loaded is compressed (either '.gz' or '.bz2'), then\n",
      "     |      `mmap=None` must be set.  Load will raise an `IOError` if this condition\n",
      "     |      is encountered.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FILE\n",
      "    d:\\360downloads\\anaconda3\\lib\\site-packages\\gensim-0.12.4-py3.4-win-amd64.egg\\gensim\\models\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "help(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module gensim.models.ldamodel in gensim.models:\n",
      "\n",
      "NAME\n",
      "    gensim.models.ldamodel - **For a faster implementation of LDA (parallelized for multicore machines), see** :mod:`gensim.models.ldamulticore`.\n",
      "\n",
      "DESCRIPTION\n",
      "    Latent Dirichlet Allocation (LDA) in Python.\n",
      "    \n",
      "    This module allows both LDA model estimation from a training corpus and inference of topic\n",
      "    distribution on new, unseen documents. The model can also be updated with new documents\n",
      "    for online training.\n",
      "    \n",
      "    The core estimation code is based on the `onlineldavb.py` script by M. Hoffman [1]_, see\n",
      "    **Hoffman, Blei, Bach: Online Learning for Latent Dirichlet Allocation, NIPS 2010.**\n",
      "    \n",
      "    The algorithm:\n",
      "    \n",
      "    * is **streamed**: training documents may come in sequentially, no random access required,\n",
      "    * runs in **constant memory** w.r.t. the number of documents: size of the\n",
      "      training corpus does not affect memory footprint, can process corpora larger than RAM, and\n",
      "    * is **distributed**: makes use of a cluster of machines, if available, to\n",
      "      speed up model estimation.\n",
      "    \n",
      "    .. [1] http://www.cs.princeton.edu/~mdhoffma\n",
      "\n",
      "CLASSES\n",
      "    gensim.interfaces.TransformationABC(gensim.utils.SaveLoad)\n",
      "        LdaModel\n",
      "    gensim.utils.SaveLoad(builtins.object)\n",
      "        LdaState\n",
      "    \n",
      "    class LdaModel(gensim.interfaces.TransformationABC)\n",
      "     |  The constructor estimates Latent Dirichlet Allocation model parameters based\n",
      "     |  on a training corpus:\n",
      "     |  \n",
      "     |  >>> lda = LdaModel(corpus, num_topics=10)\n",
      "     |  \n",
      "     |  You can then infer topic distributions on new, unseen documents, with\n",
      "     |  \n",
      "     |  >>> doc_lda = lda[doc_bow]\n",
      "     |  \n",
      "     |  The model can be updated (trained) with new documents via\n",
      "     |  \n",
      "     |  >>> lda.update(other_corpus)\n",
      "     |  \n",
      "     |  Model persistency is achieved through its `load`/`save` methods.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LdaModel\n",
      "     |      gensim.interfaces.TransformationABC\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, bow, eps=None)\n",
      "     |      Return topic distribution for the given document `bow`, as a list of\n",
      "     |      (topic_id, topic_probability) 2-tuples.\n",
      "     |      \n",
      "     |      Ignore topics with very low probability (below `eps`).\n",
      "     |  \n",
      "     |  __init__(self, corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01)\n",
      "     |      If given, start training from the iterable `corpus` straight away. If not given,\n",
      "     |      the model is left untrained (presumably because you want to call `update()` manually).\n",
      "     |      \n",
      "     |      `num_topics` is the number of requested latent topics to be extracted from\n",
      "     |      the training corpus.\n",
      "     |      \n",
      "     |      `id2word` is a mapping from word ids (integers) to words (strings). It is\n",
      "     |      used to determine the vocabulary size, as well as for debugging and topic\n",
      "     |      printing.\n",
      "     |      \n",
      "     |      `alpha` and `eta` are hyperparameters that affect sparsity of the document-topic\n",
      "     |      (theta) and topic-word (lambda) distributions. Both default to a symmetric\n",
      "     |      1.0/num_topics prior.\n",
      "     |      \n",
      "     |      `alpha` can be set to an explicit array = prior of your choice. It also\n",
      "     |      support special values of 'asymmetric' and 'auto': the former uses a fixed\n",
      "     |      normalized asymmetric 1.0/topicno prior, the latter learns an asymmetric\n",
      "     |      prior directly from your data.\n",
      "     |      \n",
      "     |      `eta` can be a scalar for a symmetric prior over topic/word\n",
      "     |      distributions, or a matrix of shape num_topics x num_words, which can\n",
      "     |      be used to impose asymmetric priors over the word distribution on a\n",
      "     |      per-topic basis. This may be useful if you want to seed certain topics\n",
      "     |      with particular words by boosting the priors for those words.  It also\n",
      "     |      supports the special value 'auto', which learns an asymmetric prior\n",
      "     |      directly from your data.\n",
      "     |      \n",
      "     |      Turn on `distributed` to force distributed computing (see the `web tutorial <http://radimrehurek.com/gensim/distributed.html>`_\n",
      "     |      on how to set up a cluster of machines for gensim).\n",
      "     |      \n",
      "     |      Calculate and log perplexity estimate from the latest mini-batch every\n",
      "     |      `eval_every` model updates (setting this to 1 slows down training ~2x;\n",
      "     |      default is 10 for better performance). Set to None to disable perplexity estimation.\n",
      "     |      \n",
      "     |      `decay` and `offset` parameters are the same as Kappa and Tau_0 in\n",
      "     |      Hoffman et al, respectively.\n",
      "     |      \n",
      "     |      `minimum_probability` controls filtering the topics returned for a document (bow).\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      \n",
      "     |      >>> lda = LdaModel(corpus, num_topics=100)  # train model\n",
      "     |      >>> print(lda[doc_bow]) # get topic probability distribution for a document\n",
      "     |      >>> lda.update(corpus2) # update the LDA model with additional documents\n",
      "     |      >>> print(lda[doc_bow])\n",
      "     |      \n",
      "     |      >>> lda = LdaModel(corpus, num_topics=50, alpha='auto', eval_every=5)  # train asymmetric alpha from data\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |  \n",
      "     |  bound(self, corpus, gamma=None, subsample_ratio=1.0)\n",
      "     |      Estimate the variational bound of documents from `corpus`:\n",
      "     |      E_q[log p(corpus)] - E_q[log q(corpus)]\n",
      "     |      \n",
      "     |      `gamma` are the variational parameters on topic weights for each `corpus`\n",
      "     |      document (=2d matrix=what comes out of `inference()`).\n",
      "     |      If not supplied, will be inferred from the model.\n",
      "     |  \n",
      "     |  clear(self)\n",
      "     |      Clear model state (free up some memory). Used in the distributed algo.\n",
      "     |  \n",
      "     |  do_estep(self, chunk, state=None)\n",
      "     |      Perform inference on a chunk of documents, and accumulate the collected\n",
      "     |      sufficient statistics in `state` (or `self.state` if None).\n",
      "     |  \n",
      "     |  do_mstep(self, rho, other, extra_pass=False)\n",
      "     |      M step: use linear interpolation between the existing topics and\n",
      "     |      collected sufficient statistics in `other` to update the topics.\n",
      "     |  \n",
      "     |  get_document_topics(self, bow, minimum_probability=None)\n",
      "     |      Return topic distribution for the given document `bow`, as a list of\n",
      "     |      (topic_id, topic_probability) 2-tuples.\n",
      "     |      \n",
      "     |      Ignore topics with very low probability (below `minimum_probability`).\n",
      "     |  \n",
      "     |  get_topic_terms(self, topicid, topn=10)\n",
      "     |      Return a list of `(word_id, probability)` 2-tuples for the most\n",
      "     |      probable words in topic `topicid`.\n",
      "     |      \n",
      "     |      Only return 2-tuples for the topn most probable words (ignore the rest).\n",
      "     |  \n",
      "     |  inference(self, chunk, collect_sstats=False)\n",
      "     |      Given a chunk of sparse document vectors, estimate gamma (parameters\n",
      "     |      controlling the topic weights) for each document in the chunk.\n",
      "     |      \n",
      "     |      This function does not modify the model (=is read-only aka const). The\n",
      "     |      whole input chunk of document is assumed to fit in RAM; chunking of a\n",
      "     |      large corpus must be done earlier in the pipeline.\n",
      "     |      \n",
      "     |      If `collect_sstats` is True, also collect sufficient statistics needed\n",
      "     |      to update the model's topic-word distributions, and return a 2-tuple\n",
      "     |      `(gamma, sstats)`. Otherwise, return `(gamma, None)`. `gamma` is of shape\n",
      "     |      `len(chunk) x self.num_topics`.\n",
      "     |      \n",
      "     |      Avoids computing the `phi` variational parameter directly using the\n",
      "     |      optimization presented in **Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001**.\n",
      "     |  \n",
      "     |  init_dir_prior(self, prior, name)\n",
      "     |  \n",
      "     |  log_perplexity(self, chunk, total_docs=None)\n",
      "     |      Calculate and return per-word likelihood bound, using the `chunk` of\n",
      "     |      documents as evaluation corpus. Also output the calculated statistics. incl.\n",
      "     |      perplexity=2^(-bound), to log at INFO level.\n",
      "     |  \n",
      "     |  print_topic(self, topicid, topn=10)\n",
      "     |      Return the result of `show_topic`, but formatted as a single string.\n",
      "     |  \n",
      "     |  print_topics(self, num_topics=10, num_words=10)\n",
      "     |  \n",
      "     |  save(self, fname, ignore=['state', 'dispatcher'], *args, **kwargs)\n",
      "     |      Save the model to file.\n",
      "     |      \n",
      "     |      Large internal arrays may be stored into separate files, with `fname` as prefix.\n",
      "     |      \n",
      "     |      `separately` can be used to define which arrays should be stored in separate files.\n",
      "     |      \n",
      "     |      `ignore` parameter can be used to define which variables should be ignored, i.e. left\n",
      "     |      out from the pickled lda model. By default the internal `state` is ignored as it uses\n",
      "     |      its own serialisation not the one provided by `LdaModel`. The `state` and `dispatcher\n",
      "     |      will be added to any ignore parameter defined.\n",
      "     |      \n",
      "     |      \n",
      "     |      Note: do not save as a compressed file if you intend to load the file back with `mmap`.\n",
      "     |      \n",
      "     |      Note: If you intend to use models across Python 2/3 versions there are a few things to\n",
      "     |      keep in mind:\n",
      "     |      \n",
      "     |        1. The pickled Python dictionaries will not work across Python versions\n",
      "     |        2. The `save` method does not automatically save all NumPy arrays using NumPy, only\n",
      "     |           those ones that exceed `sep_limit` set in `gensim.utils.SaveLoad.save`. The main\n",
      "     |           concern here is the `alpha` array if for instance using `alpha='auto'`.\n",
      "     |      \n",
      "     |      Please refer to the wiki recipes section (https://github.com/piskvorky/gensim/wiki/Recipes-&-FAQ#q9-how-do-i-load-a-model-in-python-3-that-was-trained-and-saved-using-python-2)\n",
      "     |      for an example on how to work around these issues.\n",
      "     |  \n",
      "     |  show_topic(self, topicid, topn=10)\n",
      "     |      Return a list of `(word, probability)` 2-tuples for the most probable\n",
      "     |      words in topic `topicid`.\n",
      "     |      \n",
      "     |      Only return 2-tuples for the topn most probable words (ignore the rest).\n",
      "     |  \n",
      "     |  show_topics(self, num_topics=10, num_words=10, log=False, formatted=True)\n",
      "     |      For `num_topics` number of topics, return `num_words` most significant words\n",
      "     |      (10 words per topic, by default).\n",
      "     |      \n",
      "     |      The topics are returned as a list -- a list of strings if `formatted` is\n",
      "     |      True, or a list of `(word, probability)` 2-tuples if False.\n",
      "     |      \n",
      "     |      If `log` is True, also output this result to log.\n",
      "     |      \n",
      "     |      Unlike LSA, there is no natural ordering between the topics in LDA.\n",
      "     |      The returned `num_topics <= self.num_topics` subset of all topics is therefore\n",
      "     |      arbitrary and may change between two LDA training runs.\n",
      "     |  \n",
      "     |  sync_state(self)\n",
      "     |  \n",
      "     |  top_topics(self, corpus, num_words=20)\n",
      "     |      Calculate the Umass topic coherence for each topic. Algorithm from\n",
      "     |      **Mimno, Wallach, Talley, Leenders, McCallum: Optimizing Semantic Coherence in Topic Models, CEMNLP 2011.**\n",
      "     |  \n",
      "     |  update(self, corpus, chunksize=None, decay=None, offset=None, passes=None, update_every=None, eval_every=None, iterations=None, gamma_threshold=None, chunks_as_numpy=False)\n",
      "     |      Train the model with new documents, by EM-iterating over `corpus` until\n",
      "     |      the topics converge (or until the maximum number of allowed iterations\n",
      "     |      is reached). `corpus` must be an iterable (repeatable stream of documents),\n",
      "     |      \n",
      "     |      In distributed mode, the E step is distributed over a cluster of machines.\n",
      "     |      \n",
      "     |      This update also supports updating an already trained model (`self`)\n",
      "     |      with new documents from `corpus`; the two models are then merged in\n",
      "     |      proportion to the number of old vs. new documents. This feature is still\n",
      "     |      experimental for non-stationary input streams.\n",
      "     |      \n",
      "     |      For stationary input (no topic drift in new documents), on the other hand,\n",
      "     |      this equals the online update of Hoffman et al. and is guaranteed to\n",
      "     |      converge for any `decay` in (0.5, 1.0>. Additionally, for smaller\n",
      "     |      `corpus` sizes, an increasing `offset` may be beneficial (see\n",
      "     |      Table 1 in Hoffman et al.)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ------------\n",
      "     |      corpus: (gensim corpus object, list of tuples)\n",
      "     |          The corpus with which the LDA model should be updated with.\n",
      "     |      \n",
      "     |      chunks_as_numpy: bool\n",
      "     |          Whether each chunk passed to `.inference` should be a numpy\n",
      "     |          array of not. Numpy can in some settings turn the term IDs\n",
      "     |          into floats, these will be converted back into integers in\n",
      "     |          inference, which incurs a performance hit. For distributed\n",
      "     |          computing it may be desirable to keep the chunks as numpy\n",
      "     |          arrays.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      For other parameter settings see LdaModel().\n",
      "     |  \n",
      "     |  update_alpha(self, gammat, rho)\n",
      "     |      Update parameters for the Dirichlet prior on the per-document\n",
      "     |      topic weights `alpha` given the last `gammat`.\n",
      "     |  \n",
      "     |  update_eta(self, lambdat, rho)\n",
      "     |      Update parameters for the Dirichlet prior on the per-topic\n",
      "     |      word weights `eta` given the last `lambdat`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  load(fname, *args, **kwargs) from builtins.type\n",
      "     |      Load a previously saved object from file (also see `save`).\n",
      "     |      \n",
      "     |      Large arrays can be memmap'ed back as read-only (shared memory) by setting `mmap='r'`:\n",
      "     |      \n",
      "     |          >>> LdaModel.load(fname, mmap='r')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LdaState(gensim.utils.SaveLoad)\n",
      "     |  Encapsulate information for distributed computation of LdaModel objects.\n",
      "     |  \n",
      "     |  Objects of this class are sent over the network, so try to keep them lean to\n",
      "     |  reduce traffic.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LdaState\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, eta, shape)\n",
      "     |  \n",
      "     |  blend(self, rhot, other, targetsize=None)\n",
      "     |      Given LdaState `other`, merge it with the current state. Stretch both to\n",
      "     |      `targetsize` documents before merging, so that they are of comparable\n",
      "     |      magnitude.\n",
      "     |      \n",
      "     |      Merging is done by average weighting: in the extremes, `rhot=0.0` means\n",
      "     |      `other` is completely ignored; `rhot=1.0` means `self` is completely ignored.\n",
      "     |      \n",
      "     |      This procedure corresponds to the stochastic gradient update from Hoffman\n",
      "     |      et al., algorithm 2 (eq. 14).\n",
      "     |  \n",
      "     |  blend2(self, rhot, other, targetsize=None)\n",
      "     |      Alternative, more simple blend.\n",
      "     |  \n",
      "     |  get_Elogbeta(self)\n",
      "     |  \n",
      "     |  get_lambda(self)\n",
      "     |  \n",
      "     |  merge(self, other)\n",
      "     |      Merge the result of an E step from one node with that of another node\n",
      "     |      (summing up sufficient statistics).\n",
      "     |      \n",
      "     |      The merging is trivial and after merging all cluster nodes, we have the\n",
      "     |      exact same result as if the computation was run on a single node (no\n",
      "     |      approximation).\n",
      "     |  \n",
      "     |  reset(self)\n",
      "     |      Prepare the state for a new EM iteration (reset sufficient stats).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=2)\n",
      "     |      Save the object to file (also see `load`).\n",
      "     |      \n",
      "     |      `fname_or_handle` is either a string specifying the file name to\n",
      "     |      save to, or an open file-like object which can be written to. If\n",
      "     |      the object is a file handle, no special array handling will be\n",
      "     |      performed; all attributes will be saved to the same file.\n",
      "     |      \n",
      "     |      If `separately` is None, automatically detect large\n",
      "     |      numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |      them into separate files. This avoids pickle memory errors and\n",
      "     |      allows mmap'ing large arrays back on load efficiently.\n",
      "     |      \n",
      "     |      You can also set `separately` manually, in which case it must be\n",
      "     |      a list of attribute names to be stored in separate files. The\n",
      "     |      automatic check is not performed in this case.\n",
      "     |      \n",
      "     |      `ignore` is a set of attribute names to *not* serialize (file\n",
      "     |      handles, caches etc). On subsequent load() these attributes will\n",
      "     |      be set to None.\n",
      "     |      \n",
      "     |      `pickle_protocol` defaults to 2 so the pickled object can be imported\n",
      "     |      in both Python 2 and 3.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load a previously saved object from file (also see `save`).\n",
      "     |      \n",
      "     |      If the object was saved with large arrays stored separately, you can load\n",
      "     |      these arrays via mmap (shared memory) using `mmap='r'`. Default: don't use\n",
      "     |      mmap, load large arrays as normal objects.\n",
      "     |      \n",
      "     |      If the file being loaded is compressed (either '.gz' or '.bz2'), then\n",
      "     |      `mmap=None` must be set.  Load will raise an `IOError` if this condition\n",
      "     |      is encountered.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    dirichlet_expectation(alpha)\n",
      "        For a vector `theta~Dir(alpha)`, compute `E[log(theta)]`.\n",
      "    \n",
      "    update_dir_prior(prior, N, logphat, rho)\n",
      "        Updates a given prior using Newton's method, described in\n",
      "        **Huang: Maximum Likelihood Estimation of Dirichlet Distribution Parameters.**\n",
      "        http://jonathan-huang.org/research/dirichlet/dirichlet.pdf\n",
      "\n",
      "DATA\n",
      "    gammaln = <ufunc 'gammaln'>\n",
      "    logger = <logging.Logger object>\n",
      "    psi = <ufunc 'psi'>\n",
      "\n",
      "FILE\n",
      "    d:\\360downloads\\anaconda3\\lib\\site-packages\\gensim-0.12.4-py3.4-win-amd64.egg\\gensim\\models\\ldamodel.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(models.ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus, num_topics=100)  # train model\n",
    "print(lda[doc_bow]) # get topic probability distribution for a document\n",
    "lda.update(corpus2) # update the LDA model with additional documents\n",
    "print(lda[doc_bow])\n",
    "lda = LdaModel(corpus, num_topics=50, alpha='auto', eval_every=5)  # train asymmetric alpha from data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoding:utf-8\n",
    "'''\n",
    "Created on 2015年10月25日\n",
    "\n",
    "@author: Administrator\n",
    "'''\n",
    "import pandas  as pd\n",
    "import re\n",
    "import jieba  \n",
    "import nltk  \n",
    "import jieba.posseg as pseg  \n",
    "from gensim import corpora, models, similarities\n",
    "df=pd.read_csv(u'C:\\\\Users\\\\Administrator\\\\Desktop\\\\质量记录.csv',encoding='gbk')\n",
    "cont=df['QUALITYDESC'].map(lambda x:re.sub(ur'客户反应|客户|反映','',x)).map(lambda x:re.sub(r'[a-zA-Z0-9\\.]','',x))\n",
    "#导入自己添加的用户词语\n",
    "jieba.load_userdict(u'C:\\\\Users\\\\Administrator\\\\Desktop\\\\分词.txt')\n",
    "\n",
    "nwordall = []  \n",
    "for t in cont:  \n",
    "        words =pseg.cut(t)  \n",
    "        nword = ['']  \n",
    "        for w in words:  \n",
    "            if((w.flag == 'n'or w.flag == 'v' or w.flag == 'a') and len(w.word)>1):  \n",
    "                nword.append(w.word)  \n",
    "        nwordall.append(nword) \n",
    "# 选择后的词生成字典  \n",
    "dictionary = corpora.Dictionary(nwordall)#用于生成字典类似与table，Counter模块中count\n",
    "    #print dictionary.token2id  \n",
    "# 生成语料库   \n",
    "corpus = [dictionary.doc2bow(text) for text in nwordall]\n",
    " #tfidf加权  \n",
    "tfidf = models.TfidfModel(corpus)  \n",
    "# print tfidf.dfsx  \n",
    "# print tfidf.idf  \n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    " # 4. 主题模型lda，可用于降维  \n",
    "#lda流式数据建模计算，每块10000条记录，提取50个主题  \n",
    "lda = models.ldamodel.LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=50,     update_every=1, chunksize=10000, passes=1)  \n",
    "#提取前面20个主题\n",
    "for i in range(0,20):  \n",
    "    print lda.print_topics(i)[0]  \n",
    "\n",
    "#lda全部数据建模，提取100个主题  \n",
    "#lda = models.ldamodel.LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=100, update_every=0, passes=20)  \n",
    "#利用原模型预测新文本主题  \n",
    "#doc_lda = lda[corpus_tfidf]\n",
    "\n",
    " \n",
    "#5. word2vec 词向量化，可用于比较词相似度，寻找对应关系，词聚类  \n",
    "#sentences = models.word2vec.LineSentence(nwordall)  \n",
    "#size为词向量维度数,windows窗口范围,min_count频数小于5的词忽略,workers是线程数  \n",
    "model = models.word2vec.Word2Vec(nwordall, size=100, window=5, min_count=5, workers=4)\n",
    "print model[u'指示灯']  \n",
    "#向量表示  \n",
    "sim = model.most_similar(positive=[u'指示灯', u'灯不亮'])  \n",
    "    #相近词  \n",
    "for s in sim:  \n",
    "    print \"word:%s,similar:%s \" %(s[0],s[1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.svm in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.svm - The :mod:`sklearn.svm` module includes Support Vector Machine algorithms.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    base\n",
      "    bounds\n",
      "    classes\n",
      "    liblinear\n",
      "    libsvm\n",
      "    libsvm_sparse\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        sklearn.svm.classes.LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.feature_selection.from_model._LearntSelectorMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "    sklearn.base.RegressorMixin(builtins.object)\n",
      "        sklearn.svm.classes.LinearSVR(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.svm.classes.NuSVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
      "        sklearn.svm.classes.SVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
      "    sklearn.feature_selection.from_model._LearntSelectorMixin(sklearn.base.TransformerMixin)\n",
      "        sklearn.svm.classes.LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.feature_selection.from_model._LearntSelectorMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "    sklearn.linear_model.base.LinearClassifierMixin(sklearn.base.ClassifierMixin)\n",
      "        sklearn.svm.classes.LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.feature_selection.from_model._LearntSelectorMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "    sklearn.linear_model.base.LinearModel(abc.NewBase)\n",
      "        sklearn.svm.classes.LinearSVR(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "    sklearn.linear_model.base.SparseCoefMixin(builtins.object)\n",
      "        sklearn.svm.classes.LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.feature_selection.from_model._LearntSelectorMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "    sklearn.svm.base.BaseLibSVM(abc.NewBase)\n",
      "        sklearn.svm.classes.NuSVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
      "        sklearn.svm.classes.OneClassSVM\n",
      "        sklearn.svm.classes.SVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
      "    sklearn.svm.base.BaseSVC(sklearn.svm.base.BaseLibSVM, sklearn.base.ClassifierMixin)\n",
      "        sklearn.svm.classes.NuSVC\n",
      "        sklearn.svm.classes.SVC\n",
      "    \n",
      "    class LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.feature_selection.from_model._LearntSelectorMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "     |  Linear Support Vector Classification.\n",
      "     |  \n",
      "     |  Similar to SVC with parameter kernel='linear', but implemented in terms of\n",
      "     |  liblinear rather than libsvm, so it has more flexibility in the choice of\n",
      "     |  penalties and loss functions and should scale better (to large numbers of\n",
      "     |  samples).\n",
      "     |  \n",
      "     |  This class supports both dense and sparse input and the multiclass support\n",
      "     |  is handled according to a one-vs-the-rest scheme.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  C : float, optional (default=1.0)\n",
      "     |      Penalty parameter C of the error term.\n",
      "     |  \n",
      "     |  loss : string, 'hinge' or 'squared_hinge' (default='squared_hinge')\n",
      "     |      Specifies the loss function. 'hinge' is the standard SVM loss\n",
      "     |      (used e.g. by the SVC class) while 'squared_hinge' is the\n",
      "     |      square of the hinge loss.\n",
      "     |  \n",
      "     |  penalty : string, 'l1' or 'l2' (default='l2')\n",
      "     |      Specifies the norm used in the penalization. The 'l2'\n",
      "     |      penalty is the standard used in SVC. The 'l1' leads to `coef_`\n",
      "     |      vectors that are sparse.\n",
      "     |  \n",
      "     |  dual : bool, (default=True)\n",
      "     |      Select the algorithm to either solve the dual or primal\n",
      "     |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
      "     |  \n",
      "     |  tol : float, optional (default=1e-4)\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  multi_class: string, 'ovr' or 'crammer_singer' (default='ovr')\n",
      "     |      Determines the multi-class strategy if `y` contains more than\n",
      "     |      two classes.\n",
      "     |      `ovr` trains n_classes one-vs-rest classifiers, while `crammer_singer`\n",
      "     |      optimizes a joint objective over all classes.\n",
      "     |      While `crammer_singer` is interesting from an theoretical perspective\n",
      "     |      as it is consistent it is seldom used in practice and rarely leads to\n",
      "     |      better accuracy and is more expensive to compute.\n",
      "     |      If `crammer_singer` is chosen, the options loss, penalty and dual will\n",
      "     |      be ignored.\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional (default=True)\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  intercept_scaling : float, optional (default=1)\n",
      "     |      When self.fit_intercept is True, instance vector x becomes\n",
      "     |      [x, self.intercept_scaling],\n",
      "     |      i.e. a \"synthetic\" feature with constant value equals to\n",
      "     |      intercept_scaling is appended to the instance vector.\n",
      "     |      The intercept becomes intercept_scaling * synthetic feature weight\n",
      "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "     |      as all other features.\n",
      "     |      To lessen the effect of regularization on synthetic feature weight\n",
      "     |      (and therefore on the intercept) intercept_scaling has to be increased\n",
      "     |  \n",
      "     |  class_weight : {dict, 'auto'}, optional\n",
      "     |      Set the parameter C of class i to class_weight[i]*C for\n",
      "     |      SVC. If not given, all classes are supposed to have\n",
      "     |      weight one. The 'auto' mode uses the values of y to\n",
      "     |      automatically adjust weights inversely proportional to\n",
      "     |      class frequencies.\n",
      "     |  \n",
      "     |  verbose : int, (default=0)\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in liblinear that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  random_state : int seed, RandomState instance, or None (default=None)\n",
      "     |      The seed of the pseudo random number generator to use when\n",
      "     |      shuffling the data.\n",
      "     |  \n",
      "     |  max_iter : int, (default=1000)\n",
      "     |      The maximum number of iterations to be run.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = [n_features] if n_classes == 2             else [n_classes, n_features]\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is a readonly property derived from `raw_coef_` that         follows the internal memory layout of liblinear.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The underlying C implementation uses a random number generator to\n",
      "     |  select features when fitting the model. It is thus not uncommon,\n",
      "     |  to have slightly different results for the same input data. If\n",
      "     |  that happens, try with a smaller tol parameter.\n",
      "     |  \n",
      "     |  The underlying implementation (liblinear) uses a sparse internal\n",
      "     |  representation for the data that will incur a memory copy.\n",
      "     |  \n",
      "     |  Predict output may not match that of standalone liblinear in certain\n",
      "     |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      "     |  in the narrative documentation.\n",
      "     |  \n",
      "     |  **References:**\n",
      "     |  `LIBLINEAR: A Library for Large Linear Classification\n",
      "     |  <http://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  SVC\n",
      "     |      Implementation of Support Vector Machine classifier using libsvm:\n",
      "     |      the kernel can be non-linear but its SMO algorithm does not\n",
      "     |      scale to large number of samples as LinearSVC does.\n",
      "     |  \n",
      "     |      Furthermore SVC multi-class mode is implemented using one\n",
      "     |      vs one scheme while LinearSVC uses one vs the rest. It is\n",
      "     |      possible to implement one vs the rest with SVC by using the\n",
      "     |      :class:`sklearn.multiclass.OneVsRestClassifier` wrapper.\n",
      "     |  \n",
      "     |      Finally SVC can fit dense data without memory copy if the input\n",
      "     |      is C-contiguous. Sparse data will still incur memory copy though.\n",
      "     |  \n",
      "     |  sklearn.linear_model.SGDClassifier\n",
      "     |      SGDClassifier can optimize the same cost function as LinearSVC\n",
      "     |      by adjusting the penalty and loss parameters. In addition it requires\n",
      "     |      less memory, allows incremental (online) learning, and implements\n",
      "     |      various loss functions and regularization regimes.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LinearSVC\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.feature_selection.from_model._LearntSelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.linear_model.base.SparseCoefMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training vector, where n_samples in the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target vector relative to X\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep: boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The former have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection.from_model._LearntSelectorMixin:\n",
      "     |  \n",
      "     |  transform(self, X, threshold=None)\n",
      "     |      Reduce X to its most important features.\n",
      "     |      \n",
      "     |      Uses ``coef_`` or ``feature_importances_`` to determine the most\n",
      "     |      important features.  For models with a ``coef_`` for each class, the\n",
      "     |      absolute sum over the classes is used.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array or scipy sparse matrix of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      threshold : string, float or None, optional (default=None)\n",
      "     |          The threshold value to use for feature selection. Features whose\n",
      "     |          importance is greater or equal are kept while the others are\n",
      "     |          discarded. If \"median\" (resp. \"mean\"), then the threshold value is\n",
      "     |          the median (resp. the mean) of the feature importances. A scaling\n",
      "     |          factor (e.g., \"1.25*mean\") may also be used. If None and if\n",
      "     |          available, the object attribute ``threshold`` is used. Otherwise,\n",
      "     |          \"mean\" is used by default.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self: estimator\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self: estimator\n",
      "    \n",
      "    class LinearSVR(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Linear Support Vector Regression.\n",
      "     |  \n",
      "     |  Similar to SVR with parameter kernel='linear', but implemented in terms of\n",
      "     |  liblinear rather than libsvm, so it has more flexibility in the choice of\n",
      "     |  penalties and loss functions and should scale better (to large numbers of\n",
      "     |  samples).\n",
      "     |  \n",
      "     |  This class supports both dense and sparse input.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  C : float, optional (default=1.0)\n",
      "     |      Penalty parameter C of the error term. The penalty is a squared\n",
      "     |      l2 penalty. The bigger this parameter, the less regularization is used.\n",
      "     |  \n",
      "     |  loss : string, 'epsilon_insensitive' or 'squared_epsilon_insensitive'            (default='epsilon_insensitive')\n",
      "     |      Specifies the loss function. 'l1' is the epsilon-insensitive loss\n",
      "     |      (standard SVR) while 'l2' is the squared epsilon-insensitive loss.\n",
      "     |  \n",
      "     |  epsilon : float, optional (default=0.1)\n",
      "     |      Epsilon parameter in the epsilon-insensitive loss function. Note\n",
      "     |      that the value of this parameter depends on the scale of the target\n",
      "     |      variable y. If unsure, set epsilon=0.\n",
      "     |  \n",
      "     |  dual : bool, (default=True)\n",
      "     |      Select the algorithm to either solve the dual or primal\n",
      "     |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
      "     |  \n",
      "     |  tol : float, optional (default=1e-4)\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional (default=True)\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  intercept_scaling : float, optional (default=1)\n",
      "     |      When self.fit_intercept is True, instance vector x becomes\n",
      "     |      [x, self.intercept_scaling],\n",
      "     |      i.e. a \"synthetic\" feature with constant value equals to\n",
      "     |      intercept_scaling is appended to the instance vector.\n",
      "     |      The intercept becomes intercept_scaling * synthetic feature weight\n",
      "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "     |      as all other features.\n",
      "     |      To lessen the effect of regularization on synthetic feature weight\n",
      "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "     |  \n",
      "     |  verbose : int, (default=0)\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in liblinear that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  random_state : int seed, RandomState instance, or None (default=None)\n",
      "     |      The seed of the pseudo random number generator to use when\n",
      "     |      shuffling the data.\n",
      "     |  \n",
      "     |  max_iter : int, (default=1000)\n",
      "     |      The maximum number of iterations to be run.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = [n_features] if n_classes == 2             else [n_classes, n_features]\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is a readonly property derived from `raw_coef_` that         follows the internal memory layout of liblinear.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  LinearSVC\n",
      "     |      Implementation of Support Vector Machine classifier using the\n",
      "     |      same library as this class (liblinear).\n",
      "     |  \n",
      "     |  SVR\n",
      "     |      Implementation of Support Vector Machine regression using libsvm:\n",
      "     |      the kernel can be non-linear but its SMO algorithm does not\n",
      "     |      scale to large number of samples as LinearSVC does.\n",
      "     |  \n",
      "     |  sklearn.linear_model.SGDRegressor\n",
      "     |      SGDRegressor can optimize the same cost function as LinearSVR\n",
      "     |      by adjusting the penalty and loss parameters. In addition it requires\n",
      "     |      less memory, allows incremental (online) learning, and implements\n",
      "     |      various loss functions and regularization regimes.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LinearSVR\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, epsilon=0.0, tol=0.0001, C=1.0, loss='epsilon_insensitive', fit_intercept=True, intercept_scaling=1.0, dual=True, verbose=0, random_state=None, max_iter=1000)\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training vector, where n_samples in the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target vector relative to X\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep: boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The former have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0, lower values are worse.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class NuSVC(sklearn.svm.base.BaseSVC)\n",
      "     |  Nu-Support Vector Classification.\n",
      "     |  \n",
      "     |  Similar to SVC but uses a parameter to control the number of support\n",
      "     |  vectors.\n",
      "     |  \n",
      "     |  The implementation is based on libsvm.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  nu : float, optional (default=0.5)\n",
      "     |      An upper bound on the fraction of training errors and a lower\n",
      "     |      bound of the fraction of support vectors. Should be in the\n",
      "     |      interval (0, 1].\n",
      "     |  \n",
      "     |  kernel : string, optional (default='rbf')\n",
      "     |       Specifies the kernel type to be used in the algorithm.\n",
      "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      "     |       a callable.\n",
      "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
      "     |       used to precompute the kernel matrix.\n",
      "     |  \n",
      "     |  degree : int, optional (default=3)\n",
      "     |      Degree of kernel function\n",
      "     |      is significant only in poly, rbf, sigmoid.\n",
      "     |  \n",
      "     |  gamma : float, optional (default=0.0)\n",
      "     |      Kernel coefficient for rbf and poly, if gamma is 0.0 then 1/n_features\n",
      "     |      will be taken.\n",
      "     |  \n",
      "     |  coef0 : float, optional (default=0.0)\n",
      "     |      Independent term in kernel function. It is only significant\n",
      "     |      in poly/sigmoid.\n",
      "     |  \n",
      "     |  probability: boolean, optional (default=False)\n",
      "     |      Whether to enable probability estimates. This must be enabled prior\n",
      "     |      to calling `fit`, and will slow down that method.\n",
      "     |  \n",
      "     |  shrinking: boolean, optional (default=True)\n",
      "     |      Whether to use the shrinking heuristic.\n",
      "     |  \n",
      "     |  tol : float, optional (default=1e-3)\n",
      "     |      Tolerance for stopping criterion.\n",
      "     |  \n",
      "     |  cache_size : float, optional\n",
      "     |      Specify the size of the kernel cache (in MB).\n",
      "     |  \n",
      "     |  verbose : bool, default: False\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  max_iter : int, optional (default=-1)\n",
      "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
      "     |  \n",
      "     |  random_state : int seed, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator to use when\n",
      "     |      shuffling the data for probability estimation.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  support_ : array-like, shape = [n_SV]\n",
      "     |      Indices of support vectors.\n",
      "     |  \n",
      "     |  support_vectors_ : array-like, shape = [n_SV, n_features]\n",
      "     |      Support vectors.\n",
      "     |  \n",
      "     |  n_support_ : array-like, dtype=int32, shape = [n_class]\n",
      "     |      Number of support vector for each class.\n",
      "     |  \n",
      "     |  dual_coef_ : array, shape = [n_class-1, n_SV]\n",
      "     |      Coefficients of the support vector in the decision function.         For multiclass, coefficient for all 1-vs-1 classifiers.         The layout of the coefficients in the multiclass case is somewhat         non-trivial. See the section about multi-class classification in         the SVM section of the User Guide for details.\n",
      "     |  \n",
      "     |  coef_ : array, shape = [n_class-1, n_features]\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is readonly property derived from `dual_coef_` and\n",
      "     |      `support_vectors_`.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [n_class * (n_class-1) / 2]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "     |  >>> y = np.array([1, 1, 2, 2])\n",
      "     |  >>> from sklearn.svm import NuSVC\n",
      "     |  >>> clf = NuSVC()\n",
      "     |  >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE\n",
      "     |  NuSVC(cache_size=200, coef0=0.0, degree=3, gamma=0.0, kernel='rbf',\n",
      "     |        max_iter=-1, nu=0.5, probability=False, random_state=None,\n",
      "     |        shrinking=True, tol=0.001, verbose=False)\n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  SVC\n",
      "     |      Support Vector Machine for classification using libsvm.\n",
      "     |  \n",
      "     |  LinearSVC\n",
      "     |      Scalable linear Support Vector Machine for classification using\n",
      "     |      liblinear.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NuSVC\n",
      "     |      sklearn.svm.base.BaseSVC\n",
      "     |      sklearn.svm.base.BaseLibSVM\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, nu=0.5, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, verbose=False, max_iter=-1, random_state=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm.base.BaseSVC:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on samples in X.\n",
      "     |      \n",
      "     |      For an one-class model, +1 or -1 is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array, shape = [n_samples]\n",
      "     |          Class labels for samples in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm.base.BaseSVC:\n",
      "     |  \n",
      "     |  predict_log_proba\n",
      "     |      Compute log probabilities of possible outcomes for samples in X.\n",
      "     |      \n",
      "     |      The model need to have probability information computed at training\n",
      "     |      time: fit with attribute `probability` set to True.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the log-probabilities of the sample for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute `classes_`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The probability model is created using cross validation, so\n",
      "     |      the results can be slightly different than those obtained by\n",
      "     |      predict. Also, it will produce meaningless results on very small\n",
      "     |      datasets.\n",
      "     |  \n",
      "     |  predict_proba\n",
      "     |      Compute probabilities of possible outcomes for samples in X.\n",
      "     |      \n",
      "     |      The model need to have probability information computed at training\n",
      "     |      time: fit with attribute `probability` set to True.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the probability of the sample for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute `classes_`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The probability model is created using cross validation, so\n",
      "     |      the results can be slightly different than those obtained by\n",
      "     |      predict. Also, it will produce meaningless results on very small\n",
      "     |      datasets.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Distance of the samples X to the separating hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train].\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array-like, shape = [n_samples, n_class * (n_class-1) / 2]\n",
      "     |          Returns the decision function of the sample for each class\n",
      "     |          in the model.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the SVM model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples, n_samples).\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values (class labels in classification, real numbers in\n",
      "     |          regression)\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,)\n",
      "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
      "     |          force the classifier to put more emphasis on these points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      ------\n",
      "     |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      "     |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      "     |      \n",
      "     |      If X is a dense array, then the other methods will not support sparse\n",
      "     |      matrices as input.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep: boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The former have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class NuSVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
      "     |  Nu Support Vector Regression.\n",
      "     |  \n",
      "     |  Similar to NuSVC, for regression, uses a parameter nu to control\n",
      "     |  the number of support vectors. However, unlike NuSVC, where nu\n",
      "     |  replaces C, here nu replaces the parameter epsilon of epsilon-SVR.\n",
      "     |  \n",
      "     |  The implementation is based on libsvm.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  C : float, optional (default=1.0)\n",
      "     |      Penalty parameter C of the error term.\n",
      "     |  \n",
      "     |  nu : float, optional\n",
      "     |      An upper bound on the fraction of training errors and a lower bound of\n",
      "     |      the fraction of support vectors. Should be in the interval (0, 1].  By\n",
      "     |      default 0.5 will be taken.\n",
      "     |  \n",
      "     |  kernel : string, optional (default='rbf')\n",
      "     |       Specifies the kernel type to be used in the algorithm.\n",
      "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      "     |       a callable.\n",
      "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
      "     |       used to precompute the kernel matrix.\n",
      "     |  \n",
      "     |  degree : int, optional (default=3)\n",
      "     |      Degree of kernel function\n",
      "     |      is significant only in poly, rbf, sigmoid.\n",
      "     |  \n",
      "     |  gamma : float, optional (default=0.0)\n",
      "     |      Kernel coefficient for rbf and poly, if gamma is 0.0 then 1/n_features\n",
      "     |      will be taken.\n",
      "     |  \n",
      "     |  coef0 : float, optional (default=0.0)\n",
      "     |      Independent term in kernel function. It is only significant\n",
      "     |      in poly/sigmoid.\n",
      "     |  \n",
      "     |  shrinking: boolean, optional (default=True)\n",
      "     |      Whether to use the shrinking heuristic.\n",
      "     |  \n",
      "     |  tol : float, optional (default=1e-3)\n",
      "     |      Tolerance for stopping criterion.\n",
      "     |  \n",
      "     |  cache_size : float, optional\n",
      "     |      Specify the size of the kernel cache (in MB).\n",
      "     |  \n",
      "     |  verbose : bool, default: False\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  max_iter : int, optional (default=-1)\n",
      "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  support_ : array-like, shape = [n_SV]\n",
      "     |      Indices of support vectors.\n",
      "     |  \n",
      "     |  support_vectors_ : array-like, shape = [nSV, n_features]\n",
      "     |      Support vectors.\n",
      "     |  \n",
      "     |  dual_coef_ : array, shape = [1, n_SV]\n",
      "     |      Coefficients of the support vector in the decision function.\n",
      "     |  \n",
      "     |  coef_ : array, shape = [1, n_features]\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is readonly property derived from `dual_coef_` and\n",
      "     |      `support_vectors_`.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.svm import NuSVR\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> np.random.seed(0)\n",
      "     |  >>> y = np.random.randn(n_samples)\n",
      "     |  >>> X = np.random.randn(n_samples, n_features)\n",
      "     |  >>> clf = NuSVR(C=1.0, nu=0.1)\n",
      "     |  >>> clf.fit(X, y)  #doctest: +NORMALIZE_WHITESPACE\n",
      "     |  NuSVR(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma=0.0, kernel='rbf',\n",
      "     |        max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  NuSVC\n",
      "     |      Support Vector Machine for classification implemented with libsvm\n",
      "     |      with a parameter to control the number of support vectors.\n",
      "     |  \n",
      "     |  SVR\n",
      "     |      epsilon Support Vector Machine for regression implemented with libsvm.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NuSVR\n",
      "     |      sklearn.svm.base.BaseLibSVM\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, nu=0.5, C=1.0, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, shrinking=True, tol=0.001, cache_size=200, verbose=False, max_iter=-1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Distance of the samples X to the separating hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train].\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array-like, shape = [n_samples, n_class * (n_class-1) / 2]\n",
      "     |          Returns the decision function of the sample for each class\n",
      "     |          in the model.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the SVM model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples, n_samples).\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values (class labels in classification, real numbers in\n",
      "     |          regression)\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,)\n",
      "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
      "     |          force the classifier to put more emphasis on these points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      ------\n",
      "     |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      "     |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      "     |      \n",
      "     |      If X is a dense array, then the other methods will not support sparse\n",
      "     |      matrices as input.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform regression on samples in X.\n",
      "     |      \n",
      "     |      For an one-class model, +1 or -1 is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples_test, n_samples_train).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array, shape (n_samples,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep: boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The former have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0, lower values are worse.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class OneClassSVM(sklearn.svm.base.BaseLibSVM)\n",
      "     |  Unsupervised Outlier Detection.\n",
      "     |  \n",
      "     |  Estimate the support of a high-dimensional distribution.\n",
      "     |  \n",
      "     |  The implementation is based on libsvm.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  kernel : string, optional (default='rbf')\n",
      "     |       Specifies the kernel type to be used in the algorithm.\n",
      "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      "     |       a callable.\n",
      "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
      "     |       used to precompute the kernel matrix.\n",
      "     |  \n",
      "     |  nu : float, optional\n",
      "     |      An upper bound on the fraction of training\n",
      "     |      errors and a lower bound of the fraction of support\n",
      "     |      vectors. Should be in the interval (0, 1]. By default 0.5\n",
      "     |      will be taken.\n",
      "     |  \n",
      "     |  degree : int, optional (default=3)\n",
      "     |      Degree of the polynomial kernel function ('poly').\n",
      "     |      Ignored by all other kernels.\n",
      "     |  \n",
      "     |  gamma : float, optional (default=0.0)\n",
      "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      "     |      If gamma is 0.0 then 1/n_features will be used instead.\n",
      "     |  \n",
      "     |  coef0 : float, optional (default=0.0)\n",
      "     |      Independent term in kernel function.\n",
      "     |      It is only significant in 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Tolerance for stopping criterion.\n",
      "     |  \n",
      "     |  shrinking: boolean, optional\n",
      "     |      Whether to use the shrinking heuristic.\n",
      "     |  \n",
      "     |  cache_size : float, optional\n",
      "     |      Specify the size of the kernel cache (in MB).\n",
      "     |  \n",
      "     |  verbose : bool, default: False\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  max_iter : int, optional (default=-1)\n",
      "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
      "     |  \n",
      "     |  random_state : int seed, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator to use when\n",
      "     |      shuffling the data for probability estimation.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  support_ : array-like, shape = [n_SV]\n",
      "     |      Indices of support vectors.\n",
      "     |  \n",
      "     |  support_vectors_ : array-like, shape = [nSV, n_features]\n",
      "     |      Support vectors.\n",
      "     |  \n",
      "     |  dual_coef_ : array, shape = [n_classes-1, n_SV]\n",
      "     |      Coefficients of the support vectors in the decision function.\n",
      "     |  \n",
      "     |  coef_ : array, shape = [n_classes-1, n_features]\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is readonly property derived from `dual_coef_` and\n",
      "     |      `support_vectors_`\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [n_classes-1]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OneClassSVM\n",
      "     |      sklearn.svm.base.BaseLibSVM\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, tol=0.001, nu=0.5, shrinking=True, cache_size=200, verbose=False, max_iter=-1, random_state=None)\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None, **params)\n",
      "     |      Detects the soft boundary of the set of samples X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Set of samples, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,)\n",
      "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
      "     |          force the classifier to put more emphasis on these points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If X is not a C-ordered contiguous array it is copied.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Distance of the samples X to the separating hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train].\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array-like, shape = [n_samples, n_class * (n_class-1) / 2]\n",
      "     |          Returns the decision function of the sample for each class\n",
      "     |          in the model.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform regression on samples in X.\n",
      "     |      \n",
      "     |      For an one-class model, +1 or -1 is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples_test, n_samples_train).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array, shape (n_samples,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep: boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The former have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SVC(sklearn.svm.base.BaseSVC)\n",
      "     |  C-Support Vector Classification.\n",
      "     |  \n",
      "     |  The implementation is based on libsvm. The fit time complexity\n",
      "     |  is more than quadratic with the number of samples which makes it hard\n",
      "     |  to scale to dataset with more than a couple of 10000 samples.\n",
      "     |  \n",
      "     |  The multiclass support is handled according to a one-vs-one scheme.\n",
      "     |  \n",
      "     |  For details on the precise mathematical formulation of the provided\n",
      "     |  kernel functions and how `gamma`, `coef0` and `degree` affect each\n",
      "     |  other, see the corresponding section in the narrative documentation:\n",
      "     |  :ref:`svm_kernels`.\n",
      "     |  \n",
      "     |  .. The narrative documentation is available at http://scikit-learn.org/\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  C : float, optional (default=1.0)\n",
      "     |      Penalty parameter C of the error term.\n",
      "     |  \n",
      "     |  kernel : string, optional (default='rbf')\n",
      "     |       Specifies the kernel type to be used in the algorithm.\n",
      "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      "     |       a callable.\n",
      "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
      "     |       used to precompute the kernel matrix.\n",
      "     |  \n",
      "     |  degree : int, optional (default=3)\n",
      "     |      Degree of the polynomial kernel function ('poly').\n",
      "     |      Ignored by all other kernels.\n",
      "     |  \n",
      "     |  gamma : float, optional (default=0.0)\n",
      "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      "     |      If gamma is 0.0 then 1/n_features will be used instead.\n",
      "     |  \n",
      "     |  coef0 : float, optional (default=0.0)\n",
      "     |      Independent term in kernel function.\n",
      "     |      It is only significant in 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |  probability: boolean, optional (default=False)\n",
      "     |      Whether to enable probability estimates. This must be enabled prior\n",
      "     |      to calling `fit`, and will slow down that method.\n",
      "     |  \n",
      "     |  shrinking: boolean, optional (default=True)\n",
      "     |      Whether to use the shrinking heuristic.\n",
      "     |  \n",
      "     |  tol : float, optional (default=1e-3)\n",
      "     |      Tolerance for stopping criterion.\n",
      "     |  \n",
      "     |  cache_size : float, optional\n",
      "     |      Specify the size of the kernel cache (in MB).\n",
      "     |  \n",
      "     |  class_weight : {dict, 'auto'}, optional\n",
      "     |      Set the parameter C of class i to class_weight[i]*C for\n",
      "     |      SVC. If not given, all classes are supposed to have\n",
      "     |      weight one. The 'auto' mode uses the values of y to\n",
      "     |      automatically adjust weights inversely proportional to\n",
      "     |      class frequencies.\n",
      "     |  \n",
      "     |  verbose : bool, default: False\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  max_iter : int, optional (default=-1)\n",
      "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
      "     |  \n",
      "     |  random_state : int seed, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator to use when\n",
      "     |      shuffling the data for probability estimation.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  support_ : array-like, shape = [n_SV]\n",
      "     |      Indices of support vectors.\n",
      "     |  \n",
      "     |  support_vectors_ : array-like, shape = [n_SV, n_features]\n",
      "     |      Support vectors.\n",
      "     |  \n",
      "     |  n_support_ : array-like, dtype=int32, shape = [n_class]\n",
      "     |      Number of support vectors for each class.\n",
      "     |  \n",
      "     |  dual_coef_ : array, shape = [n_class-1, n_SV]\n",
      "     |      Coefficients of the support vector in the decision function.         For multiclass, coefficient for all 1-vs-1 classifiers.         The layout of the coefficients in the multiclass case is somewhat         non-trivial. See the section about multi-class classification in the         SVM section of the User Guide for details.\n",
      "     |  \n",
      "     |  coef_ : array, shape = [n_class-1, n_features]\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is a readonly property derived from `dual_coef_` and\n",
      "     |      `support_vectors_`.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [n_class * (n_class-1) / 2]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "     |  >>> y = np.array([1, 1, 2, 2])\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> clf = SVC()\n",
      "     |  >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE\n",
      "     |  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,\n",
      "     |      gamma=0.0, kernel='rbf', max_iter=-1, probability=False,\n",
      "     |      random_state=None, shrinking=True, tol=0.001, verbose=False)\n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  SVR\n",
      "     |      Support Vector Machine for Regression implemented using libsvm.\n",
      "     |  \n",
      "     |  LinearSVC\n",
      "     |      Scalable Linear Support Vector Machine for classification\n",
      "     |      implemented using liblinear. Check the See also section of\n",
      "     |      LinearSVC for more comparison element.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SVC\n",
      "     |      sklearn.svm.base.BaseSVC\n",
      "     |      sklearn.svm.base.BaseLibSVM\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, C=1.0, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm.base.BaseSVC:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on samples in X.\n",
      "     |      \n",
      "     |      For an one-class model, +1 or -1 is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array, shape = [n_samples]\n",
      "     |          Class labels for samples in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm.base.BaseSVC:\n",
      "     |  \n",
      "     |  predict_log_proba\n",
      "     |      Compute log probabilities of possible outcomes for samples in X.\n",
      "     |      \n",
      "     |      The model need to have probability information computed at training\n",
      "     |      time: fit with attribute `probability` set to True.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the log-probabilities of the sample for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute `classes_`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The probability model is created using cross validation, so\n",
      "     |      the results can be slightly different than those obtained by\n",
      "     |      predict. Also, it will produce meaningless results on very small\n",
      "     |      datasets.\n",
      "     |  \n",
      "     |  predict_proba\n",
      "     |      Compute probabilities of possible outcomes for samples in X.\n",
      "     |      \n",
      "     |      The model need to have probability information computed at training\n",
      "     |      time: fit with attribute `probability` set to True.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the probability of the sample for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute `classes_`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The probability model is created using cross validation, so\n",
      "     |      the results can be slightly different than those obtained by\n",
      "     |      predict. Also, it will produce meaningless results on very small\n",
      "     |      datasets.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Distance of the samples X to the separating hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train].\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array-like, shape = [n_samples, n_class * (n_class-1) / 2]\n",
      "     |          Returns the decision function of the sample for each class\n",
      "     |          in the model.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the SVM model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples, n_samples).\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values (class labels in classification, real numbers in\n",
      "     |          regression)\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,)\n",
      "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
      "     |          force the classifier to put more emphasis on these points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      ------\n",
      "     |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      "     |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      "     |      \n",
      "     |      If X is a dense array, then the other methods will not support sparse\n",
      "     |      matrices as input.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep: boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The former have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class SVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
      "     |  Epsilon-Support Vector Regression.\n",
      "     |  \n",
      "     |  The free parameters in the model are C and epsilon.\n",
      "     |  \n",
      "     |  The implementation is based on libsvm.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  C : float, optional (default=1.0)\n",
      "     |      Penalty parameter C of the error term.\n",
      "     |  \n",
      "     |  epsilon : float, optional (default=0.1)\n",
      "     |       Epsilon in the epsilon-SVR model. It specifies the epsilon-tube\n",
      "     |       within which no penalty is associated in the training loss function\n",
      "     |       with points predicted within a distance epsilon from the actual\n",
      "     |       value.\n",
      "     |  \n",
      "     |  kernel : string, optional (default='rbf')\n",
      "     |       Specifies the kernel type to be used in the algorithm.\n",
      "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      "     |       a callable.\n",
      "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
      "     |       used to precompute the kernel matrix.\n",
      "     |  \n",
      "     |  degree : int, optional (default=3)\n",
      "     |      Degree of kernel function\n",
      "     |      is significant only in poly, rbf, sigmoid.\n",
      "     |  \n",
      "     |  gamma : float, optional (default=0.0)\n",
      "     |      Kernel coefficient for rbf and poly, if gamma is 0.0 then 1/n_features\n",
      "     |      will be taken.\n",
      "     |  \n",
      "     |  coef0 : float, optional (default=0.0)\n",
      "     |      independent term in kernel function. It is only significant\n",
      "     |      in poly/sigmoid.\n",
      "     |  \n",
      "     |  shrinking: boolean, optional (default=True)\n",
      "     |      Whether to use the shrinking heuristic.\n",
      "     |  \n",
      "     |  tol : float, optional (default=1e-3)\n",
      "     |      Tolerance for stopping criterion.\n",
      "     |  \n",
      "     |  cache_size : float, optional\n",
      "     |      Specify the size of the kernel cache (in MB).\n",
      "     |  \n",
      "     |  verbose : bool, default: False\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  max_iter : int, optional (default=-1)\n",
      "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  support_ : array-like, shape = [n_SV]\n",
      "     |      Indices of support vectors.\n",
      "     |  \n",
      "     |  support_vectors_ : array-like, shape = [nSV, n_features]\n",
      "     |      Support vectors.\n",
      "     |  \n",
      "     |  dual_coef_ : array, shape = [1, n_SV]\n",
      "     |      Coefficients of the support vector in the decision function.\n",
      "     |  \n",
      "     |  coef_ : array, shape = [1, n_features]\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is readonly property derived from `dual_coef_` and\n",
      "     |      `support_vectors_`.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.svm import SVR\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> np.random.seed(0)\n",
      "     |  >>> y = np.random.randn(n_samples)\n",
      "     |  >>> X = np.random.randn(n_samples, n_features)\n",
      "     |  >>> clf = SVR(C=1.0, epsilon=0.2)\n",
      "     |  >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE\n",
      "     |  SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma=0.0,\n",
      "     |      kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  NuSVR\n",
      "     |      Support Vector Machine for regression implemented using libsvm\n",
      "     |      using a parameter to control the number of support vectors.\n",
      "     |  \n",
      "     |  LinearSVR\n",
      "     |      Scalable Linear Support Vector Machine for regression\n",
      "     |      implemented using liblinear.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SVR\n",
      "     |      sklearn.svm.base.BaseLibSVM\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Distance of the samples X to the separating hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train].\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array-like, shape = [n_samples, n_class * (n_class-1) / 2]\n",
      "     |          Returns the decision function of the sample for each class\n",
      "     |          in the model.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the SVM model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples, n_samples).\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values (class labels in classification, real numbers in\n",
      "     |          regression)\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,)\n",
      "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
      "     |          force the classifier to put more emphasis on these points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      ------\n",
      "     |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      "     |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      "     |      \n",
      "     |      If X is a dense array, then the other methods will not support sparse\n",
      "     |      matrices as input.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform regression on samples in X.\n",
      "     |      \n",
      "     |      For an one-class model, +1 or -1 is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples_test, n_samples_train).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array, shape (n_samples,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep: boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The former have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0, lower values are worse.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "\n",
      "FUNCTIONS\n",
      "    l1_min_c(X, y, loss='squared_hinge', fit_intercept=True, intercept_scaling=1.0)\n",
      "        Return the lowest bound for C such that for C in (l1_min_C, infinity)\n",
      "        the model is guaranteed not to be empty. This applies to l1 penalized\n",
      "        classifiers, such as LinearSVC with penalty='l1' and\n",
      "        linear_model.LogisticRegression with penalty='l1'.\n",
      "        \n",
      "        This value is valid if class_weight parameter in fit() is not set.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "            Training vector, where n_samples in the number of samples and\n",
      "            n_features is the number of features.\n",
      "        \n",
      "        y : array, shape = [n_samples]\n",
      "            Target vector relative to X\n",
      "        \n",
      "        loss : {'squared_hinge', 'log'}, default 'squared_hinge'\n",
      "            Specifies the loss function.\n",
      "            With 'squared_hinge' it is the squared hinge loss (a.k.a. L2 loss).\n",
      "            With 'log' it is the loss of logistic regression models.\n",
      "            'l2' is accepted as an alias for 'squared_hinge', for backward\n",
      "            compatibility reasons, but should not be used in new code.\n",
      "        \n",
      "        fit_intercept : bool, default: True\n",
      "            Specifies if the intercept should be fitted by the model.\n",
      "            It must match the fit() method parameter.\n",
      "        \n",
      "        intercept_scaling : float, default: 1\n",
      "            when fit_intercept is True, instance vector x becomes\n",
      "            [x, intercept_scaling],\n",
      "            i.e. a \"synthetic\" feature with constant value equals to\n",
      "            intercept_scaling is appended to the instance vector.\n",
      "            It must match the fit() method parameter.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        l1_min_c: float\n",
      "            minimum value for C\n",
      "\n",
      "DATA\n",
      "    __all__ = ['LinearSVC', 'LinearSVR', 'NuSVC', 'NuSVR', 'OneClassSVM', ...\n",
      "\n",
      "FILE\n",
      "    d:\\360downloads\\anaconda3\\lib\\site-packages\\sklearn\\svm\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import svm\n",
    "help(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.svm.tests in sklearn.svm:\n",
      "\n",
      "NAME\n",
      "    sklearn.svm.tests\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    test_bounds\n",
      "    test_sparse\n",
      "    test_svm\n",
      "\n",
      "FILE\n",
      "    d:\\360downloads\\anaconda3\\lib\\site-packages\\sklearn\\svm\\tests\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import tests\n",
    "help(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module sklearn.svm.tests.test_svm in sklearn.svm.tests:\n",
      "\n",
      "NAME\n",
      "    sklearn.svm.tests.test_svm - Testing for Support Vector Machine module (sklearn.svm)\n",
      "\n",
      "DESCRIPTION\n",
      "    TODO: remove hard coded numerical results when possible\n",
      "\n",
      "FUNCTIONS\n",
      "    test_auto_weight()\n",
      "    \n",
      "    test_bad_input()\n",
      "    \n",
      "    test_consistent_proba()\n",
      "    \n",
      "    test_crammer_singer_binary()\n",
      "    \n",
      "    test_decision_function()\n",
      "    \n",
      "    test_dense_liblinear_intercept_handling(classifier=<class 'sklearn.svm.classes.LinearSVC'>)\n",
      "    \n",
      "    test_immutable_coef_property()\n",
      "    \n",
      "    test_inheritance()\n",
      "    \n",
      "    test_liblinear_set_coef()\n",
      "    \n",
      "    test_libsvm_iris()\n",
      "    \n",
      "    test_libsvm_parameters()\n",
      "    \n",
      "    test_linear_svc_convergence_warnings()\n",
      "    \n",
      "    test_linear_svc_intercept_scaling()\n",
      "    \n",
      "    test_linear_svx_uppercase_loss_penalty()\n",
      "        # FIXME remove in 0.18\n",
      "    \n",
      "    test_linearsvc()\n",
      "    \n",
      "    test_linearsvc_crammer_singer()\n",
      "    \n",
      "    test_linearsvc_iris()\n",
      "    \n",
      "    test_linearsvc_parameters()\n",
      "    \n",
      "    test_linearsvc_verbose()\n",
      "    \n",
      "    test_linearsvr()\n",
      "    \n",
      "    test_linearsvx_loss_penalty_deprecations()\n",
      "        # FIXME remove in 1.0\n",
      "    \n",
      "    test_lsvc_intercept_scaling_zero()\n",
      "    \n",
      "    test_oneclass()\n",
      "    \n",
      "    test_oneclass_decision_function()\n",
      "    \n",
      "    test_precomputed()\n",
      "    \n",
      "    test_probability()\n",
      "    \n",
      "    test_sample_weights()\n",
      "    \n",
      "    test_single_sample_1d()\n",
      "    \n",
      "    test_sparse_precomputed()\n",
      "    \n",
      "    test_svc_bad_kernel()\n",
      "    \n",
      "    test_svc_clone_with_callable_kernel()\n",
      "    \n",
      "    test_svr()\n",
      "    \n",
      "    test_svr_coef_sign()\n",
      "    \n",
      "    test_svr_decision_function()\n",
      "    \n",
      "    test_svr_errors()\n",
      "    \n",
      "    test_timeout()\n",
      "    \n",
      "    test_tweak_params()\n",
      "    \n",
      "    test_unfitted()\n",
      "    \n",
      "    test_weight()\n",
      "\n",
      "DATA\n",
      "    T = [[-1, -1], [2, 2], [3, 2]]\n",
      "    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n",
      "    Y = [1, 1, 1, 2, 2, 2]\n",
      "    iris = {'DESCR': 'Iris Plants Database\\n\\nNotes\\n-----\\..., 2,\n",
      "           ...\n",
      "    perm = array([ 73,  18, 118,  78,  76,  31,  64, 141,  ..., 121,\n",
      "         ...\n",
      "    rng = <mtrand.RandomState object>\n",
      "    true_result = [1, 2, 2]\n",
      "\n",
      "FILE\n",
      "    d:\\360downloads\\anaconda3\\lib\\site-packages\\sklearn\\svm\\tests\\test_svm.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm.tests import test_svm\n",
    "help(tests.test_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    y_train.extend([i]*1592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14328"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = []\n",
    "for i in range(9):\n",
    "    y_test.extend([i]*398)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3582"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TransformedCorpus in module gensim.interfaces:\n",
      "\n",
      "class TransformedCorpus(CorpusABC)\n",
      " |  Method resolution order:\n",
      " |      TransformedCorpus\n",
      " |      CorpusABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, docno)\n",
      " |  \n",
      " |  __init__(self, obj, corpus, chunksize=None)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from CorpusABC:\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from CorpusABC:\n",
      " |  \n",
      " |  save_corpus(fname, corpus, id2word=None, metadata=False)\n",
      " |      Save an existing `corpus` to disk.\n",
      " |      \n",
      " |      Some formats also support saving the dictionary (`feature_id->word` mapping),\n",
      " |      which can in this case be provided by the optional `id2word` parameter.\n",
      " |      \n",
      " |      >>> MmCorpus.save_corpus('file.mm', corpus)\n",
      " |      \n",
      " |      Some corpora also support an index of where each document begins, so\n",
      " |      that the documents on disk can be accessed in O(1) time (see the\n",
      " |      `corpora.IndexedCorpus` base class). In this case, `save_corpus` is automatically\n",
      " |      called internally by `serialize`, which does `save_corpus` plus saves the index\n",
      " |      at the same time, so you want to store the corpus with::\n",
      " |      \n",
      " |      >>> MmCorpus.serialize('file.mm', corpus) # stores index as well, allowing random access to individual documents\n",
      " |      \n",
      " |      Calling `serialize()` is preferred to calling `save_corpus()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  load(fname, mmap=None) from builtins.type\n",
      " |      Load a previously saved object from file (also see `save`).\n",
      " |      \n",
      " |      If the object was saved with large arrays stored separately, you can load\n",
      " |      these arrays via mmap (shared memory) using `mmap='r'`. Default: don't use\n",
      " |      mmap, load large arrays as normal objects.\n",
      " |      \n",
      " |      If the file being loaded is compressed (either '.gz' or '.bz2'), then\n",
      " |      `mmap=None` must be set.  Load will raise an `IOError` if this condition\n",
      " |      is encountered.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim.interfaces\n",
    "help(gensim.interfaces.TransformedCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "help(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
